{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#how data are represented at each level (forward, backward, forward with padding on top) needs a little\n",
    "    #experimentation to determine the best representation\n",
    "    #also, is encoding at each layer really the best way? or just feeding the raw through?\n",
    "    \n",
    "#Outside web ips are going to be a problem/messy/noisy. Start by categorizing all outside ips by <OUTSIDE_IP>\n",
    "    #instead of the ip address, or another 4 digit symbol to insert into the hex string.\n",
    "    \n",
    "#to help the models generalize more, for a given source ip address with probability p (say p = 0.1) \n",
    "    #use the token <OTHER_MACHINE>\n",
    "    \n",
    "#should we remove random parts of the header, i.e. checksum\n",
    "\n",
    "#should I take out bias for RNNs?\n",
    "\n",
    "#for the decoder,does the fork encoding need to happen ?\n",
    "    #do we simply cat the hContext with the next words?\n",
    "    \n",
    "#Should the architecture just be encode, context and then prediction???\n",
    "\n",
    "#Input data, should it have character and hex pair encoding as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled, CuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "#%matplotlib inline\n",
    "import os\n",
    "os.environ['THEANO_FLAGS'] = 'floatX=float32,device=gpu'\n",
    "\n",
    "import sessionizer\n",
    "import learningfunctions\n",
    "import adversarialfunctions\n",
    "import json\n",
    "import subprocess\n",
    "import cPickle\n",
    "import sys\n",
    "import binascii\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import copy\n",
    "\n",
    "import blocks\n",
    "from blocks.bricks import Linear, Softmax, Softplus, NDimensionalSoftmax, BatchNormalizedMLP, \\\n",
    "                                Rectifier, Logistic, Tanh, MLP\n",
    "from blocks.bricks.recurrent import GatedRecurrent, Fork, LSTM\n",
    "from blocks.initialization import Constant, IsotropicGaussian, Identity, Uniform\n",
    "from blocks.bricks.cost import BinaryCrossEntropy, CategoricalCrossEntropy\n",
    "from blocks.filter import VariableFilter\n",
    "from blocks.roles import PARAMETER\n",
    "from blocks.graph import ComputationGraph\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "\n",
    "sys.setrecursionlimit(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxPackets = 3\n",
    "loadPrepedData = True  # load preprocessed data\n",
    "dataPath = '/data/fs4/home/bradh/bigFlows.pickle'  # path to data\n",
    "savePath = '/data/fs4/home/bradh/outputs/'  # where to save outputs\n",
    "\n",
    "packetTimeSteps = 40 # number of hex pairs\n",
    "packetReverse = False # reverse the order of packets ala seq2seq\n",
    "padOldTimeSteps = True # pad short sessions/packets at beginning(True) or end (False)\n",
    "onlyEssentials = True  # extracts only length,protocol,frag,srcIP,dstIP,srcport,dstport from header\n",
    "if onlyEssentials:  \n",
    "    packetTimeSteps = 16\n",
    "\n",
    "runname = 'noiseclassification3packsmall'\n",
    "rnnType = 'gru'  # gru or lstm\n",
    "\n",
    "wtstd = 0.2  # standard dev for Isotropic weight initialization\n",
    "dimIn = 257  # hex has 256 characters + the <EOP> character\n",
    "dim = 100  # dimension reduction size\n",
    "clippings = 1  # for gradient clipping\n",
    "batch_size = 20  \n",
    "binaryTarget = True\n",
    "\n",
    "if binaryTarget:\n",
    "    numClasses = 2\n",
    "else:\n",
    "    numClasses = 5\n",
    "\n",
    "epochs = 50 \n",
    "lr = 0.0001\n",
    "decay = 0.9\n",
    "trainPercent = 0.9  # training testing split\n",
    "\n",
    "module_logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load and save files and models\n",
    "def pickleFile(thing2save, file2save2 = None, filePath='/work/notebooks/drawModels/', fileName = 'myModels'):\n",
    "    \n",
    "    if file2save2 == None:\n",
    "        f=file(filePath+fileName+'.pickle', 'wb')\n",
    "    else:\n",
    "        f=file(filePath+file2save2, 'wb')\n",
    "        \n",
    "    cPickle.dump(thing2save, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    f.close()\n",
    "    \n",
    "def loadFile(filePath):\n",
    "    file2open = file(filePath, 'rb')\n",
    "    loadedFile = cPickle.load(file2open)\n",
    "    file2open.close()\n",
    "    \n",
    "    return loadedFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Making the hex dictionary\n",
    "def hexTokenizer():\n",
    "    hexstring = '0,\t1,\t2,\t3,\t4,\t5,\t6,\t7,\t8,\t9,\tA,\tB,\tC,\tD,\tE,\tF,\t10,\t11,\t12,\t13,\t14,\t15,\t16,\t17,\t18,\t19\\\n",
    "    ,\t1A,\t1B,\t1C,\t1D,\t1E,\t1F,\t20,\t21,\t22,\t23,\t24,\t25,\t26,\t27,\t28,\t29,\t2A,\t2B,\t2C,\t2D,\t2E,\t2F,\t30,\t31,\t32,\t33,\t34,\t35\\\n",
    "    ,\t36,\t37,\t38,\t39,\t3A,\t3B,\t3C,\t3D,\t3E,\t3F,\t40,\t41,\t42,\t43,\t44,\t45,\t46,\t47,\t48,\t49,\t4A,\t4B,\t4C,\t4D,\t4E,\t4F,\t50,\t51\\\n",
    "    ,\t52,\t53,\t54,\t55,\t56,\t57,\t58,\t59,\t5A,\t5B,\t5C,\t5D,\t5E,\t5F,\t60,\t61,\t62,\t63,\t64,\t65,\t66,\t67,\t68,\t69,\t6A,\t6B,\t6C,\t6D\\\n",
    "    ,\t6E,\t6F,\t70,\t71,\t72,\t73,\t74,\t75,\t76,\t77,\t78,\t79,\t7A,\t7B,\t7C,\t7D,\t7E,\t7F,\t80,\t81,\t82,\t83,\t84,\t85,\t86,\t87,\t88,\t89\\\n",
    "    ,\t8A,\t8B,\t8C,\t8D,\t8E,\t8F,\t90,\t91,\t92,\t93,\t94,\t95,\t96,\t97,\t98,\t99,\t9A,\t9B,\t9C,\t9D,\t9E,\t9F,\tA0,\tA1,\tA2,\tA3,\tA4,\tA5\\\n",
    "    ,\tA6,\tA7,\tA8,\tA9,\tAA,\tAB,\tAC,\tAD,\tAE,\tAF,\tB0,\tB1,\tB2,\tB3,\tB4,\tB5,\tB6,\tB7,\tB8,\tB9,\tBA,\tBB,\tBC,\tBD,\tBE,\tBF,\tC0,\tC1\\\n",
    "    ,\tC2,\tC3,\tC4,\tC5,\tC6,\tC7,\tC8,\tC9,\tCA,\tCB,\tCC,\tCD,\tCE,\tCF,\tD0,\tD1,\tD2,\tD3,\tD4,\tD5,\tD6,\tD7,\tD8,\tD9,\tDA,\tDB,\tDC,\tDD\\\n",
    "    ,\tDE,\tDF,\tE0,\tE1,\tE2,\tE3,\tE4,\tE5,\tE6,\tE7,\tE8,\tE9,\tEA,\tEB,\tEC,\tED,\tEE,\tEF,\tF0,\tF1,\tF2,\tF3,\tF4,\tF5,\tF6,\tF7,\tF8,\tF9\\\n",
    "    ,\tFA,\tFB,\tFC,\tFD,\tFE,\tFF'.replace('\\t', '')\n",
    "\n",
    "    hexList = [x.strip() for x in hexstring.lower().split(',')]\n",
    "    hexList.append('<EOP>') #End Of Packet token\n",
    "    #EOS token??????\n",
    "    hexDict = {}\n",
    "\n",
    "    for key, val in enumerate(hexList):\n",
    "        if len(val) == 1:\n",
    "            val = '0'+val\n",
    "        hexDict[val] = key  #dictionary: k=hex, v=int  \n",
    "    \n",
    "    return hexDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary of IP communications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def srcIpDict(hexSessionDict):\n",
    "    ''' \n",
    "    input: dictionary of key = sessions, value = list of HEX HEADERS of packets in session\n",
    "    output: dictionary of key = source IP, value/subkey = dictionary of destination IPs, \n",
    "                                           subvalue = [[sport], [dport], [plen], [protocol]]\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    srcIpDict = {}   \n",
    "    uniqIPs = [] #some ips are dest only. this will collect all ips, not just srcIpDict.keys()\n",
    "    \n",
    "    for session in hexSessionDict.keys():\n",
    "        \n",
    "        for rawpacket in hexSessionDict[session][0]:\n",
    "            packet = copy(rawpacket)\n",
    "            \n",
    "            dstIpSubDict = {}\n",
    "            \n",
    "            sourceMAC = packet[:12]\n",
    "            destMAC = packet[12:24]\n",
    "            srcip = packet[52:60]\n",
    "            dstip = packet[60:68]\n",
    "            sport = packet[68:72]\n",
    "            dport = packet[72:76]\n",
    "            plen = packet[32:36]\n",
    "            protocol = packet[46:48]\n",
    "            \n",
    "            uniqIPs = list(set(uniqIPs) | set([dstip, srcip]))\n",
    "\n",
    "            if srcip not in srcIpDict:\n",
    "                dstIpSubDict[dstip] = [[sport], [dport], [plen], [protocol], [sourceMAC], [destMAC]]\n",
    "                srcIpDict[srcip] = dstIpSubDict\n",
    "\n",
    "            if dstip not in srcIpDict[srcip]:    \n",
    "                srcIpDict[srcip][dstip] = [[sport], [dport], [plen], [protocol], [sourceMAC], [destMAC]]\n",
    "            else:\n",
    "                srcIpDict[srcip][dstip][0].append(sport)\n",
    "                srcIpDict[srcip][dstip][1].append(dport)\n",
    "                srcIpDict[srcip][dstip][2].append(plen)\n",
    "                srcIpDict[srcip][dstip][3].append(protocol)\n",
    "                srcIpDict[srcip][dstip][4].append(sourceMAC)\n",
    "                srcIpDict[srcip][dstip][5].append(destMAC)\n",
    "                \n",
    "    return srcIpDict, uniqIPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dictUniquerizer(dictOdictsOlistOlists):\n",
    "    '''\n",
    "    input is the output of srcIpDict\n",
    "    input: dictionary of dictionaries that have a list of lists \n",
    "           ex. srcIpDict[srcip][dstip] = [[sport], [dport], [plen], [protocol]]\n",
    "    output: dictionary of dictionaries with list of lists with unique items in the final sublist\n",
    "    \n",
    "    WARNING: will overwrite your input dictionary. Make a copy if you want to preserve dictOdictsOlistOlists.\n",
    "    '''\n",
    "    #dictCopy\n",
    "    for key in dictOdictsOlistOlists.keys():\n",
    "        for subkey in dictOdictsOlistOlists[key].keys():\n",
    "            for sublist in xrange(len(dictOdictsOlistOlists[key][subkey])):\n",
    "                dictOdictsOlistOlists[key][subkey][sublist] = list(set(dictOdictsOlistOlists[key][subkey][sublist]))\n",
    "    \n",
    "    return dictOdictsOlistOlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def oneHot(index, granular = 'hex'):\n",
    "    if granular == 'hex':\n",
    "        vecLen = 257\n",
    "    else:\n",
    "        vecLen = 17\n",
    "    \n",
    "    zeroVec = np.zeros(vecLen)\n",
    "    zeroVec[index] = 1.0\n",
    "    \n",
    "    return zeroVec\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO: add character level encoding\n",
    "def oneSessionEncoder(sessionPackets, hexDict, maxPackets = 2, packetTimeSteps = 100,\n",
    "                      packetReverse = False, charLevel = False, padOldTimeSteps = True, \n",
    "                      onlyEssentials = False):    \n",
    "            \n",
    "    sessionCollect = []\n",
    "    packetCollect = []\n",
    "    \n",
    "    if charLevel:\n",
    "        vecLen = 17\n",
    "    else:\n",
    "        vecLen = 257\n",
    "    \n",
    "    if len(sessionPackets) > maxPackets: #crop the number of sessions to maxPackets\n",
    "        sessionList = copy(sessionPackets[:maxPackets])\n",
    "    else:\n",
    "        sessionList = copy(sessionPackets)\n",
    "\n",
    "    for rawpacket in sessionList:\n",
    "        packet = copy(rawpacket)\n",
    "        \n",
    "        if onlyEssentials: #cat of length,protocol,frag,srcIP,dstIP,srcport,dstport\n",
    "            packet = packet[32:36]+packet[44:46]+packet[46:48]+packet[52:60]+packet[60:68]+\\\n",
    "                     packet[68:72]+packet[72:76]\n",
    "        \n",
    "        packet = [hexDict[packet[i:i+2]] for i in xrange(0,len(packet)-2+1,2)] #get hex pairs\n",
    "            \n",
    "        if len(packet) >= packetTimeSteps: #crop packet to length packetTimeSteps rel to hex pairs\n",
    "            packet = packet[:packetTimeSteps]\n",
    "        \n",
    "        packet = packet+[256] #add <EOP> end of packet token\n",
    "        \n",
    "        packetCollect.append(packet)\n",
    "        \n",
    "        pacMat = np.array([oneHot(x) for x in packet]) #one hot encoding of packet into a matrix\n",
    "        pacMatLen = len(pacMat)\n",
    "        \n",
    "        #padding packet\n",
    "        if packetReverse:\n",
    "            pacMat = pacMat[::-1]\n",
    "\n",
    "        if pacMatLen < packetTimeSteps:\n",
    "            #pad by stacking zeros on top of data so that earlier timesteps do not have information\n",
    "            #padding the packet such that zeros are after the actual info for better translation\n",
    "            if padOldTimeSteps:\n",
    "                pacMat = np.vstack( ( np.zeros((packetTimeSteps-pacMatLen,vecLen)), pacMat) ) \n",
    "            else:\n",
    "                pacMat = np.vstack( (pacMat, np.zeros((packetTimeSteps-pacMatLen,vecLen))) ) \n",
    "\n",
    "        if pacMatLen > packetTimeSteps:\n",
    "            pacMat = pacMat[:packetTimeSteps, :]\n",
    "\n",
    "        sessionCollect.append(pacMat)\n",
    "\n",
    "    #padding session\n",
    "    sessionCollect = np.asarray(sessionCollect, dtype=theano.config.floatX)\n",
    "    numPacketsInSession = sessionCollect.shape[0]\n",
    "    if numPacketsInSession < maxPackets:\n",
    "        #pad sessions to fit the \n",
    "        sessionCollect = np.vstack( (sessionCollect,np.zeros((maxPackets-numPacketsInSession, \n",
    "                                                             packetTimeSteps, vecLen))) )\n",
    "    \n",
    "    return sessionCollect, packetCollect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predictClass(predictFun, hexSessionsDict, comsDict, uniqIPs, hexDict, hexSessionsKeys,\n",
    "                 binaryTarget, numClasses, trainPercent = 0.9, dimIn=257, maxPackets=2,\n",
    "                 packetTimeSteps = 16, padOldTimeSteps=True):\n",
    "    \n",
    "    testCollect = []\n",
    "    predtargets = []\n",
    "    actualtargets = []\n",
    "    trainPercent = 0.9\n",
    "    trainIndex = int(len(hexSessionsKeys)*trainPercent)\n",
    "        \n",
    "    start = trainIndex\n",
    "    end = len(hexSessionsKeys)\n",
    "        \n",
    "    trainingSessions = []\n",
    "    trainingTargets = []\n",
    "\n",
    "    for trainKey in range(start, end):\n",
    "        sessionForEncoding = list(hexSessionsDict[hexSessionsKeys[trainKey]][0])\n",
    "\n",
    "        adfun = adversarialfunctions.Adversary(sessionForEncoding)\n",
    "        adversaryList = [sessionForEncoding, \n",
    "                         adfun.dstIpSwapOut(comsDict, uniqIPs),\n",
    "                         adfun.portDirSwitcher(),\n",
    "                         adfun.ipDirSwitcher(),\n",
    "                         adfun.noisyPacketMaker(maxPackets, packetTimeSteps, percentNoisy = 0.2)]\n",
    "        if binaryTarget:\n",
    "            # choose normal and one of the abnormal types\n",
    "            abbyIndex = random.sample([0, random.sample(xrange(1,len(adversaryList)), 1)[0]], 1)[0]\n",
    "            if abbyIndex == 0:\n",
    "                targetClasses = [1,0]\n",
    "            else:\n",
    "                targetClasses = [0,1]\n",
    "        else:\n",
    "            assert len(adversaryList)==numClasses\n",
    "            abbyIndex = random.sample(range(len(adversaryList)), 1)[0]\n",
    "            targetClasses = [0]*numClasses\n",
    "            targetClasses[abbyIndex] = 1\n",
    "            \n",
    "        abbyOneHotSes = oneSessionEncoder(adversaryList[abbyIndex],\n",
    "                                          hexDict = hexDict,\n",
    "                                          packetReverse=packetReverse, \n",
    "                                          padOldTimeSteps = padOldTimeSteps, \n",
    "                                          maxPackets = maxPackets, \n",
    "                                          packetTimeSteps = packetTimeSteps)\n",
    "\n",
    "        trainingSessions.append(abbyOneHotSes[0])\n",
    "        trainingTargets.append(np.array(targetClasses, dtype=theano.config.floatX))\n",
    "\n",
    "    sessionsMinibatch = np.asarray(trainingSessions, dtype=theano.config.floatX)\\\n",
    "                                   .reshape((-1, packetTimeSteps, 1, dimIn))\n",
    "    targetsMinibatch = np.asarray(trainingTargets, dtype=theano.config.floatX)\n",
    "\n",
    "    predcostfun = predictFun(sessionsMinibatch)\n",
    "    testCollect.append(np.mean(np.argmax(predcostfun,axis=1) == np.argmax(targetsMinibatch, axis=1)))\n",
    "\n",
    "    predtargets = np.argmax(predcostfun,axis=1)\n",
    "    actualtargets = np.argmax(targetsMinibatch, axis=1)\n",
    "\n",
    "    print \"TEST accuracy:         \", np.mean(testCollect)\n",
    "    print\n",
    "\n",
    "    return predtargets, actualtargets, np.mean(testCollect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binaryPrecisionRecall(predictions, targets, numClasses = 4):\n",
    "    for cla in range(numClasses):\n",
    "        \n",
    "        confustop = np.array([])\n",
    "        confusbottom = np.array([])\n",
    "\n",
    "        predictions = np.asarray(predictions).flatten()\n",
    "        targets = np.asarray(targets).flatten()\n",
    "\n",
    "        pred1 = np.where(predictions == cla)\n",
    "        pred0 = np.where(predictions != cla)\n",
    "        target1 = np.where(targets == cla)\n",
    "        target0 = np.where(targets != cla)\n",
    "\n",
    "        truePos = np.intersect1d(pred1[0],target1[0]).shape[0]\n",
    "        trueNeg = np.intersect1d(pred0[0],target0[0]).shape[0]\n",
    "        falsePos = np.intersect1d(pred1[0],target0[0]).shape[0]\n",
    "        falseNeg = np.intersect1d(pred0[0],target1[0]).shape[0]\n",
    "\n",
    "        top = np.append(confustop, (truePos, falsePos))\n",
    "        bottom = np.append(confusbottom, (falseNeg, trueNeg))\n",
    "        confusionMatrix = np.vstack((top, bottom))\n",
    "        \n",
    "        precision  = float(truePos)/(truePos + falsePos + 0.00001) #1 - (how much junk did we give user)\n",
    "        recall = float(truePos)/(truePos + falseNeg + 0.00001) #1 - (how much good stuff did we miss)\n",
    "        f1 = 2*((precision*recall)/(precision+recall+0.00001))\n",
    "        \n",
    "        print 'class '+str(cla)+' precision: ', precision\n",
    "        print 'class '+str(cla)+' recall:    ', recall\n",
    "        print 'class '+str(cla)+' f1:        ', f1\n",
    "        print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised feature extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization for both the unsupervised net and the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def training(runname, rnnType, onlyEssentials, maxPackets, packetTimeSteps, packetReverse, padOldTimeSteps, wtstd, \n",
    "             lr, decay, clippings, dimIn, dim, binaryTarget, numClasses, batch_size, epochs, \n",
    "             trainPercent, dataPath, savePath, loadPrepedData = False):\n",
    "    print locals()\n",
    "    print\n",
    "    \n",
    "    X = T.tensor4('inputs')\n",
    "    Y = T.matrix('targets')\n",
    "    linewt_init = IsotropicGaussian(wtstd)\n",
    "    line_bias = Constant(1.0)\n",
    "    rnnwt_init = IsotropicGaussian(wtstd)\n",
    "    rnnbias_init = Constant(0.0)\n",
    "    classifierWts = IsotropicGaussian(wtstd)\n",
    "\n",
    "    learning_rate = theano.shared(np.array(lr, dtype=theano.config.floatX))\n",
    "    learning_decay = np.array(decay, dtype=theano.config.floatX)\n",
    "    \n",
    "    ###DATA PREP\n",
    "    print 'loading data'\n",
    "    if loadPrepedData:\n",
    "        hexSessions = loadFile(dataPath)\n",
    "\n",
    "    else:\n",
    "        sessioner = sessionizer.HexSessionizer(dataPath)\n",
    "        hexSessions = sessioner.read_pcap()\n",
    "    \n",
    "    hexSessions = sessionizer.removeBadSessionizer(hexSessions)\n",
    "    hexSessionsKeys = sessionizer.order_keys(hexSessions)\n",
    "    hexDict = hexTokenizer()\n",
    "    \n",
    "    print 'creating dictionary of ip communications'\n",
    "    comsDict, uniqIPs = srcIpDict(hexSessions)\n",
    "    comsDict = dictUniquerizer(comsDict)\n",
    "     \n",
    "    print 'initializing network graph'\n",
    "    ###ENCODER RNN\n",
    "    if rnnType == 'gru':\n",
    "        rnn = GatedRecurrent(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'gru')\n",
    "        dimMultiplier = 2\n",
    "    else:\n",
    "        rnn = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'lstm')\n",
    "        dimMultiplier = 4\n",
    "\n",
    "    fork = Fork(output_names=['linear', 'gates'],\n",
    "                name='fork', input_dim=dimIn, output_dims=[dim, dim * dimMultiplier], \n",
    "                weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "    ###CONTEXT RNN\n",
    "    if rnnType == 'gru':\n",
    "        rnnContext = GatedRecurrent(dim=dim, weights_init = rnnwt_init, \n",
    "                                    biases_init = rnnbias_init, name = 'gruContext')\n",
    "    else:\n",
    "        rnnContext = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, \n",
    "                          name = 'lstmContext')\n",
    "\n",
    "    forkContext = Fork(output_names=['linearContext', 'gatesContext'],\n",
    "                name='forkContext', input_dim=dim, output_dims=[dim, dim * dimMultiplier], \n",
    "                weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "    forkDec = Fork(output_names=['linear', 'gates'],\n",
    "                name='forkDec', input_dim=dim, output_dims=[dim, dim*dimMultiplier], \n",
    "                weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "    #CLASSIFIER\n",
    "    bmlp = BatchNormalizedMLP( activations=[Tanh(), Logistic()], \n",
    "               dims=[dim, dim, numClasses],\n",
    "               weights_init=classifierWts,\n",
    "               biases_init=Constant(0.0001) )\n",
    "\n",
    "    fork.initialize()\n",
    "    rnn.initialize()\n",
    "    forkContext.initialize()\n",
    "    rnnContext.initialize()\n",
    "    forkDec.initialize()\n",
    "    bmlp.initialize()\n",
    "\n",
    "    def onestepEnc(X):\n",
    "        data1, data2 = fork.apply(X) \n",
    "\n",
    "        if rnnType == 'gru':\n",
    "            hEnc = rnn.apply(data1, data2) \n",
    "        else:\n",
    "            hEnc, _ = rnn.apply(data2)\n",
    "\n",
    "        return hEnc\n",
    "\n",
    "    hEnc, _ = theano.scan(onestepEnc, X) \n",
    "    hEncReshape = T.reshape(hEnc[:,-1], (-1, maxPackets, 1, dim)) #[:,-1] takes the last rep for each packet\n",
    "                                                                 \n",
    "    def onestepContext(hEncReshape):\n",
    "\n",
    "        data3, data4 = forkContext.apply(hEncReshape)\n",
    "\n",
    "        if rnnType == 'gru':\n",
    "            hContext = rnnContext.apply(data3, data4)\n",
    "        else:\n",
    "            hContext, _ = rnnContext.apply(data4)\n",
    "\n",
    "        return hContext\n",
    "\n",
    "    hContext, _ = theano.scan(onestepContext, hEncReshape)\n",
    "    hContextReshape = T.reshape(hContext[:,-1], (-1,dim))\n",
    "\n",
    "    data5, _ = forkDec.apply(hContextReshape)\n",
    "    pyx = bmlp.apply(data5)\n",
    "    softmax = Softmax()\n",
    "    softoutClass = softmax.apply(pyx)\n",
    "    costClass = T.mean(CategoricalCrossEntropy().apply(Y, softoutClass))\n",
    "\n",
    "    #CREATE GRAPH\n",
    "    cgClass = ComputationGraph([costClass])\n",
    "    paramsClass = VariableFilter(roles = [PARAMETER])(cgClass.variables)\n",
    "    learning = learningfunctions.Learning(costClass,paramsClass,learning_rate,l1=0.,l2=0.,maxnorm=0.,c=clippings)\n",
    "    updatesClass = learning.Adam() \n",
    "\n",
    "    ###To check gradients for explosion/shrinkage\n",
    "    #gradients = T.grad(costClass, paramsClass)\n",
    "    #gradients = clip_norms(gradients, clippings)\n",
    "    #gradientFun = theano.function([X,Y], gradients, allow_input_downcast=True)\n",
    "\n",
    "    print 'compiling graph you talented soul'\n",
    "    classifierTrain = theano.function([X,Y], [costClass, softoutClass], \n",
    "                                      updates=updatesClass, allow_input_downcast=True)\n",
    "    classifierPredict = theano.function([X], softoutClass, allow_input_downcast=True)\n",
    "    print 'finished compiling'\n",
    "\n",
    "    trainIndex = int(len(hexSessionsKeys)*trainPercent)\n",
    "\n",
    "    epochCost = []\n",
    "    gradNorms = []\n",
    "    trainAcc = []\n",
    "    testAcc = []\n",
    "\n",
    "    costCollect = []\n",
    "    trainCollect = []\n",
    "\n",
    "    print 'training begins'\n",
    "    iteration = 0\n",
    "    for epoch in xrange(epochs):\n",
    "\n",
    "        #iteration/minibatch\n",
    "        for start, end in zip(range(0, trainIndex, batch_size),\n",
    "                              range(batch_size, trainIndex, batch_size)):\n",
    "\n",
    "            trainingTargets = []\n",
    "            trainingSessions = []\n",
    "            for trainKey in range(start, end):\n",
    "                sessionForEncoding = hexSessions[hexSessions.keys()[trainKey]][0]\n",
    "\n",
    "                adfun = adversarialfunctions.Adversary(sessionForEncoding)\n",
    "                adversaryList = [sessionForEncoding, \n",
    "                                 adfun.dstIpSwapOut(comsDict, uniqIPs),\n",
    "                                 adfun.portDirSwitcher(),\n",
    "                                 adfun.ipDirSwitcher(),\n",
    "                                 adfun.noisyPacketMaker(maxPackets, packetTimeSteps, percentNoisy = 0.2)]\n",
    "                \n",
    "                if binaryTarget:\n",
    "                    # choose normal and one of the abnormal types\n",
    "                    abbyIndex = random.sample([0, random.sample(xrange(1,len(adversaryList)), 1)[0]], 1)[0]\n",
    "                    if abbyIndex == 0:\n",
    "                        targetClasses = [1,0]\n",
    "                    else:\n",
    "                        targetClasses = [0,1]\n",
    "                else:\n",
    "                    assert len(adversaryList)==numClasses\n",
    "                    abbyIndex = random.sample(range(len(adversaryList)), 1)[0]\n",
    "                    targetClasses = [0]*numClasses\n",
    "                    targetClasses[abbyIndex] = 1\n",
    "                    \n",
    "                abbyOneHotSes = oneSessionEncoder(adversaryList[abbyIndex],\n",
    "                                                  hexDict = hexDict,\n",
    "                                                  packetReverse=packetReverse, \n",
    "                                                  padOldTimeSteps = padOldTimeSteps, \n",
    "                                                  maxPackets = maxPackets, \n",
    "                                                  packetTimeSteps = packetTimeSteps,\n",
    "                                                  onlyEssentials = onlyEssentials)\n",
    "\n",
    "                trainingSessions.append(abbyOneHotSes[0])\n",
    "                trainingTargets.append(np.array(targetClasses, dtype=theano.config.floatX))\n",
    "\n",
    "            sessionsMinibatch = np.asarray(trainingSessions).reshape((-1, packetTimeSteps, 1, dimIn))\n",
    "            targetsMinibatch = np.asarray(trainingTargets)\n",
    "\n",
    "            costfun = classifierTrain(sessionsMinibatch, targetsMinibatch)\n",
    "\n",
    "            costCollect.append(costfun[0])\n",
    "            trainCollect.append(np.mean(np.argmax(costfun[-1],axis=1) == np.argmax(targetsMinibatch, axis=1)))\n",
    "\n",
    "            iteration+=1\n",
    "\n",
    "            if iteration == 1:\n",
    "                print 'you are amazing'\n",
    "\n",
    "            # decay learning rate\n",
    "            #if iteration%500 == 0:\n",
    "            #    learning_rate.set_value(learning_rate.get_value() * learning_decay)\n",
    "            \n",
    "            # collect training stats\n",
    "            if iteration%200 == 0:\n",
    "                print '   Iteration: ', iteration\n",
    "                print '   Cost: ', np.mean(costCollect[-20:])\n",
    "                print '   TRAIN accuracy: ', np.mean(trainCollect[-20:])\n",
    "                print\n",
    "                \n",
    "                ###To check gradients for explosion/shrinkage\n",
    "                #grads = gradientFun(sessionsMinibatch, targetsMinibatch)\n",
    "                #for gra in grads:\n",
    "                #    print '  gradient norms: ', np.linalg.norm(gra)\n",
    "                \n",
    "                np.savetxt(savePath+runname+\"_TRAIN.csv\", trainCollect[::50], delimiter=\",\")\n",
    "                np.savetxt(savePath+runname+\"_COST.csv\", costCollect[::50], delimiter=\",\")\n",
    "\n",
    "            #testing accuracy\n",
    "            if iteration%500 == 0:\n",
    "                predtar, acttar, testCollect = predictClass(classifierPredict, hexSessions, comsDict, uniqIPs, hexDict,\n",
    "                                                            hexSessionsKeys, binaryTarget, numClasses, trainPercent, \n",
    "                                                            dimIn, maxPackets, packetTimeSteps, padOldTimeSteps)\n",
    "\n",
    "                binaryPrecisionRecall(predtar, acttar, numClasses)\n",
    "                testAcc.append(testCollect)\n",
    "                np.savetxt(savePath+runname+\"_TEST.csv\", testAcc, delimiter=\",\")\n",
    "\n",
    "            #save the models\n",
    "            if iteration%1500 == 0:\n",
    "                pickleFile(classifierTrain, filePath=savePath,\n",
    "                            fileName=runname+'TRAIN'+str(iteration))\n",
    "                pickleFile(classifierPredict, filePath=savePath,\n",
    "                            fileName=runname+'PREDICT'+str(iteration))\n",
    "\n",
    "        epochCost.append(np.mean(costCollect[-50:]))\n",
    "        trainAcc.append(np.mean(trainCollect[-50:]))\n",
    "        \n",
    "        print 'Epoch: ', epoch\n",
    "        #module_logger.debug('Epoch:%r',epoch)\n",
    "        print 'Epoch cost average: ', epochCost[-1]\n",
    "        print 'Epoch TRAIN accuracy: ', trainAcc[-1]\n",
    "    \n",
    "    return classifierTrain, classifierPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dim': 100, 'onlyEssentials': True, 'loadPrepedData': True, 'dimIn': 257, 'decay': 0.9, 'runname': 'noiseclassification3packsmall', 'padOldTimeSteps': True, 'packetTimeSteps': 16, 'dataPath': '/data/fs4/home/bradh/bigFlows.pickle', 'batch_size': 20, 'trainPercent': 0.9, 'epochs': 50, 'clippings': 1, 'savePath': '/data/fs4/home/bradh/outputs/', 'lr': 0.0001, 'numClasses': 2, 'packetReverse': False, 'binaryTarget': True, 'maxPackets': 3, 'wtstd': 0.2, 'rnnType': 'gru'}\n",
      "\n",
      "loading data\n",
      "creating dictionary of ip communications\n"
     ]
    }
   ],
   "source": [
    "#TODO: expose classifier dim\n",
    "train, predict = training(runname, rnnType, onlyEssentials, maxPackets, packetTimeSteps, packetReverse,\n",
    "                          padOldTimeSteps, wtstd, lr, decay, clippings, dimIn, dim, binaryTarget, numClasses,\n",
    "                          batch_size, epochs, trainPercent, dataPath, savePath, loadPrepedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
