{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled, CuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['THEANO_FLAGS'] = 'floatX=float32,device=gpu'\n",
    "\n",
    "import json\n",
    "import subprocess\n",
    "import cPickle\n",
    "import sys\n",
    "import binascii\n",
    "import multiprocessing as mp\n",
    "from itertools import chain\n",
    "from collections import OrderedDict\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import copy\n",
    "\n",
    "import blocks\n",
    "from blocks.bricks import Linear, Softmax, Softplus, NDimensionalSoftmax,\\\n",
    "                            BatchNormalizedMLP,Rectifier, Logistic, Tanh, MLP\n",
    "from blocks.bricks.recurrent import GatedRecurrent, Fork, LSTM\n",
    "from blocks.initialization import Constant, IsotropicGaussian, Identity, Uniform\n",
    "from blocks.bricks.cost import BinaryCrossEntropy, CategoricalCrossEntropy\n",
    "from blocks.filter import VariableFilter\n",
    "from blocks.roles import PARAMETER\n",
    "from blocks.graph import ComputationGraph\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "\n",
    "sys.setrecursionlimit(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_header(line):\n",
    "    ret_dict = {}\n",
    "    h = line.split()\n",
    "    #ret_dict['direction'] = \" \".join(h[3:6])\n",
    "    if h[2] == 'IP6':\n",
    "        \"\"\"\n",
    "        Conditional formatting based on ethernet type.\n",
    "        IPv4 format: 0.0.0.0.port\n",
    "        IPv6 format (one of many): 0:0:0:0:0:0.port\n",
    "        \"\"\"\n",
    "        ret_dict['src_port'] = h[3].split('.')[-1]\n",
    "        ret_dict['src_ip'] = h[3].split('.')[0]\n",
    "        ret_dict['dest_port'] = h[5].split('.')[-1].split(':')[0]\n",
    "        ret_dict['dest_ip'] = h[5].split('.')[0]\n",
    "    else:\n",
    "        if len(h[3].split('.')) > 4:\n",
    "            ret_dict['src_port'] = h[3].split('.')[-1]\n",
    "            ret_dict['src_ip'] = '.'.join(h[3].split('.')[:-1])\n",
    "        else:\n",
    "            ret_dict['src_ip'] = h[3]\n",
    "            ret_dict['src_port'] = ''\n",
    "        if len(h[5].split('.')) > 4:\n",
    "            ret_dict['dest_port'] = h[5].split('.')[-1].split(':')[0]\n",
    "            ret_dict['dest_ip'] = '.'.join(h[5].split('.')[:-1])\n",
    "        else:\n",
    "            ret_dict['dest_ip'] = h[5].split(':')[0]\n",
    "            ret_dict['dest_port'] = ''\n",
    "    return ret_dict\n",
    "\n",
    "def parse_data(line):\n",
    "    ret_str = ''\n",
    "    h, d = line.split(':', 1)\n",
    "    ret_str = d.strip().replace(' ', '')\n",
    "    return ret_str\n",
    "\n",
    "def process_packet(output):\n",
    "    # TODO!! throws away the first packet!\n",
    "    ret_header = {}\n",
    "    ret_dict = {}\n",
    "    ret_data = ''\n",
    "    hasHeader = False\n",
    "    for line in output:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            if not line.startswith('0x'):\n",
    "                # header line\n",
    "                if ret_dict and ret_data:\n",
    "                    # about to start new header, finished with hex\n",
    "                    ret_dict['data'] = ret_data\n",
    "                    yield ret_dict\n",
    "                    ret_dict.clear()\n",
    "                    ret_header.clear()\n",
    "                    ret_data = ''\n",
    "                    hasHeader = False\n",
    "                    \n",
    "                # parse next header    \n",
    "                try:\n",
    "                    ret_header = parse_header(line)\n",
    "                    ret_dict.update(ret_header)\n",
    "                    hasHeader = True\n",
    "                except:\n",
    "                    ret_header.clear()\n",
    "                    ret_dict.clear()\n",
    "                    ret_data = ''\n",
    "                    hasHeader = False\n",
    "                    \n",
    "            else:\n",
    "                # hex data line\n",
    "                if hasHeader:\n",
    "                    data = parse_data(line)\n",
    "                    ret_data = ret_data + data\n",
    "                else:\n",
    "                    continue\n",
    "    \n",
    "def is_clean_packet(packet):\n",
    "    \"\"\"\n",
    "    Returns whether or not the parsed packet is valid\n",
    "    or not. Checks that both the src and dest\n",
    "    ports are integers. Checks that src and dest IPs\n",
    "    are valid address formats. Checks that packet data\n",
    "    is hex. Returns True if all tests pass, False otherwise.\n",
    "    \"\"\"\n",
    "    if not packet['src_port'].isdigit(): return False\n",
    "    if not packet['dest_port'].isdigit(): return False\n",
    "    \n",
    "    if packet['src_ip'].isalpha(): return False\n",
    "    if packet['dest_ip'].isalpha(): return False\n",
    "    #try:\n",
    "    #    ipaddress.ip_address(packet['src_ip'])\n",
    "    #    ipaddress.ip_address(packet['dest_ip'])\n",
    "    #except:\n",
    "    #    return False\n",
    "     \n",
    "    if 'data' in packet:\n",
    "        try:\n",
    "            int(packet['data'], 16)\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def order_keys(hexSessionDict):\n",
    "    orderedKeys = []\n",
    "    \n",
    "    for key in sorted(hexSessionDict.keys(), key=lambda key: hexSessionDict[key][1]):\n",
    "        orderedKeys.append(key) \n",
    "        \n",
    "    return orderedKeys\n",
    "\n",
    "def read_pcap(path):\n",
    "    hex_sessions = {}\n",
    "    proc = subprocess.Popen('tcpdump -nn -tttt -xx -r '+path,\n",
    "                            shell=True,\n",
    "                            stdout=subprocess.PIPE)\n",
    "    insert_num = 0  # keeps track of insertion order into dict\n",
    "    for packet in process_packet(proc.stdout):\n",
    "        if not is_clean_packet(packet):\n",
    "            continue\n",
    "        if 'data' in packet:\n",
    "            key = (packet['src_ip']+\":\"+packet['src_port'], packet['dest_ip']+\":\"+packet['dest_port'])\n",
    "            rev_key = (key[1], key[0])\n",
    "            if key in hex_sessions:\n",
    "                hex_sessions[key][0].append(packet['data'])\n",
    "            elif rev_key in hex_sessions:\n",
    "                hex_sessions[rev_key][0].append(packet['data'])\n",
    "            else:\n",
    "                hex_sessions[key] = ([packet['data']], insert_num)\n",
    "                insert_num += 1\n",
    "        \n",
    "    return hex_sessions\n",
    "\n",
    "def pickleFile(thing2save, file2save2 = None, filePath='/work/notebooks/drawModels/', fileName = 'myModels'):\n",
    "    \n",
    "    if file2save2 == None:\n",
    "        f=file(filePath+fileName+'.pickle', 'wb')\n",
    "    else:\n",
    "        f=file(filePath+file2save2, 'wb')\n",
    "        \n",
    "    cPickle.dump(thing2save, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    f.close()\n",
    "    \n",
    "def loadFile(filePath):\n",
    "    file2open = file(filePath, 'rb')\n",
    "    loadedFile = cPickle.load(file2open)\n",
    "    file2open.close()\n",
    "    \n",
    "    return loadedFile\n",
    "\n",
    "def removeBadSessionizer(hexSessionDict, saveFile=False, dataPath=None, fileName=None):\n",
    "    for ses in hexSessionDict.keys():\n",
    "        paclens = []\n",
    "        for pac in hexSessionDict[ses][0]:\n",
    "            paclens.append(len(pac))\n",
    "        if np.min(paclens)<80:\n",
    "            del hexSessionDict[ses]\n",
    "\n",
    "    if saveFile:\n",
    "        print 'pickling sessions dictionary... mmm'\n",
    "        pickleFile(hexSessionDict, filePath=dataPath, fileName=fileName)\n",
    "        \n",
    "        #with open(dataPath+'/'+fileName+'.pickle', 'wb') as handle:\n",
    "        #    cPickle.dump(hexSessions, handle)\n",
    "            \n",
    "    return hexSessionDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataPath = '/data/fs4/datasets/pcaps/gregPcaps/'\n",
    "dirList = os.listdir('/data/fs4/datasets/pcaps/gregPcaps/')\n",
    "dirList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "complicated = {}\n",
    "\n",
    "for capture in dirList:\n",
    "    dictName = capture.split('.')[0]\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    hexSessions = read_pcap(dataPath+capture)\n",
    "    hexSessions = removeBadSessionizer(hexSessions)\n",
    "    complicated[dictName] = hexSessions\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    print dictName + '  is done'\n",
    "    print 'time to run (secs): ', (end - start)\n",
    "    \n",
    "    \n",
    "pickleFile(complicated, filePath='/data/fs4/home/bradh/', fileName='complicated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#big file\n",
    "with open('complicated.pickle', 'rb') as unhandle:\n",
    "     compDict= cPickle.load(unhandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b-dc-24hrs-96bytes-E-FT-SRV-FW1A-2016-08-09_18-16-vlan210    116427\n",
      "NESTthermostat-nf-10days-96bytes    4933\n",
      "a-printers-24hrs-96bytes-E-VA-SRV-FW1A-2016-08-09_14-17-vlan34    22029\n",
      "SonySmartTV-nf-10days-96bytes    147236\n",
      "a-fs-24hrs-96bytes-E-ASH-SRV-FW1A-2016-08-09_18-17-vlan40    9250\n",
      "TiVoSeries4-nf-10days-96bytes    2121\n",
      "b-dc-24hrs-96bytes-E-QD-SRV-FW1A-2016-08-09_18-16-vlan210    365186\n",
      "b-dc-24hrs-96bytes-E-QD-SRV-FW1A-2016-08-09_18-16-vlan294    86524\n",
      "753706\n"
     ]
    }
   ],
   "source": [
    "sess = 0\n",
    "for di in compDict.keys():\n",
    "    sess += len(compDict[di].keys())\n",
    "    print di, \"  \", len(compDict[di].keys())\n",
    "\n",
    "print sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "\n",
    "maxPackets = 2\n",
    "packetTimeSteps = 80\n",
    "loadPrepedData = True\n",
    "dataPath = '/data/fs4/home/bradh/bigFlows.pickle'\n",
    "\n",
    "packetReverse = False\n",
    "padOldTimeSteps = True\n",
    "\n",
    "runname = 'hredClassify2smallpackets'\n",
    "rnnType = 'gru' #gru or lstm\n",
    "\n",
    "wtstd = 0.2\n",
    "dimIn = 257 #hex has 256 characters + the <EOP> character\n",
    "dim = 100 #dimension reduction size\n",
    "batch_size = 20\n",
    "numClasses = 6\n",
    "clippings = 1\n",
    "\n",
    "epochs = 500\n",
    "lr = 0.0001\n",
    "decay = 0.9\n",
    "trainPercent = 0.8\n",
    "\n",
    "module_logger = logging.getLogger(__name__)\n",
    "\n",
    "import ast\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "\n",
    "def pickleFile(thing2save, file2save2 = None, filePath='/work/notebooks/drawModels/', fileName = 'myModels'):\n",
    "    \n",
    "    if file2save2 == None:\n",
    "        f=file(filePath+fileName+'.pickle', 'wb')\n",
    "    else:\n",
    "        f=file(filePath+file2save2, 'wb')\n",
    "        \n",
    "    cPickle.dump(thing2save, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    f.close()\n",
    "    \n",
    "def loadFile(filePath):\n",
    "    file2open = file(filePath, 'rb')\n",
    "    loadedFile = cPickle.load(file2open)\n",
    "    file2open.close()\n",
    "    \n",
    "    return loadedFile\n",
    "\n",
    "\n",
    "def removeBadSessionizer(hexSessionDict, saveFile=False, dataPath=None, fileName=None):\n",
    "    for ses in hexSessionDict.keys():\n",
    "        paclens = []\n",
    "        for pac in hexSessionDict[ses][0]:\n",
    "            paclens.append(len(pac))\n",
    "        if np.min(paclens)<80:\n",
    "            del hexSessionDict[ses]\n",
    "\n",
    "    if saveFile:\n",
    "        print 'pickling sessions'\n",
    "        pickleFile(hexSessionDict, filePath=dataPath, fileName=fileName)\n",
    "        \n",
    "    return hexSessionDict\n",
    "\n",
    "\n",
    "#Making the hex dictionary\n",
    "\n",
    "#def dstPortSwapOneOut(hexSessionList):\n",
    "    #THINK THROUGH    \n",
    "\n",
    "def oneHot(index, granular = 'hex'):\n",
    "    if granular == 'hex':\n",
    "        vecLen = 257\n",
    "    else:\n",
    "        vecLen = 17\n",
    "    \n",
    "    zeroVec = np.zeros(vecLen)\n",
    "    zeroVec[index] = 1.0\n",
    "    \n",
    "    return zeroVec\n",
    "\n",
    "\n",
    "def oneSessionEncoder(sessionPackets, hexDict, maxPackets = 2, packetTimeSteps = 100,\n",
    "                       packetReverse = False, charLevel = False, padOldTimeSteps = True):    \n",
    "            \n",
    "    sessionCollect = []\n",
    "    packetCollect = []\n",
    "    \n",
    "    if charLevel:\n",
    "        vecLen = 17\n",
    "    else:\n",
    "        vecLen = 257\n",
    "    \n",
    "    if len(sessionPackets) > maxPackets: #crop the number of sessions to maxPackets\n",
    "        sessionList = copy(sessionPackets[:maxPackets])\n",
    "    else:\n",
    "        sessionList = copy(sessionPackets)\n",
    "\n",
    "    for rawpacket in sessionList:\n",
    "        packet = copy(rawpacket)\n",
    "        packet = packet[32:36]+packet[44:46]+packet[46:48]+packet[52:60]+packet[60:68]+                 packet[68:70]+packet[70:72]+packet[72:74]\n",
    "        packet = [hexDict[packet[i:i+2]] for i in xrange(0,len(packet)-2+1,2)]\n",
    "            \n",
    "        if len(packet) >= packetTimeSteps: #crop packet to length packetTimeSteps\n",
    "            packet = packet[:packetTimeSteps]\n",
    "            packet = packet+[256] #add <EOP> end of packet token\n",
    "        else:\n",
    "            packet = packet+[256] #add <EOP> end of packet token\n",
    "        \n",
    "        packetCollect.append(packet)\n",
    "        \n",
    "        pacMat = np.array([oneHot(x) for x in packet]) #one hot encoding of packet into a matrix\n",
    "        pacMatLen = len(pacMat)\n",
    "        \n",
    "        #padding packet\n",
    "        if packetReverse:\n",
    "            pacMat = pacMat[::-1]\n",
    "\n",
    "        if pacMatLen < packetTimeSteps:\n",
    "            #pad by stacking zeros on top of data so that earlier timesteps do not have information\n",
    "            #padding the packet such that zeros are after the actual info for better translation\n",
    "            if padOldTimeSteps:\n",
    "                pacMat = np.vstack( ( np.zeros((packetTimeSteps-pacMatLen,vecLen)), pacMat) ) \n",
    "            else:\n",
    "                pacMat = np.vstack( (pacMat, np.zeros((packetTimeSteps-pacMatLen,vecLen))) ) \n",
    "\n",
    "        if pacMatLen > packetTimeSteps:\n",
    "            pacMat = pacMat[:packetTimeSteps, :]\n",
    "\n",
    "        sessionCollect.append(pacMat)\n",
    "\n",
    "    #padding session\n",
    "    sessionCollect = np.asarray(sessionCollect, dtype=theano.config.floatX)\n",
    "    numPacketsInSession = sessionCollect.shape[0]\n",
    "    if numPacketsInSession < maxPackets:\n",
    "        #pad sessions to fit the \n",
    "        sessionCollect = np.vstack( (sessionCollect,np.zeros((maxPackets-numPacketsInSession, \n",
    "                                                             packetTimeSteps, vecLen))) )\n",
    "    \n",
    "    return sessionCollect, packetCollect\n",
    "\n",
    "\n",
    "# # Learning functions\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def dropout(X, p=0.):\n",
    "    if p != 0:\n",
    "        retain_prob = 1 - p\n",
    "        X = X / retain_prob * srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX)\n",
    "    return X\n",
    "\n",
    "# Gradient clipping\n",
    "def clip_norm(g, c, n): \n",
    "    '''n is the norm, c is the threashold, and g is the gradient'''\n",
    "    \n",
    "    if c > 0: \n",
    "        g = T.switch(T.ge(n, c), g*c/n, g) \n",
    "    return g\n",
    "\n",
    "def clip_norms(gs, c):\n",
    "    norm = T.sqrt(sum([T.sum(g**2) for g in gs]))\n",
    "    return [clip_norm(g, c, norm) for g in gs]\n",
    "\n",
    "# Regularizers\n",
    "def max_norm(p, maxnorm = 0.):\n",
    "    if maxnorm > 0:\n",
    "        norms = T.sqrt(T.sum(T.sqr(p), axis=0))\n",
    "        desired = T.clip(norms, 0, maxnorm)\n",
    "        p = p * (desired/ (1e-7 + norms))\n",
    "    return p\n",
    "\n",
    "def gradient_regularize(p, g, l1 = 0., l2 = 0.):\n",
    "    g += p * l2\n",
    "    g += T.sgn(p) * l1\n",
    "    return g\n",
    "\n",
    "def weight_regularize(p, maxnorm = 0.):\n",
    "    p = max_norm(p, maxnorm)\n",
    "    return p\n",
    "\n",
    "def Adam(params, cost, lr=0.0002, b1=0.1, b2=0.001, e=1e-8, l1 = 0., l2 = 0., maxnorm = 0., c = 8):\n",
    "    \n",
    "    updates = []\n",
    "    grads = T.grad(cost, params)\n",
    "    grads = clip_norms(grads, c)\n",
    "    \n",
    "    i = theano.shared(floatX(0.))\n",
    "    i_t = i + 1.\n",
    "    fix1 = 1. - b1**(i_t)\n",
    "    fix2 = 1. - b2**(i_t)\n",
    "    lr_t = lr * (T.sqrt(fix2) / fix1)\n",
    "    \n",
    "    for p, g in zip(params, grads):\n",
    "        m = theano.shared(p.get_value() * 0.)\n",
    "        v = theano.shared(p.get_value() * 0.)\n",
    "        m_t = (b1 * g) + ((1. - b1) * m)\n",
    "        v_t = (b2 * T.sqr(g)) + ((1. - b2) * v)\n",
    "        g_t = m_t / (T.sqrt(v_t) + e)\n",
    "        g_t = gradient_regularize(p, g_t, l1=l1, l2=l2)\n",
    "        p_t = p - (lr_t * g_t)\n",
    "        p_t = weight_regularize(p_t, maxnorm=maxnorm)\n",
    "        \n",
    "        updates.append((m, m_t))\n",
    "        updates.append((v, v_t))\n",
    "        updates.append((p, p_t))\n",
    "    \n",
    "    updates.append((i, i_t))\n",
    "    \n",
    "    return updates\n",
    "\n",
    "def RMSprop(cost, params, lr = 0.001, l1 = 0., l2 = 0., maxnorm = 0., rho=0.9, epsilon=1e-6, c = 8):\n",
    "    \n",
    "    grads = T.grad(cost, params)\n",
    "    grads = clip_norms(grads, c)\n",
    "    updates = []\n",
    "    \n",
    "    for p, g in zip(params, grads):\n",
    "        g = gradient_regularize(p, g, l1 = l1, l2 = l2)\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        updates.append((acc, acc_new))\n",
    "        \n",
    "        updated_p = p - lr * (g / T.sqrt(acc_new + epsilon))\n",
    "        updated_p = weight_regularize(updated_p, maxnorm = maxnorm)\n",
    "        updates.append((p, updated_p))\n",
    "    return updates\n",
    "\n",
    "\n",
    "# # Training functions\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "def predictClass(predictFun, hexSessionsDict, comsDict, uniqIPs, hexDict, hexSessionsKeys,\n",
    "                 sampleList, numClasses = 4, trainPercent = 0.9, dimIn=257, maxPackets=2,\n",
    "                 packetTimeSteps = 16, padOldTimeSteps=True, epochs = 300):\n",
    "    \n",
    "    testCollect = []\n",
    "    predtargets = []\n",
    "    actualtargets = []\n",
    "        \n",
    "    start = trainIndex\n",
    "    end = len(hexSessionsKeys)\n",
    "        \n",
    "    trainingSessions = []\n",
    "    trainingTargets = []\n",
    "    \n",
    "    for epoch in xrange(epochs):\n",
    "\n",
    "        #iteration/minibatch\n",
    "        #for start, end in zip(range(0, trainIndex,batch_size),\n",
    "        #                      range(batch_size, trainIndex, batch_size)):\n",
    "\n",
    "        #    trainingTargets = []\n",
    "        #    trainingSessions = []\n",
    "\n",
    "            #create one minibatch with 0.5 normal and 0.5 abby normal traffic\n",
    "        for d in range(len(sampleList)):\n",
    "            sampleLen = len(compDict[sampleList[d]].keys())\n",
    "            sampleKeys = random.sample(compDict[sampleList[d]].keys()[:sampleLen]\n",
    "            for key in sampleKeys:\n",
    "                oneEncoded = oneSessionEncoder(compDict[sampleList[d]][key][0],\n",
    "                                                          hexDict = hexDict,\n",
    "                                                          packetReverse=packetReverse, \n",
    "                                                          padOldTimeSteps = padOldTimeSteps, \n",
    "                                                          maxPackets = maxPackets, \n",
    "                                                          packetTimeSteps = packetTimeSteps)\n",
    "                trainingSessions.append(oneEncoded)\n",
    "                trainIndex = [0]*numClasses\n",
    "                trainIndex[d] = 1\n",
    "                trainingTargets.append(trainIndex)\n",
    "\n",
    "    sessionsMinibatch = np.asarray(trainingSessions, dtype=theano.config.floatX).reshape((-1, packetTimeSteps, 1, dimIn))\n",
    "    targetsMinibatch = np.asarray(trainingTargets, dtype=theano.config.floatX)\n",
    "\n",
    "    predcostfun = predictFun(sessionsMinibatch)\n",
    "    testCollect.append(np.mean(np.argmax(predcostfun,axis=1) == np.argmax(targetsMinibatch, axis=1)))\n",
    "\n",
    "    predtargets = np.argmax(predcostfun,axis=1)\n",
    "    actualtargets = np.argmax(targetsMinibatch, axis=1)\n",
    "\n",
    "    print \"TEST accuracy:         \", np.mean(testCollect)\n",
    "    print\n",
    "\n",
    "    return predtargets, actualtargets, np.mean(testCollect)\n",
    "\n",
    "\n",
    "def binaryPrecisionRecall(predictions, targets, numClasses = 4):\n",
    "    for cla in range(numClasses):\n",
    "        \n",
    "        confustop = np.array([])\n",
    "        confusbottom = np.array([])\n",
    "\n",
    "        predictions = np.asarray(predictions).flatten()\n",
    "        targets = np.asarray(targets).flatten()\n",
    "\n",
    "        pred1 = np.where(predictions == cla)\n",
    "        pred0 = np.where(predictions != cla)\n",
    "        target1 = np.where(targets == cla)\n",
    "        target0 = np.where(targets != cla)\n",
    "\n",
    "        truePos = np.intersect1d(pred1[0],target1[0]).shape[0]\n",
    "        trueNeg = np.intersect1d(pred0[0],target0[0]).shape[0]\n",
    "        falsePos = np.intersect1d(pred1[0],target0[0]).shape[0]\n",
    "        falseNeg = np.intersect1d(pred0[0],target1[0]).shape[0]\n",
    "\n",
    "        top = np.append(confustop, (truePos, falsePos))\n",
    "        bottom = np.append(confusbottom, (falseNeg, trueNeg))\n",
    "        confusionMatrix = np.vstack((top, bottom))\n",
    "        \n",
    "        precision  = float(truePos)/(truePos + falsePos + 0.00001) #1 - (how much junk did we give user)\n",
    "        recall = float(truePos)/(truePos + falseNeg + 0.00001) #1 - (how much good stuff did we miss)\n",
    "        f1 = 2*((precision*recall)/(precision+recall+0.00001))\n",
    "        \n",
    "        print 'class '+str(cla)+' precision: ', precision\n",
    "        print 'class '+str(cla)+' recall:    ', recall\n",
    "        print 'class '+str(cla)+' f1:        ', f1\n",
    "        print\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b-dc-24hrs-96bytes-E-FT-SRV-FW1A-2016-08-09_18-16-vlan210',\n",
       " 'NESTthermostat-nf-10days-96bytes',\n",
       " 'a-printers-24hrs-96bytes-E-VA-SRV-FW1A-2016-08-09_14-17-vlan34',\n",
       " 'SonySmartTV-nf-10days-96bytes',\n",
       " 'a-fs-24hrs-96bytes-E-ASH-SRV-FW1A-2016-08-09_18-17-vlan40',\n",
       " 'TiVoSeries4-nf-10days-96bytes',\n",
       " 'b-dc-24hrs-96bytes-E-QD-SRV-FW1A-2016-08-09_18-16-vlan210',\n",
       " 'b-dc-24hrs-96bytes-E-QD-SRV-FW1A-2016-08-09_18-16-vlan294']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#b-dc-24hrs-96bytes-E-FT-SRV-FW1A-2016-08-09_18-16-vlan210    116427\n",
    "NESTthermostat-nf-10days-96bytes    4933\n",
    "a-printers-24hrs-96bytes-E-VA-SRV-FW1A-2016-08-09_14-17-vlan34    22029\n",
    "SonySmartTV-nf-10days-96bytes    147236\n",
    "a-fs-24hrs-96bytes-E-ASH-SRV-FW1A-2016-08-09_18-17-vlan40    9250\n",
    "TiVoSeries4-nf-10days-96bytes    2121\n",
    "b-dc-24hrs-96bytes-E-QD-SRV-FW1A-2016-08-09_18-16-vlan210    365186\n",
    "#b-dc-24hrs-96bytes-E-QD-SRV-FW1A-2016-08-09_18-16-vlan294    86524"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampleList = ['NESTthermostat-nf-10days-96bytes',\n",
    " 'a-printers-24hrs-96bytes-E-VA-SRV-FW1A-2016-08-09_14-17-vlan34',\n",
    " 'SonySmartTV-nf-10days-96bytes',\n",
    " 'a-fs-24hrs-96bytes-E-ASH-SRV-FW1A-2016-08-09_18-17-vlan40',\n",
    " 'TiVoSeries4-nf-10days-96bytes',\n",
    " 'b-dc-24hrs-96bytes-E-QD-SRV-FW1A-2016-08-09_18-16-vlan210']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hexDict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-5238ab6315d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msampleKeys\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         oneEncoded = oneSessionEncoder(compDict[sampleList[d]][key][0],\n\u001b[1;32m----> 8\u001b[1;33m                                                   \u001b[0mhexDict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhexDict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m                                                   \u001b[0mpacketReverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpacketReverse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                                                   \u001b[0mpadOldTimeSteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpadOldTimeSteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hexDict' is not defined"
     ]
    }
   ],
   "source": [
    "trainingSessions=[]\n",
    "trainingTargets=[]\n",
    "\n",
    "for d in range(len(sampleList)):\n",
    "    sampleKeys = random.sample(compDict[sampleList[d]].keys(), 5)\n",
    "    for key in sampleKeys:\n",
    "        oneEncoded = oneSessionEncoder(compDict[sampleList[d]][key][0],\n",
    "                                                  hexDict = hexDict,\n",
    "                                                  packetReverse=packetReverse, \n",
    "                                                  padOldTimeSteps = padOldTimeSteps, \n",
    "                                                  maxPackets = maxPackets, \n",
    "                                                  packetTimeSteps = packetTimeSteps)\n",
    "        trainingSessions.append(oneEncoded)\n",
    "        trainIndex = [0]*len(sampleList)\n",
    "        trainIndex[d] = 1\n",
    "        trainingTargets.append(trainIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(trainingSessions[0:2]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 0, 1]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def training(runname, rnnType, maxPackets, packetTimeSteps, packetReverse, padOldTimeSteps, wtstd, \n",
    "             lr, decay, clippings, dimIn, dim, numClasses, batch_size, epochs, \n",
    "             trainPercent):\n",
    "    print locals()\n",
    "    print\n",
    "    \n",
    "    X = T.tensor4('inputs')\n",
    "    Y = T.matrix('targets')\n",
    "    linewt_init = IsotropicGaussian(wtstd)\n",
    "    line_bias = Constant(1.0)\n",
    "    rnnwt_init = IsotropicGaussian(wtstd)\n",
    "    rnnbias_init = Constant(0.0)\n",
    "    classifierWts = IsotropicGaussian(wtstd)\n",
    "\n",
    "    learning_rateClass = theano.shared(np.array(lr, dtype=theano.config.floatX))\n",
    "    learning_decay = np.array(decay, dtype=theano.config.floatX)\n",
    "    \n",
    "    ###DATA PREP\n",
    "     \n",
    "    print 'initializing network graph'\n",
    "    ###ENCODER\n",
    "    if rnnType == 'gru':\n",
    "        rnn = GatedRecurrent(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'gru')\n",
    "        dimMultiplier = 2\n",
    "    else:\n",
    "        rnn = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'lstm')\n",
    "        dimMultiplier = 4\n",
    "\n",
    "    fork = Fork(output_names=['linear', 'gates'],\n",
    "                name='fork', input_dim=dimIn, output_dims=[dim, dim * dimMultiplier], \n",
    "                weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "    ###CONTEXT\n",
    "    if rnnType == 'gru':\n",
    "        rnnContext = GatedRecurrent(dim=dim, weights_init = rnnwt_init, \n",
    "                                    biases_init = rnnbias_init, name = 'gruContext')\n",
    "    else:\n",
    "        rnnContext = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, \n",
    "                          name = 'lstmContext')\n",
    "\n",
    "    forkContext = Fork(output_names=['linearContext', 'gatesContext'],\n",
    "                name='forkContext', input_dim=dim, output_dims=[dim, dim * dimMultiplier], \n",
    "                weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "    forkDec = Fork(output_names=['linear', 'gates'],\n",
    "                name='forkDec', input_dim=dim, output_dims=[dim, dim*dimMultiplier], \n",
    "                weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "    #CLASSIFIER\n",
    "    bmlp = BatchNormalizedMLP( activations=[Logistic(),Logistic()], \n",
    "               dims=[dim, dim, numClasses],\n",
    "               weights_init=classifierWts,\n",
    "               biases_init=Constant(0.0001) )\n",
    "\n",
    "    #initialize the weights in all the functions\n",
    "    fork.initialize()\n",
    "    rnn.initialize()\n",
    "    forkContext.initialize()\n",
    "    rnnContext.initialize()\n",
    "    forkDec.initialize()\n",
    "    bmlp.initialize()\n",
    "\n",
    "    def onestepEnc(X):\n",
    "        data1, data2 = fork.apply(X) \n",
    "\n",
    "        if rnnType == 'gru':\n",
    "            hEnc = rnn.apply(data1, data2) \n",
    "        else:\n",
    "            hEnc, _ = rnn.apply(data2)\n",
    "\n",
    "        return hEnc\n",
    "\n",
    "    hEnc, _ = theano.scan(onestepEnc, X) #(mini*numPackets, packetLen, 1, hexdictLen)\n",
    "    hEncReshape = T.reshape(hEnc[:,-1], (-1, maxPackets, 1, dim)) #[:,-1] takes the last rep for each packet\n",
    "                                                                 #(mini, numPackets, 1, dimReduced)\n",
    "    def onestepContext(hEncReshape):\n",
    "\n",
    "        data3, data4 = forkContext.apply(hEncReshape)\n",
    "\n",
    "        if rnnType == 'gru':\n",
    "            hContext = rnnContext.apply(data3, data4)\n",
    "        else:\n",
    "            hContext, _ = rnnContext.apply(data4)\n",
    "\n",
    "        return hContext\n",
    "\n",
    "    hContext, _ = theano.scan(onestepContext, hEncReshape)\n",
    "    hContextReshape = T.reshape(hContext[:,-1], (-1,dim))\n",
    "\n",
    "    data5, _ = forkDec.apply(hContextReshape)\n",
    "\n",
    "    pyx = bmlp.apply(data5)\n",
    "    softmax = Softmax()\n",
    "    softoutClass = softmax.apply(pyx)\n",
    "    costClass = T.mean(CategoricalCrossEntropy().apply(Y, softoutClass))\n",
    "\n",
    "    #CREATE GRAPH\n",
    "    cgClass = ComputationGraph([costClass])\n",
    "    paramsClass = VariableFilter(roles = [PARAMETER])(cgClass.variables)\n",
    "    updatesClass = Adam(paramsClass, costClass, learning_rateClass, c=clippings) \n",
    "    #updatesClass = RMSprop(costClass, paramsClass, learning_rateClass, c=clippings)\n",
    "\n",
    "    #print 'grad compiling'\n",
    "    #gradients = T.grad(costClass, paramsClass)\n",
    "    #gradients = clip_norms(gradients, clippings)\n",
    "    #gradientFun = theano.function([X,Y], gradients, allow_input_downcast=True)\n",
    "    #print 'finish with grads'\n",
    "\n",
    "    print 'compiling graph you talented soul'\n",
    "    classifierTrain = theano.function([X,Y], [costClass, hEnc, hContext, pyx, softoutClass], \n",
    "                                      updates=updatesClass, allow_input_downcast=True)\n",
    "    classifierPredict = theano.function([X], softoutClass, allow_input_downcast=True)\n",
    "    print 'finished compiling'\n",
    "\n",
    "    #trainIndex = int(len(hexSessionsKeys)*trainPercent)\n",
    "\n",
    "    epochCost = []\n",
    "    gradNorms = []\n",
    "    trainAcc = []\n",
    "    testAcc = []\n",
    "\n",
    "    costCollect = []\n",
    "    trainCollect = []\n",
    "\n",
    "    print 'training begins'\n",
    "    iteration = 0\n",
    "    #epoch\n",
    "    for epoch in xrange(epochs):\n",
    "\n",
    "        #iteration/minibatch\n",
    "        #for start, end in zip(range(0, trainIndex,batch_size),\n",
    "        #                      range(batch_size, trainIndex, batch_size)):\n",
    "\n",
    "        #    trainingTargets = []\n",
    "        #    trainingSessions = []\n",
    "\n",
    "            #create one minibatch with 0.5 normal and 0.5 abby normal traffic\n",
    "        for d in range(len(sampleList)):\n",
    "            sampleLen = len(compDict[sampleList[d]].keys())\n",
    "            sampleKeys = random.sample(compDict[sampleList[d]].keys()[:sampleLen], 5)\n",
    "            for key in sampleKeys:\n",
    "                oneEncoded = oneSessionEncoder(compDict[sampleList[d]][key][0],\n",
    "                                                          hexDict = hexDict,\n",
    "                                                          packetReverse=packetReverse, \n",
    "                                                          padOldTimeSteps = padOldTimeSteps, \n",
    "                                                          maxPackets = maxPackets, \n",
    "                                                          packetTimeSteps = packetTimeSteps)\n",
    "                trainingSessions.append(oneEncoded)\n",
    "                trainIndex = [0]*numClasses\n",
    "                trainIndex[d] = 1\n",
    "                trainingTargets.append(trainIndex)\n",
    "\n",
    "\n",
    "        '''for trainKey in range(start, end):\n",
    "            sessionForEncoding = list(hexSessions[hexSessions.keys()[trainKey]][0])\n",
    "\n",
    "            adversaryList = [sessionForEncoding, \n",
    "                             dstIpSwapOut(sessionForEncoding, comsDict, uniqIPs), \n",
    "                             portDirSwitcher(sessionForEncoding), \n",
    "                             ipDirSwitcher(sessionForEncoding)]\n",
    "            abbyIndex = random.sample(range(len(adversaryList)), 1)[0]\n",
    "\n",
    "            abbyOneHotSes = oneSessionEncoder(adversaryList[abbyIndex],\n",
    "                                              hexDict = hexDict,\n",
    "                                              packetReverse=packetReverse, \n",
    "                                              padOldTimeSteps = padOldTimeSteps, \n",
    "                                              maxPackets = maxPackets, \n",
    "                                              packetTimeSteps = packetTimeSteps)\n",
    "\n",
    "            targetClasses = [0]*numClasses\n",
    "            targetClasses[abbyIndex] = 1\n",
    "            abbyTarget = np.array(targetClasses, dtype=theano.config.floatX)\n",
    "            trainingSessions.append(abbyOneHotSes[0])\n",
    "            trainingTargets.append(abbyTarget)'''\n",
    "\n",
    "        sessionsMinibatch = np.asarray(trainingSessions).reshape((-1, packetTimeSteps, 1, dimIn))\n",
    "        targetsMinibatch = np.asarray(trainingTargets)\n",
    "\n",
    "        costfun = classifierTrain(sessionsMinibatch, targetsMinibatch)\n",
    "\n",
    "        costCollect.append(costfun[0])\n",
    "        trainCollect.append(np.mean(np.argmax(costfun[-1],axis=1) == np.argmax(targetsMinibatch, axis=1)))\n",
    "\n",
    "        iteration+=1\n",
    "\n",
    "        if iteration == 1:\n",
    "            print 'you are amazing'\n",
    "\n",
    "\n",
    "        if iteration%2 == 0:\n",
    "            print\n",
    "            print '   Iteration: ', iteration\n",
    "            print '   Cost: ', np.mean(costCollect[-20:])\n",
    "            print '   TRAIN accuracy: ', np.mean(trainCollect[-20:])\n",
    "            print\n",
    "\n",
    "            #grads = gradientFun(sessionsMinibatch, targetsMinibatch)\n",
    "            #for gra in grads:\n",
    "            #    print '  gradient norms: ', np.linalg.norm(gra)\n",
    "\n",
    "            np.savetxt('/data/fs4/home/bradh/outputs/'+runname+\"_TRAIN.csv\", trainCollect[::50], delimiter=\",\")\n",
    "            np.savetxt('/data/fs4/home/bradh/outputs/'+runname+\"_COST.csv\", costCollect[::50], delimiter=\",\")\n",
    "\n",
    "        ''' #testing accuracy\n",
    "        if iteration%500 == 0:\n",
    "            predtar, acttar, testCollect = predictClass(classifierPredict, hexSessions, comsDict, uniqIPs, hexDict,\n",
    "                                                        hexSessionsKeys,\n",
    "                                                        numClasses, trainPercent, dimIn, maxPackets, packetTimeSteps,\n",
    "                                                     padOldTimeSteps)\n",
    "\n",
    "            binaryPrecisionRecall(predtar, acttar)\n",
    "\n",
    "            testAcc.append(testCollect)\n",
    "            np.savetxt('/data/fs4/home/bradh/outputs/'+runname+\"_TEST.csv\", testAcc, delimiter=\",\")\n",
    "\n",
    "        #save the models\n",
    "        if iteration%1500 == 0:\n",
    "            pickleFile(classifierTrain, filePath='/data/fs4/home/bradh/outputs/',\n",
    "                        fileName=runname+'TRAIN'+str(iteration))\n",
    "            pickleFile(classifierPredict, filePath='/data/fs4/home/bradh/outputs/',\n",
    "                        fileName=runname+'PREDICT'+str(iteration))\n",
    "\n",
    "    epochCost.append(np.mean(costCollect[-50:]))\n",
    "    trainAcc.append(np.mean(trainCollect[-50:]))\n",
    "\n",
    "    print 'Epoch: ', epoch\n",
    "    #module_logger.debug('Epoch:%r',epoch)\n",
    "    print 'Epoch cost average: ', epochCost[-1]\n",
    "    print 'Epoch TRAIN accuracy: ', trainAcc[-1]'''\n",
    "    \n",
    "    return classifierPredict, classifierTrain\n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dim': 100, 'clippings': 1, 'dimIn': 257, 'decay': 0.9, 'runname': 'hredClassify2smallpackets', 'packetTimeSteps': 80, 'trainPercent': 0.9, 'batch_size': 20, 'maxPackets': 2, 'epochs': 5, 'padOldTimeSteps': True, 'lr': 0.0001, 'numClasses': 4, 'packetReverse': False, 'wtstd': 0.2, 'rnnType': 'gru'}\n",
      "\n",
      "initializing network graph\n",
      "compiling graph you talented soul\n",
      "finished compiling\n",
      "training begins\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'hexDict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-fc4d8d31b3d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m train, predict = training(runname, rnnType, maxPackets, packetTimeSteps, packetReverse, padOldTimeSteps, wtstd, \n\u001b[0;32m      3\u001b[0m              \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclippings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdimIn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumClasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m              trainPercent)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-34-7e520e7932b3>\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(runname, rnnType, maxPackets, packetTimeSteps, packetReverse, padOldTimeSteps, wtstd, lr, decay, clippings, dimIn, dim, numClasses, batch_size, epochs, trainPercent)\u001b[0m\n\u001b[0;32m    141\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msampleKeys\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m                 oneEncoded = oneSessionEncoder(compDict[sampleList[d]][key][0],\n\u001b[1;32m--> 143\u001b[1;33m                                                           \u001b[0mhexDict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhexDict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m                                                           \u001b[0mpacketReverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpacketReverse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m                                                           \u001b[0mpadOldTimeSteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpadOldTimeSteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: global name 'hexDict' is not defined"
     ]
    }
   ],
   "source": [
    "#TODO: expose classifier dim\n",
    "train, predict = training(runname, rnnType, maxPackets, packetTimeSteps, packetReverse, padOldTimeSteps, wtstd, \n",
    "             lr, decay, clippings, dimIn, dim, numClasses, batch_size, epochs, \n",
    "             trainPercent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
