{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import theano\n",
    "from theano import tensor\n",
    "import theano.tensor as T\n",
    "\n",
    "import blocks\n",
    "from blocks.algorithms import GradientDescent, Adam\n",
    "from blocks.bricks import MLP, Tanh, WEIGHT, Rectifier\n",
    "from blocks.initialization import Constant, Sparse, Orthogonal\n",
    "from fuel.streams import DataStream\n",
    "from fuel.datasets import MNIST\n",
    "from fuel.schemes import SequentialScheme, ShuffledScheme\n",
    "from blocks.filter import VariableFilter\n",
    "from blocks.graph import ComputationGraph, apply_dropout\n",
    "from blocks.model import Model\n",
    "from blocks.monitoring import aggregation\n",
    "from blocks.extensions import FinishAfter, Timing, Printing\n",
    "from blocks.extensions.saveload import Checkpoint, Load\n",
    "from blocks.extensions.monitoring import (DataStreamMonitoring,\n",
    "                                          TrainingDataMonitoring)\n",
    "from blocks.extensions.plot import Plot\n",
    "from blocks.main_loop import MainLoop\n",
    "\n",
    "from blocks.bricks.cost import BinaryCrossEntropy\n",
    "from blocks.bricks import Logistic\n",
    "import fuel\n",
    "import os\n",
    "from fuel.datasets.hdf5 import H5PYDataset\n",
    "floatX = theano.config.floatX\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "from fuel.transformers import Flatten\n",
    "\n",
    "import sys\n",
    "if sys.gettrace() is not None:\n",
    "    print \"Debugging\"\n",
    "    theano.config.optimizer='fast_compile' #\"None\"  #\n",
    "    theano.config.exception_verbosity='high'\n",
    "    theano.config.compute_test_value = 'warn'\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "from blocks.bricks import Initializable, Random, Linear\n",
    "from blocks.bricks.base import application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Qlinear(Initializable):\n",
    "    \"\"\"\n",
    "    brick to handle the intermediate layer of an Autoencoder.\n",
    "    In this brick a simple linear mix is performed (a kind of PCA.)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, **kwargs):\n",
    "        super(Qlinear, self).__init__(**kwargs)\n",
    "\n",
    "        self.mean_transform = Linear(\n",
    "                name=self.name+'_mean',\n",
    "                input_dim=input_dim, output_dim=output_dim,\n",
    "                weights_init=self.weights_init, biases_init=self.biases_init,\n",
    "                use_bias=True)\n",
    "\n",
    "        self.children = [self.mean_transform]\n",
    "\n",
    "    def get_dim(self, name):\n",
    "        if name == 'input':\n",
    "            return self.mean_transform.get_dim('input')\n",
    "        elif name == 'output':\n",
    "            return self.mean_transform.get_dim('output')\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    @application(inputs=['x'], outputs=['z', 'kl_term'])\n",
    "    def sample(self, x):\n",
    "        \"\"\"Sampling is trivial in this case\n",
    "        \"\"\"\n",
    "        mean = self.mean_transform.apply(x)\n",
    "\n",
    "        z = mean\n",
    "\n",
    "        # Calculate KL\n",
    "        batch_size = x.shape[0]\n",
    "        kl = T.zeros((batch_size,),dtype=floatX)\n",
    "\n",
    "        return z, kl\n",
    "\n",
    "    @application(inputs=['x'], outputs=['z'])\n",
    "    def mean_z(self, x):\n",
    "        return self.mean_transform.apply(x)\n",
    "\n",
    "\n",
    "class Qsampler(Qlinear, Random):\n",
    "    \"\"\"\n",
    "    brick to handle the intermediate layer of an Autoencoder.\n",
    "    The intermidate layer predict the mean and std of each dimension\n",
    "    of the intermediate layer and then sample from a normal distribution.\n",
    "    \"\"\"\n",
    "    # Special brick to handle Variatonal Autoencoder statistical sampling\n",
    "    def __init__(self, input_dim, output_dim, **kwargs):\n",
    "        super(Qsampler, self).__init__(input_dim, output_dim, **kwargs)\n",
    "\n",
    "        self.prior_mean = 0.\n",
    "        self.prior_log_sigma = 0.\n",
    "\n",
    "        self.log_sigma_transform = Linear(\n",
    "                name=self.name+'_log_sigma',\n",
    "                input_dim=input_dim, output_dim=output_dim,\n",
    "                weights_init=self.weights_init, biases_init=self.biases_init,\n",
    "                use_bias=True)\n",
    "\n",
    "        self.children.append(self.log_sigma_transform)\n",
    "\n",
    "    @application(inputs=['x'], outputs=['z', 'kl_term'])\n",
    "    def sample(self, x):\n",
    "        \"\"\"Return a samples and the corresponding KL term\n",
    "        Parameters\n",
    "        ----------\n",
    "        x :\n",
    "        Returns\n",
    "        -------\n",
    "        z : tensor.matrix\n",
    "            Samples drawn from Q(z|x)\n",
    "        kl : tensor.vector\n",
    "            KL(Q(z|x) || P_z)\n",
    "        \"\"\"\n",
    "        mean = self.mean_transform.apply(x)\n",
    "        log_sigma = self.log_sigma_transform.apply(x)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        dim_z = self.get_dim('output')\n",
    "\n",
    "        # Sample from mean-zeros std.-one Gaussian\n",
    "        u = self.theano_rng.normal(\n",
    "                    size=(batch_size, dim_z),\n",
    "                    avg=0., std=1.)\n",
    "        z = mean + tensor.exp(log_sigma) * u\n",
    "\n",
    "        # Calculate KL\n",
    "        kl = (\n",
    "            self.prior_log_sigma - log_sigma\n",
    "            + 0.5 * (\n",
    "                tensor.exp(2 * log_sigma) + (mean - self.prior_mean) ** 2\n",
    "                ) / tensor.exp(2 * self.prior_log_sigma)\n",
    "            - 0.5\n",
    "        ).sum(axis=-1)\n",
    "\n",
    "        return z, kl\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class VAEModel(Initializable):\n",
    "    \"\"\"\n",
    "    A brick to perform the entire auto-encoding process\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                    encoder_mlp, sampler,\n",
    "                    decoder_mlp, **kwargs):\n",
    "        super(VAEModel, self).__init__(**kwargs)\n",
    "\n",
    "        self.encoder_mlp = encoder_mlp\n",
    "        self.sampler = sampler\n",
    "        self.decoder_mlp = decoder_mlp\n",
    "\n",
    "        self.children = [self.encoder_mlp, self.sampler, self.decoder_mlp]\n",
    "\n",
    "    def get_dim(self, name):\n",
    "        if name in ['z', 'z_mean', 'z_log_sigma']:\n",
    "            return self.sampler.get_dim('output')\n",
    "        elif name == 'kl':\n",
    "            return 0\n",
    "        else:\n",
    "            super(VAEModel, self).get_dim(name)\n",
    "\n",
    "    @application(inputs=['features'], outputs=['reconstruction', 'kl_term'])\n",
    "    def reconstruct(self, features):\n",
    "        enc = self.encoder_mlp.apply(features)\n",
    "        z, kl = self.sampler.sample(enc)\n",
    "\n",
    "        x_recons = self.decoder_mlp.apply(z)\n",
    "        x_recons.name = \"reconstruction\"\n",
    "\n",
    "        kl.name = \"kl\"\n",
    "\n",
    "        return x_recons, kl\n",
    "\n",
    "    @application(inputs=['features'], outputs=['z', 'enc'])\n",
    "    def mean_z(self, features):\n",
    "        enc = self.encoder_mlp.apply(features)\n",
    "        z = self.sampler.mean_z(enc)\n",
    "\n",
    "        return z, enc\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "def shnum(value):\n",
    "    \"\"\" Convert a float into a short tag-usable string representation. E.g.:\n",
    "        <=0 -> 0\n",
    "        0.1   -> 11\n",
    "        0.01  -> 12\n",
    "        0.001 -> 13\n",
    "        0.005 -> 53\n",
    "    \"\"\"\n",
    "    if value <= 0.:\n",
    "        return '0'\n",
    "    exp = np.floor(np.log10(value))\n",
    "    leading = (\"%e\"%value)[0]\n",
    "    return \"%s%d\" % (leading, -exp)\n",
    "\n",
    "def main(name, model, epochs, batch_size, learning_rate, bokeh, layers, gamma,\n",
    "         rectifier, predict, dropout, qlinear, sparse):\n",
    "    runname = \"vae%s-L%s%s%s%s-l%s-g%s-b%d\" % (name, layers,\n",
    "                                            'r' if rectifier else '',\n",
    "                                            'd' if dropout else '',\n",
    "                                            'l' if qlinear else '',\n",
    "                                      shnum(learning_rate), shnum(gamma), batch_size//100)\n",
    "    if rectifier:\n",
    "        activation = Rectifier()\n",
    "        full_weights_init = Orthogonal()\n",
    "    else:\n",
    "        activation = Tanh()\n",
    "        full_weights_init = Orthogonal()\n",
    "\n",
    "    if sparse:\n",
    "        runname += '-s%d'%sparse\n",
    "        weights_init = Sparse(num_init=sparse, weights_init=full_weights_init)\n",
    "    else:\n",
    "        weights_init = full_weights_init\n",
    "\n",
    "    layers = map(int,layers.split(','))\n",
    "\n",
    "    encoder_layers = layers[:-1]\n",
    "    encoder_mlp = MLP([activation] * (len(encoder_layers)-1),\n",
    "              encoder_layers,\n",
    "              name=\"MLP_enc\", biases_init=Constant(0.), weights_init=weights_init)\n",
    "\n",
    "    enc_dim = encoder_layers[-1]\n",
    "    z_dim = layers[-1]\n",
    "    if qlinear:\n",
    "        sampler = Qlinear(input_dim=enc_dim, output_dim=z_dim, biases_init=Constant(0.), weights_init=full_weights_init)\n",
    "    else:\n",
    "        sampler = Qsampler(input_dim=enc_dim, output_dim=z_dim, biases_init=Constant(0.), weights_init=full_weights_init)\n",
    "\n",
    "    decoder_layers = layers[:]  ## includes z_dim as first layer\n",
    "    decoder_layers.reverse()\n",
    "    decoder_mlp = MLP([activation] * (len(decoder_layers)-2) + [Logistic()],\n",
    "              decoder_layers,\n",
    "              name=\"MLP_dec\", biases_init=Constant(0.), weights_init=weights_init)\n",
    "\n",
    "\n",
    "    vae = VAEModel(encoder_mlp, sampler, decoder_mlp)\n",
    "    vae.initialize()\n",
    "\n",
    "    x = tensor.matrix('features')/256.\n",
    "    x.tag.test_value = np.random.random((batch_size,layers[0])).astype(np.float32)\n",
    "\n",
    "    if predict:\n",
    "        mean_z, enc = vae.mean_z(x)\n",
    "        # cg = ComputationGraph([mean_z, enc])\n",
    "        newmodel = Model([mean_z,enc])\n",
    "    else:\n",
    "        x_recons, kl_terms = vae.reconstruct(x)\n",
    "        recons_term = BinaryCrossEntropy().apply(x, x_recons)\n",
    "        recons_term.name = \"recons_term\"\n",
    "\n",
    "        cost = recons_term + kl_terms.mean()\n",
    "        cg = ComputationGraph([cost])\n",
    "\n",
    "        if gamma > 0:\n",
    "            weights = VariableFilter(roles=[WEIGHT])(cg.variables)\n",
    "            cost += gamma * blocks.theano_expressions.l2_norm(weights)\n",
    "\n",
    "        cost.name = \"nll_bound\"\n",
    "        newmodel = Model(cost)\n",
    "\n",
    "        if dropout:\n",
    "            from blocks.roles import INPUT\n",
    "            inputs = VariableFilter(roles=[INPUT])(cg.variables)\n",
    "            # dropout_target = [v for k,v in newmodel.get_params().iteritems()\n",
    "            #            if k.find('MLP')>=0 and k.endswith('.W') and not k.endswith('MLP_enc/linear_0.W')]\n",
    "            dropout_target = filter(lambda x: x.name.startswith('linear_'), inputs)\n",
    "            cg = apply_dropout(cg, dropout_target, 0.5)\n",
    "            target_cost = cg.outputs[0]\n",
    "        else:\n",
    "            target_cost = cost\n",
    "\n",
    "    if name == 'mnist':\n",
    "        if predict:\n",
    "            train_ds = MNIST(\"train\")\n",
    "        else:\n",
    "            train_ds = MNIST(\"train\", sources=['features'])\n",
    "        test_ds = MNIST(\"test\")\n",
    "    else:\n",
    "        datasource_dir = os.path.join(fuel.config.data_path, name)\n",
    "        datasource_fname = os.path.join(datasource_dir , name+'.hdf5')\n",
    "        if predict:\n",
    "            train_ds = H5PYDataset(datasource_fname, which_set='train')\n",
    "        else:\n",
    "            train_ds = H5PYDataset(datasource_fname, which_set='train', sources=['features'])\n",
    "        test_ds = H5PYDataset(datasource_fname, which_set='test')\n",
    "    train_s = Flatten(DataStream(train_ds,\n",
    "                 iteration_scheme=ShuffledScheme(\n",
    "                     train_ds.num_examples, batch_size)))\n",
    "    test_s = Flatten(DataStream(test_ds,\n",
    "                 iteration_scheme=ShuffledScheme(\n",
    "                     test_ds.num_examples, batch_size)))\n",
    "\n",
    "    if predict:\n",
    "        from itertools import chain\n",
    "        fprop = newmodel.get_theano_function()\n",
    "        allpdata = None\n",
    "        alledata = None\n",
    "        f = train_s.sources.index('features')\n",
    "        assert f == test_s.sources.index('features')\n",
    "        sources = test_s.sources\n",
    "        alllabels = dict((s,[]) for s in sources if s != 'features')\n",
    "        for data in chain(train_s.get_epoch_iterator(), test_s.get_epoch_iterator()):\n",
    "            for s,d in zip(sources,data):\n",
    "                if s != 'features':\n",
    "                    alllabels[s].extend(list(d))\n",
    "\n",
    "            pdata, edata = fprop(data[f])\n",
    "            if allpdata is None:\n",
    "                allpdata = pdata\n",
    "            else:\n",
    "                allpdata = np.vstack((allpdata, pdata))\n",
    "            if alledata is None:\n",
    "                alledata = edata\n",
    "            else:\n",
    "                alledata = np.vstack((alledata, edata))\n",
    "        print 'Saving',allpdata.shape,'intermidiate layer, for all training and test examples, to',name+'_z.npy'\n",
    "        np.save(name+'_z', allpdata)\n",
    "        print 'Saving',alledata.shape,'last encoder layer to',name+'_e.npy'\n",
    "        np.save(name+'_e', alledata)\n",
    "        print 'Saving additional labels/targets:',','.join(alllabels.keys()),\n",
    "        print ' of size',','.join(map(lambda x: str(len(x)),alllabels.values())),\n",
    "        print 'to',name+'_labels.pkl'\n",
    "        with open(name+'_labels.pkl','wb') as fp:\n",
    "            pickle.dump(alllabels, fp, -1)\n",
    "    else:\n",
    "        cg = ComputationGraph([target_cost])\n",
    "        algorithm = GradientDescent(\n",
    "            cost=target_cost, params=cg.parameters,\n",
    "            step_rule=Adam(learning_rate)  # Scale(learning_rate=learning_rate)\n",
    "        )\n",
    "        extensions = []\n",
    "        if model:\n",
    "            extensions.append(Load(model))\n",
    "\n",
    "        extensions += [Timing(),\n",
    "                      FinishAfter(after_n_epochs=epochs),\n",
    "                      DataStreamMonitoring(\n",
    "                          [cost, recons_term],\n",
    "                          test_s,\n",
    "                          prefix=\"test\"),\n",
    "                      TrainingDataMonitoring(\n",
    "                          [cost,\n",
    "                           aggregation.mean(algorithm.total_gradient_norm)],\n",
    "                          prefix=\"train\",\n",
    "                          after_epoch=True),\n",
    "                      Checkpoint(runname, every_n_epochs=10),\n",
    "                      Printing()]\n",
    "\n",
    "        if bokeh:\n",
    "            extensions.append(Plot(\n",
    "                'Auto',\n",
    "                channels=[\n",
    "                    ['test_recons_term','test_nll_bound','train_nll_bound'\n",
    "                     ],\n",
    "                    ['train_total_gradient_norm']]))\n",
    "\n",
    "        main_loop = MainLoop(\n",
    "            algorithm,\n",
    "            train_s,\n",
    "            model=newmodel,\n",
    "            extensions=extensions)\n",
    "\n",
    "        main_loop.run()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    parser = ArgumentParser(\"An example of training a Variational-Autoencoder.\")\n",
    "    parser.add_argument(\"--name\", default=\"mnist\",\n",
    "                        help=\"name of hdf5 data set\")\n",
    "    parser.add_argument(\"--model\",\n",
    "                        help=\"start model to read\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=1000,\n",
    "                        help=\"Number of training epochs to do.\")\n",
    "    parser.add_argument(\"--bs\", \"--batch-size\", type=int, dest=\"batch_size\",\n",
    "                default=500, help=\"Size of each mini-batch\")\n",
    "    parser.add_argument(\"--lr\", \"--learning-rate\", type=float, dest=\"learning_rate\",\n",
    "                default=1e-3, help=\"Learning rate\")\n",
    "    parser.add_argument(\"--bokeh\", action='store_true', default=False,\n",
    "                        help=\"Set if you want to use Bokeh \")\n",
    "    parser.add_argument(\"--layers\",\n",
    "                default=\"784,100,20\", help=\"number of units in each layer of the encoder\"\n",
    "                                           \" (use 784, on first layer, for mnist.)\"\n",
    "                                           \" The last number (e.g. 20) is the dimension of the intermidiate layer.\"\n",
    "                                           \" The decoder has the same layers as the encoder but in reverse\"\n",
    "                                           \" (e.g. 100, 784)\")\n",
    "    parser.add_argument(\"--gamma\", type=float,\n",
    "                default=3e-4, help=\"L2 weight\")\n",
    "    parser.add_argument(\"-r\",\"--rectifier\",action='store_true',default=False,\n",
    "                        help=\"Use RELU activation on hidden (default Tanh)\")\n",
    "    parser.add_argument(\"-p\",\"--predict\",action='store_true',default=False,\n",
    "                        help=\"Generate prediction of the  intermidate layer and last layer of the encoder\"\n",
    "                             \" instead of training.\"\n",
    "                             \" You must supply a pre-trained model and define all parameters to be the same\"\n",
    "                             \" as in training. \")\n",
    "    parser.add_argument(\"-d\",\"--dropout\",action='store_true',default=False,\n",
    "                        help=\"Use dropout\")\n",
    "    parser.add_argument(\"-l\",\"--qlinear\",action='store_true',default=False,\n",
    "                        help=\"Perform a deterministic linear transformation instead of sampling\"\n",
    "                             \" on the intermidiate layer\")\n",
    "    parser.add_argument(\"-s\",\"--sparse\",type=int,\n",
    "                        help=\"Use sparse weight initialization. Give the number of non zero weights\")\n",
    "    args = parser.parse_args()\n",
    "    main(**vars(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def dropout(X, p=0.):\n",
    "    if p != 0:\n",
    "        retain_prob = 1 - p\n",
    "        X = X / retain_prob * srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX)\n",
    "    return X\n",
    "\n",
    "# Gradient clipping\n",
    "def clip_norm(g, c, n): \n",
    "    '''n is the norm, c is the threashold, and g is the gradient'''\n",
    "    \n",
    "    if c > 0: \n",
    "        g = T.switch(T.ge(n, c), g*c/n, g) \n",
    "    return g\n",
    "\n",
    "def clip_norms(gs, c):\n",
    "    norm = T.sqrt(sum([T.sum(g**2) for g in gs]))\n",
    "    return [clip_norm(g, c, norm) for g in gs]\n",
    "\n",
    "# Regularizers\n",
    "def max_norm(p, maxnorm = 0.):\n",
    "    if maxnorm > 0:\n",
    "        norms = T.sqrt(T.sum(T.sqr(p), axis=0))\n",
    "        desired = T.clip(norms, 0, maxnorm)\n",
    "        p = p * (desired/ (1e-7 + norms))\n",
    "    return p\n",
    "\n",
    "def gradient_regularize(p, g, l1 = 0., l2 = 0.):\n",
    "    g += p * l2\n",
    "    g += T.sgn(p) * l1\n",
    "    return g\n",
    "\n",
    "def weight_regularize(p, maxnorm = 0.):\n",
    "    p = max_norm(p, maxnorm)\n",
    "    return p\n",
    "\n",
    "def Adam(params, cost, lr=0.0002, b1=0.1, b2=0.001, e=1e-8, l1 = 0., l2 = 0., maxnorm = 0., c = 8):\n",
    "    \n",
    "    updates = []\n",
    "    grads = T.grad(cost, params)\n",
    "    grads = clip_norms(grads, c)\n",
    "    \n",
    "    i = theano.shared(floatX(0.))\n",
    "    i_t = i + 1.\n",
    "    fix1 = 1. - b1**(i_t)\n",
    "    fix2 = 1. - b2**(i_t)\n",
    "    lr_t = lr * (T.sqrt(fix2) / fix1)\n",
    "    \n",
    "    for p, g in zip(params, grads):\n",
    "        m = theano.shared(p.get_value() * 0.)\n",
    "        v = theano.shared(p.get_value() * 0.)\n",
    "        m_t = (b1 * g) + ((1. - b1) * m)\n",
    "        v_t = (b2 * T.sqr(g)) + ((1. - b2) * v)\n",
    "        g_t = m_t / (T.sqrt(v_t) + e)\n",
    "        g_t = gradient_regularize(p, g_t, l1=l1, l2=l2)\n",
    "        p_t = p - (lr_t * g_t)\n",
    "        p_t = weight_regularize(p_t, maxnorm=maxnorm)\n",
    "        \n",
    "        updates.append((m, m_t))\n",
    "        updates.append((v, v_t))\n",
    "        updates.append((p, p_t))\n",
    "    \n",
    "    updates.append((i, i_t))\n",
    "    return updates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
