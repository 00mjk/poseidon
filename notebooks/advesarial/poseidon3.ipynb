{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#how data are represented at each level (forward, backward, forward with padding on top) needs a little\n",
    "    #experimentation to determine the best representation\n",
    "    #also, is encoding at each layer really the best way? or just feeding the raw through?\n",
    "    \n",
    "#Outside web ips are going to be a problem/messy/noisy. Start by categorizing all outside ips by <OUTSIDE_IP>\n",
    "    #instead of the ip address, or another 4 digit symbol to insert into the hex string.\n",
    "    \n",
    "#to help the models generalize more, for a given source ip address with probability p (say p = 0.1) \n",
    "    #use the token <OTHER_MACHINE>\n",
    "    \n",
    "#should we remove random parts of the header, i.e. checksum\n",
    "\n",
    "#should I take out bias for RNNs?\n",
    "\n",
    "#for the decoder,does the fork encoding need to happen ?\n",
    "    #do we simply cat the hContext with the next words?\n",
    "    \n",
    "#Should the architecture just be encode, context and then prediction???\n",
    "\n",
    "#Input data, should it have character and hex pair encoding as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to execute tcpdump. Check it is installed and in the PATH\n",
      "WARNING: No route found for IPv6 destination :: (no default route?)\n",
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled, CuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ['THEANO_FLAGS'] = 'floatX=float32,device=gpu,optimizer=fast_compile'\n",
    "\n",
    "import sys\n",
    "import binascii\n",
    "import multiprocessing as mp\n",
    "from itertools import chain\n",
    "from scapy.all import *\n",
    "sys.path.append('hed-dlg/')\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.stats import itemfreq\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import blocks\n",
    "from blocks.bricks import Linear, Softmax, Softplus, NDimensionalSoftmax, BatchNormalizedMLP, \\\n",
    "                                Rectifier, Logistic, Tanh, MLP\n",
    "from blocks.bricks.recurrent import GatedRecurrent, Fork, LSTM\n",
    "from blocks.initialization import Constant, IsotropicGaussian, Identity, Uniform\n",
    "from blocks.bricks.cost import BinaryCrossEntropy, CategoricalCrossEntropy\n",
    "from blocks.filter import VariableFilter\n",
    "from blocks.roles import PARAMETER\n",
    "from blocks.graph import ComputationGraph\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "\n",
    "###These warnings do not impede progress\n",
    "#WARNING: Failed to execute tcpdump. Check it is installed and in the PATH\n",
    "#WARNING: No route found for IPv6 destination :: (no default route?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataPath = '/data/fs4/datasets/pcaps/bradsfirstpcaps.pcap'\n",
    "pcaps = rdpcap(dataPath)\n",
    "sessionPrep = pcaps.sessions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ipComs(listOsessions, hexOut = False):\n",
    "    '''\n",
    "    takes scapy sessions\n",
    "    \n",
    "    returns a dictionary of source ips and the ips they talk to, and a list of \n",
    "    all of the unique ip addresses in the data\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    ipAddressDict = {}\n",
    "    uniqIPs = []\n",
    "    \n",
    "    for k,v in listOsessions.items():\n",
    "        for p in v:\n",
    "            sourceIP = p.payload.fields['src']\n",
    "            destIP = p.payload.fields['dst']\n",
    "            \n",
    "            #if source ip is not in dictionary, then add it with dest ip as list\n",
    "            if sourceIP not in ipAddressDict:\n",
    "                ipAddressDict[sourceIP] = [destIP]\n",
    "            \n",
    "            else:\n",
    "                ipAddressDict[sourceIP] = list(set(ipAddressDict[sourceIP]) | set([destIP]))\n",
    "            \n",
    "            uniqIPs = list(set(uniqIPs) | set([destIP, sourceIP]))\n",
    "            \n",
    "    return ipAddressDict, uniqIPs\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sessionDict, uniqips = ipComs(sessionPrep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def oneIpDirSwitcher(normHexSessionList):\n",
    "    '''\n",
    "    input is a list of packets from ONE session\n",
    "    '''\n",
    "    \n",
    "    session = []\n",
    "        \n",
    "    for p in normHexSessionList:\n",
    "        sourceIP = p[52:60]\n",
    "        destIP = p[60:68]\n",
    "\n",
    "        session.append(p[:52]+destIP+sourceIP+p[68:])\n",
    "\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ipDirectionSwitcher(hexSessionsDict):\n",
    "    '''\n",
    "    input is a dictionary of many sessions\n",
    "    '''\n",
    "    badSessions = {}\n",
    "    \n",
    "    for k in hexSessionsDict.keys():\n",
    "        \n",
    "        session = oneIpDirSwitcher(k)\n",
    "        \n",
    "        badSessions[k] = session\n",
    "            \n",
    "    return badSessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#turns the sessions into a dictionary key = session_number, val = list of packages in hex\n",
    "\n",
    "i=0\n",
    "hexSessions = {}\n",
    "\n",
    "for k,v in sessionPrep.items(): # v is the session\n",
    "    #for attr, value in v.__dict__.iteritems(): THIS IS TO GET DICT OF VALUES\n",
    "    #    print attr, value\n",
    "    #if i == 2:\n",
    "    #    break\n",
    "    scpcaps = []    \n",
    "    for p in v: #p is the individual packet in the session\n",
    "        \n",
    "        try:\n",
    "            rawindex = len(p[Raw])\n",
    "            payloadLens.append(rawindex)\n",
    "            scpcaps.append(binascii.hexlify(str(p.original)[:-rawindex])) #turn it into hex\n",
    "        except:\n",
    "            scpcaps.append(binascii.hexlify(p.original))\n",
    "        #for attr, value in p.payload.__dict__.iteritems():#this give the fields that are accessable\n",
    "        #    print attr, value\n",
    "        \n",
    "        #print len(binascii.hexlify(p.original))\n",
    "    hexSessions['session_' + str(i)] = scpcaps\n",
    "    \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Making the hex dictionary\n",
    "hexstring = '0,\t1,\t2,\t3,\t4,\t5,\t6,\t7,\t8,\t9,\tA,\tB,\tC,\tD,\tE,\tF,\t10,\t11,\t12,\t13,\t14,\t15,\t16,\t17,\t18,\t19\\\n",
    ",\t1A,\t1B,\t1C,\t1D,\t1E,\t1F,\t20,\t21,\t22,\t23,\t24,\t25,\t26,\t27,\t28,\t29,\t2A,\t2B,\t2C,\t2D,\t2E,\t2F,\t30,\t31,\t32,\t33,\t34,\t35\\\n",
    ",\t36,\t37,\t38,\t39,\t3A,\t3B,\t3C,\t3D,\t3E,\t3F,\t40,\t41,\t42,\t43,\t44,\t45,\t46,\t47,\t48,\t49,\t4A,\t4B,\t4C,\t4D,\t4E,\t4F,\t50,\t51\\\n",
    ",\t52,\t53,\t54,\t55,\t56,\t57,\t58,\t59,\t5A,\t5B,\t5C,\t5D,\t5E,\t5F,\t60,\t61,\t62,\t63,\t64,\t65,\t66,\t67,\t68,\t69,\t6A,\t6B,\t6C,\t6D\\\n",
    ",\t6E,\t6F,\t70,\t71,\t72,\t73,\t74,\t75,\t76,\t77,\t78,\t79,\t7A,\t7B,\t7C,\t7D,\t7E,\t7F,\t80,\t81,\t82,\t83,\t84,\t85,\t86,\t87,\t88,\t89\\\n",
    ",\t8A,\t8B,\t8C,\t8D,\t8E,\t8F,\t90,\t91,\t92,\t93,\t94,\t95,\t96,\t97,\t98,\t99,\t9A,\t9B,\t9C,\t9D,\t9E,\t9F,\tA0,\tA1,\tA2,\tA3,\tA4,\tA5\\\n",
    ",\tA6,\tA7,\tA8,\tA9,\tAA,\tAB,\tAC,\tAD,\tAE,\tAF,\tB0,\tB1,\tB2,\tB3,\tB4,\tB5,\tB6,\tB7,\tB8,\tB9,\tBA,\tBB,\tBC,\tBD,\tBE,\tBF,\tC0,\tC1\\\n",
    ",\tC2,\tC3,\tC4,\tC5,\tC6,\tC7,\tC8,\tC9,\tCA,\tCB,\tCC,\tCD,\tCE,\tCF,\tD0,\tD1,\tD2,\tD3,\tD4,\tD5,\tD6,\tD7,\tD8,\tD9,\tDA,\tDB,\tDC,\tDD\\\n",
    ",\tDE,\tDF,\tE0,\tE1,\tE2,\tE3,\tE4,\tE5,\tE6,\tE7,\tE8,\tE9,\tEA,\tEB,\tEC,\tED,\tEE,\tEF,\tF0,\tF1,\tF2,\tF3,\tF4,\tF5,\tF6,\tF7,\tF8,\tF9\\\n",
    ",\tFA,\tFB,\tFC,\tFD,\tFE,\tFF'.replace('\\t', '')\n",
    "\n",
    "hexList = hexstring.lower().split(',')\n",
    "hexList.append('<EOP>') #End Of Packet token\n",
    "hexDict = {}\n",
    "    \n",
    "for key, val in enumerate(hexList):\n",
    "    if len(val) == 1:\n",
    "        val = '0'+val\n",
    "    hexDict[val] = key  #dictionary k=hex, v=int  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def oneHotSessions(sessionDict, hexDict = hexDict, maxPackets = maxPackets, packetTimeSteps = packetTimeSteps,\\n                   reverse = False, charLevel = False):\\n    \"\"\"\\n    This takes a list of int tokens and onehot encodes them, pads sessions with zero tensors according to maxPackets\\n    and packet according to packetTimeSteps\\n    \\n    sessionDict = dict of lists of key = sessions and value = list of packets\\n    timeSteps = maximum len of packet. it will be padded with zero vectors is packet is too short.\\n    \\n    \"\"\"\\n    \\n    listOsessions = []\\n\\n    if charLevel:\\n        vecLen = 17\\n    else:\\n        vecLen = 257\\n    \\n    sessionKeys = sessionDict.keys()\\n    \\n    for session in sessionKeys:\\n        \\n        sessionCollect = oneSessionEncoder(session, hexDict = hexDict, maxPackets = maxPackets, \\n                                           packetTimeSteps = packetTimeSteps, reverse = reverse, \\n                                           charLevel = charLevel )\\n        \\n        listOsessions.append(sessionCollect)\\n        \\n    return listOsessions'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def oneHot(index, granular = 'hex'):\n",
    "    if granular == 'hex':\n",
    "        vecLen = 257\n",
    "    else:\n",
    "        vecLen = 17\n",
    "    \n",
    "    zeroVec = np.zeros(vecLen)\n",
    "    zeroVec[index] = 1.0\n",
    "    \n",
    "    return zeroVec\n",
    "    \n",
    "'''def oneHotSessions(sessionDict, hexDict = hexDict, maxPackets = maxPackets, packetTimeSteps = packetTimeSteps,\n",
    "                   reverse = False, charLevel = False):\n",
    "    \"\"\"\n",
    "    This takes a list of int tokens and onehot encodes them, pads sessions with zero tensors according to maxPackets\n",
    "    and packet according to packetTimeSteps\n",
    "    \n",
    "    sessionDict = dict of lists of key = sessions and value = list of packets\n",
    "    timeSteps = maximum len of packet. it will be padded with zero vectors is packet is too short.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    listOsessions = []\n",
    "\n",
    "    if charLevel:\n",
    "        vecLen = 17\n",
    "    else:\n",
    "        vecLen = 257\n",
    "    \n",
    "    sessionKeys = sessionDict.keys()\n",
    "    \n",
    "    for session in sessionKeys:\n",
    "        \n",
    "        sessionCollect = oneSessionEncoder(session, hexDict = hexDict, maxPackets = maxPackets, \n",
    "                                           packetTimeSteps = packetTimeSteps, reverse = reverse, \n",
    "                                           charLevel = charLevel )\n",
    "        \n",
    "        listOsessions.append(sessionCollect)\n",
    "        \n",
    "    return listOsessions'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxPackets = 5\n",
    "packetTimeSteps = 300\n",
    "def oneSessionEncoder(sessionPackets, hexDict, maxPackets, packetTimeSteps,\n",
    "                       packetReverse = True, charLevel = False, padOldTimeSteps = True):    \n",
    "            \n",
    "    sessionCollect = []\n",
    "    \n",
    "    if charLevel:\n",
    "        vecLen = 17\n",
    "    else:\n",
    "        vecLen = 257\n",
    "    \n",
    "    if len(sessionPackets) > maxPackets: #crop the number of sessions to maxPackets\n",
    "        sessionList = sessionPackets[:maxPackets]\n",
    "    else:\n",
    "        sessionList = sessionPackets\n",
    "\n",
    "    for packet in sessionList:\n",
    "        packet = [hexDict[packet[i:i+2]] for i in xrange(0,len(packet)-2+1,2)]\n",
    "\n",
    "        if len(packet) >= packetTimeSteps: #crop packet to length packetTimeSteps\n",
    "            packet = packet[:packetTimeSteps-1]\n",
    "\n",
    "        packet = packet+[256] #add <EOP> end of packet token\n",
    "\n",
    "        pacMat = np.array([oneHot(x) for x in packet]) #one hot encoding of packet into a matrix\n",
    "        pacMatLen = len(pacMat)\n",
    "        \n",
    "        #padding packet\n",
    "        if packetReverse:\n",
    "            pacMat = pacMat[::-1]\n",
    "\n",
    "        if pacMatLen < packetTimeSteps:\n",
    "            #pad by stacking zeros on top of data so that earlier timesteps do not have information\n",
    "            #padding the packet such that zeros are after the actual info for better translation\n",
    "            if padOldTimeSteps:\n",
    "                pacMat = np.vstack( ( np.zeros((packetTimeSteps-pacMatLen,vecLen)), pacMat) ) \n",
    "            else:\n",
    "                pacMat = np.vstack( (pacMat, np.zeros((packetTimeSteps-pacMatLen,vecLen))) ) \n",
    "\n",
    "        if pacMatLen > packetTimeSteps:\n",
    "            pacMat = pacMat[:packetTimeSteps, :]\n",
    "\n",
    "        sessionCollect.append(pacMat)\n",
    "\n",
    "    #padding session\n",
    "    sessionCollect = np.asarray(sessionCollect, dtype=theano.config.floatX)\n",
    "    numPacketsInSession = sessionCollect.shape[0]\n",
    "    if numPacketsInSession < maxPackets:\n",
    "        #pad sessions to fit the \n",
    "        sessionCollect = np.vstack( (sessionCollect,np.zeros((maxPackets-numPacketsInSession, \n",
    "                                                             packetTimeSteps, vecLen))) )\n",
    "    return sessionCollect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "#input is a 4d tensor (numMiniBatch, session, packetRow, packetCol)\n",
    "sessions = oneHotSessions(hexSessions)\n",
    "#badSessions = oneHotSessions(badHexSessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def dropout(X, p=0.):\n",
    "    if p != 0:\n",
    "        retain_prob = 1 - p\n",
    "        X = X / retain_prob * srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX)\n",
    "    return X\n",
    "\n",
    "# Gradient clipping\n",
    "def clip_norm(g, c, n): \n",
    "    '''n is the norm, c is the threashold, and g is the gradient'''\n",
    "    \n",
    "    if c > 0: \n",
    "        g = T.switch(T.ge(n, c), g*c/n, g) \n",
    "    return g\n",
    "\n",
    "def clip_norms(gs, c):\n",
    "    norm = T.sqrt(sum([T.sum(g**2) for g in gs]))\n",
    "    return [clip_norm(g, c, norm) for g in gs]\n",
    "\n",
    "# Regularizers\n",
    "def max_norm(p, maxnorm = 0.):\n",
    "    if maxnorm > 0:\n",
    "        norms = T.sqrt(T.sum(T.sqr(p), axis=0))\n",
    "        desired = T.clip(norms, 0, maxnorm)\n",
    "        p = p * (desired/ (1e-7 + norms))\n",
    "    return p\n",
    "\n",
    "def gradient_regularize(p, g, l1 = 0., l2 = 0.):\n",
    "    g += p * l2\n",
    "    g += T.sgn(p) * l1\n",
    "    return g\n",
    "\n",
    "def weight_regularize(p, maxnorm = 0.):\n",
    "    p = max_norm(p, maxnorm)\n",
    "    return p\n",
    "\n",
    "def Adam(params, cost, lr=0.0002, b1=0.1, b2=0.001, e=1e-8, l1 = 0., l2 = 0., maxnorm = 0., c = 8):\n",
    "    \n",
    "    updates = []\n",
    "    grads = T.grad(cost, params)\n",
    "    grads = clip_norms(grads, c)\n",
    "    \n",
    "    i = theano.shared(floatX(0.))\n",
    "    i_t = i + 1.\n",
    "    fix1 = 1. - b1**(i_t)\n",
    "    fix2 = 1. - b2**(i_t)\n",
    "    lr_t = lr * (T.sqrt(fix2) / fix1)\n",
    "    \n",
    "    for p, g in zip(params, grads):\n",
    "        m = theano.shared(p.get_value() * 0.)\n",
    "        v = theano.shared(p.get_value() * 0.)\n",
    "        m_t = (b1 * g) + ((1. - b1) * m)\n",
    "        v_t = (b2 * T.sqr(g)) + ((1. - b2) * v)\n",
    "        g_t = m_t / (T.sqrt(v_t) + e)\n",
    "        g_t = gradient_regularize(p, g_t, l1=l1, l2=l2)\n",
    "        p_t = p - (lr_t * g_t)\n",
    "        p_t = weight_regularize(p_t, maxnorm=maxnorm)\n",
    "        \n",
    "        updates.append((m, m_t))\n",
    "        updates.append((v, v_t))\n",
    "        updates.append((p, p_t))\n",
    "    \n",
    "    updates.append((i, i_t))\n",
    "    return updates\n",
    "\n",
    "def RMSprop(cost, params, lr = 0.001, l1 = 0., l2 = 0., maxnorm = 0., rho=0.9, epsilon=1e-6, c = 8):\n",
    "    \n",
    "    grads = T.grad(cost, params)\n",
    "    grads = clip_norms(grads, c)\n",
    "    updates = []\n",
    "    \n",
    "    for p, g in zip(params, grads):\n",
    "        g = gradient_regularize(p, g, l1 = l1, l2 = l2)\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        updates.append((acc, acc_new))\n",
    "        \n",
    "        updated_p = p - lr * (g / T.sqrt(acc_new + epsilon))\n",
    "        updated_p = weight_regularize(updated_p, maxnorm = maxnorm)\n",
    "        updates.append((p, updated_p))\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#makes output by shifting inputs down in time one step and then copying the last time step to the end.\n",
    "#def targetModifier(targetArray):\n",
    "#    newTarget = np.vstack((targetArray[1:, :], targetArray[-1,:]))\n",
    "#    return newTarget\n",
    "\n",
    "#def targetMaker(listOinputs):\n",
    "    #TODO: do this with arrays\n",
    "#    outputs = []\n",
    "#    for inp in listOinputs:\n",
    "#        outputs.append(targetModifier(inp))\n",
    "#    outputs = np.asarray(outputs)\n",
    "#    \n",
    "#    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = T.tensor4('inputs')\n",
    "Y = T.matrix('targets')\n",
    "\n",
    "dimIn = 257 #hex has 256 characters + the <EOP> character\n",
    "dim = 50 #dimension reduction size\n",
    "rnnType = 'lstm' #gru or lstm\n",
    "bidirectional = False\n",
    "linewt_init = Uniform(width=0.08)\n",
    "line_bias = Constant(0.0)\n",
    "rnnwt_init = IsotropicGaussian(0.08)\n",
    "rnnbias_init = Constant(0.0)\n",
    "packetReverse = False\n",
    "\n",
    "###ENCODER\n",
    "if rnnType == 'gru':\n",
    "    rnn = GatedRecurrent(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'gru')\n",
    "    dimMultiplier = 2\n",
    "else:\n",
    "    rnn = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'lstm')\n",
    "    dimMultiplier = 4\n",
    "\n",
    "fork = Fork(output_names=['linear', 'gates'],\n",
    "            name='fork', input_dim=dimIn, output_dims=[dim, dim * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "\n",
    "###CONTEXT\n",
    "if rnnType == 'gru':\n",
    "    rnnContext = GatedRecurrent(dim=dim, weights_init = rnnwt_init, \n",
    "                                biases_init = rnnbias_init, name = 'gruContext')\n",
    "else:\n",
    "    rnnContext = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, \n",
    "                      name = 'lstmContext')\n",
    "\n",
    "forkContext = Fork(output_names=['linearContext', 'gatesContext'],\n",
    "            name='forkContext', input_dim=dim, output_dims=[dim, dim * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "if bidirectional:\n",
    "    dimDec = dim*2\n",
    "    \n",
    "    if rnnType == 'gru':\n",
    "        rnnContextRev = GatedRecurrent(dim=dim, weights_init = rnnwt_init, \n",
    "                                       biases_init = rnnbias_init, name = 'gruContextRev')\n",
    "        \n",
    "    else:\n",
    "        rnnContextRev = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init,\n",
    "                             name = 'lstmContextRev')\n",
    "    \n",
    "    rnnContextRev.initialize()\n",
    "\n",
    "else:\n",
    "    dimDec = dim\n",
    "\n",
    "\n",
    "###DECODER\n",
    "if rnnType == 'gru':\n",
    "    rnnDec = GatedRecurrent(dim=dimIn, weights_init = rnnwt_init, \n",
    "                            biases_init = rnnbias_init, name = 'gruDecoder')\n",
    "else:\n",
    "    rnnDec = LSTM(dim=dimIn, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'lstmDecoder')\n",
    "\n",
    "\n",
    "forkDec = Fork(output_names=['linear', 'gates'],\n",
    "            name='forkDec', input_dim=dimDec, output_dims=[dim, dim*dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "forkFinal = Fork(output_names=['linear', 'gates'],\n",
    "            name='forkFinal', input_dim=dim, output_dims=[dimIn, dimIn*dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "#reduce dimension of bidirectLSTM\n",
    "\n",
    "fork.initialize()\n",
    "rnn.initialize()\n",
    "\n",
    "forkContext.initialize()\n",
    "rnnContext.initialize()\n",
    "\n",
    "forkDec.initialize()\n",
    "forkFinal.initialize()\n",
    "rnnDec.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "  \n",
    "def oneStep(X):\n",
    "    ###ENCODER\n",
    "    i = 0\n",
    "    data1, data2 = fork.apply(X)\n",
    "\n",
    "    if rnnType == 'gru':\n",
    "        hEnc = rnn.apply(data1, data2)[:,-1] #the [:,-1] gets the last hidden state for each obs in minibatch\n",
    "                                             #i.e. the last state for each sentence\n",
    "    else:\n",
    "        hinit, _ = rnn.apply(data2)\n",
    "        hEnc = hinit[:,-1]\n",
    "    \n",
    "    hEnc = T.reshape(hEnc,(maxPackets, 1, dim))\n",
    "    \n",
    "    data3, data4 = forkContext.apply(hEnc)\n",
    "    \n",
    "    if rnnType == 'gru':\n",
    "        hContext = rnnContext.apply(data3, data4)\n",
    "    else:\n",
    "        hinitContext, _ = rnnContext.apply(data4)\n",
    "        hContext = hinitContext\n",
    "    \n",
    "    if bidirectional:\n",
    "        data3 = data3[::-1]\n",
    "        data4 = data4[::-1]\n",
    "        \n",
    "        if rnnType == 'gru':\n",
    "            hContextRev = rnnContextRev.apply(data3, data4)\n",
    "        else:\n",
    "            hinitContext, _ = rnnContextRev.apply(data4)\n",
    "            hContextRev = hinitContext\n",
    "        \n",
    "        hContext = T.concatenate((hContext, hContextRev), axis=2)\n",
    "            \n",
    "    data5, data6 = forkDec.apply(hContext)\n",
    "    \n",
    "    #decoding data needs to be one timestep (next packet in session) ahead, thus data1 we ignore the first packet\n",
    "    #and the last hidden state of the context RNN.\n",
    "    #Think about L2 pooling before cat\n",
    "    if packetReverse:\n",
    "        data1 = data1[:,::-1]\n",
    "        \n",
    "    data7 = T.concatenate((data5[:-1,:,:], data1[1:, :-1, :]), axis=1) #data1 is the original embedding of X\n",
    "\n",
    "    #data8 = T.concatenate((data7, data5), axis = 2)\n",
    "    data8, data9 = forkFinal.apply(data7)\n",
    "\n",
    "\n",
    "    if rnnType == 'gru':\n",
    "        hDec = rnnDec.apply(data8, data9) \n",
    "    else:\n",
    "        hinit, _ = rnnDec.apply(data9)\n",
    "        hDec = hinit\n",
    "    \n",
    "    softmax = NDimensionalSoftmax()\n",
    "    softout = softmax.apply(hDec, extra_ndim = 1)\n",
    "\n",
    "    precost = X[1:, :, :]*np.log(softout) + (1-X[1:, :, :])*np.log(1-softout)\n",
    "    precost2 = -T.sum(T.sum(precost, axis = 2), axis = 1)\n",
    "    #precost2 = -T.mean(T.sum(T.sum(precost, axis = 2), axis = 1))\n",
    "    i+=1\n",
    "    #precost2 = BinaryCrossEntropy().apply(X[1:, :, :], softout)\n",
    "\n",
    "    #return data4\n",
    "    return precost2, softout, hContext, data5, data7, data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#hContext, _ = theano.scan(fn = oneStep, sequences=[X])\n",
    "[scanOut, softout, hContext, data5, data7, data1], _ = theano.scan(fn = oneStep, sequences=[X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testCont = theano.function([X], [hContext, data7, data5, data1], allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainingSessions = []\n",
    "hexTests = []\n",
    "for trainKey in range(0, 20):\n",
    "    sessionForEncoding = hexSessions[hexSessions.keys()[trainKey]]\n",
    "    hexTests.append(sessionForEncoding)\n",
    "    oneHotSes = oneSessionEncoder(sessionForEncoding,packetReverse=packetReverse, hexDict = hexDict, \n",
    "                                  padOldTimeSteps = packetReverse,\n",
    "                                  maxPackets = maxPackets, \n",
    "                                  packetTimeSteps = packetTimeSteps)\n",
    "    trainingSessions.append(oneHotSes)\n",
    "\n",
    "sessionsMinibatch = np.asarray(trainingSessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num5 = testCont(sessionsMinibatch)[2]\n",
    "num7 = testCont(sessionsMinibatch)[1]\n",
    "num1 = testCont(sessionsMinibatch)[3]\n",
    "cont = testCont(sessionsMinibatch)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 5, 1, 50)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 5, 1, 50)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 4, 300, 50)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 5, 300, 50)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.11812173, -0.13523397,  0.00882616,  0.08315604, -0.07806508,\n",
       "       -0.13128696, -0.01247992,  0.13481268, -0.01794721, -0.09885799,\n",
       "        0.0364034 ,  0.10697754, -0.12391753,  0.08445285, -0.12339272,\n",
       "        0.07350034,  0.04322248,  0.03930046,  0.10175701,  0.1186583 ,\n",
       "       -0.07125095,  0.09562849, -0.09229624, -0.10282856,  0.00242822,\n",
       "        0.09491683,  0.11119862,  0.00549224, -0.12482708,  0.01341937,\n",
       "       -0.14872101,  0.13791214, -0.13989726,  0.10578853,  0.13687721,\n",
       "       -0.10548743, -0.12911218, -0.13981357, -0.15064925, -0.12105531,\n",
       "       -0.0174122 ,  0.09502137, -0.08487622, -0.05395069,  0.01626256,\n",
       "       -0.09085742, -0.13775007, -0.11115627, -0.12096445,  0.11322218], dtype=float32)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num1[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.68411112,  0.5826242 ,  0.72484797, ..., -0.63207465,\n",
       "        -0.47040081,  0.39061105],\n",
       "       [ 0.06159166, -0.05838058,  0.05886749, ..., -0.05713895,\n",
       "        -0.05602812,  0.05661277],\n",
       "       [ 0.06159166, -0.05838058,  0.05886749, ..., -0.05713895,\n",
       "        -0.05602812,  0.05661277],\n",
       "       ..., \n",
       "       [ 0.06159166, -0.05838058,  0.05886749, ..., -0.05713895,\n",
       "        -0.05602812,  0.05661277],\n",
       "       [ 0.06159166, -0.05838058,  0.05886749, ..., -0.05713895,\n",
       "        -0.05602812,  0.05661277],\n",
       "       [ 0.06159166, -0.05838058,  0.05886749, ..., -0.05713895,\n",
       "        -0.05602812,  0.05661277]], dtype=float32)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num7[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.68411112,  0.5826242 ,  0.72484797, ..., -0.63207465,\n",
       "          -0.47040081,  0.39061105]],\n",
       "\n",
       "        [[ 1.34414136,  1.1608721 ,  1.43275785, ..., -1.25721765,\n",
       "          -0.93490183,  0.7647264 ]],\n",
       "\n",
       "        [[ 2.0300622 ,  1.76323247,  2.16503215, ..., -1.90539002,\n",
       "          -1.4130187 ,  1.15334141]],\n",
       "\n",
       "        [[ 2.49310875,  2.16928077,  2.65606427, ..., -2.33995342,\n",
       "          -1.7324754 ,  1.41815138]],\n",
       "\n",
       "        [[ 2.70787096,  2.35714245,  2.88287592, ..., -2.53964591,\n",
       "          -1.87953901,  1.54241514]]],\n",
       "\n",
       "\n",
       "       [[[ 0.68411112,  0.5826242 ,  0.72484797, ..., -0.63207465,\n",
       "          -0.47040081,  0.39061105]],\n",
       "\n",
       "        [[ 1.34414136,  1.1608721 ,  1.43275785, ..., -1.25721765,\n",
       "          -0.93490183,  0.7647264 ]],\n",
       "\n",
       "        [[ 2.07360387,  1.80123818,  2.21118808, ..., -1.94570577,\n",
       "          -1.44277203,  1.17842233]],\n",
       "\n",
       "        [[ 2.56175303,  2.22877884,  2.72884774, ..., -2.40302658,\n",
       "          -1.77935147,  1.45781732]],\n",
       "\n",
       "        [[ 2.77643704,  2.41570234,  2.95556736, ..., -2.60177588,\n",
       "          -1.92640305,  1.58192146]]],\n",
       "\n",
       "\n",
       "       [[[ 0.68411112,  0.5826242 ,  0.72484797, ..., -0.63207465,\n",
       "          -0.47040081,  0.39061105]],\n",
       "\n",
       "        [[ 1.34414136,  1.1608721 ,  1.43275785, ..., -1.25721765,\n",
       "          -0.93490183,  0.7647264 ]],\n",
       "\n",
       "        [[ 2.07360387,  1.80123818,  2.21118808, ..., -1.94570577,\n",
       "          -1.44277203,  1.17842233]],\n",
       "\n",
       "        [[ 2.53761292,  2.20812583,  2.70326853, ..., -2.38113117,\n",
       "          -1.7628268 ,  1.44389939]],\n",
       "\n",
       "        [[ 2.75531912,  2.39773917,  2.93318558, ..., -2.58265972,\n",
       "          -1.91195333,  1.5697639 ]]],\n",
       "\n",
       "\n",
       "       ..., \n",
       "       [[[ 0.68411112,  0.5826242 ,  0.72484797, ..., -0.63207465,\n",
       "          -0.47040081,  0.39061105]],\n",
       "\n",
       "        [[ 1.34414136,  1.1608721 ,  1.43275785, ..., -1.25721765,\n",
       "          -0.93490183,  0.7647264 ]],\n",
       "\n",
       "        [[ 2.07360387,  1.80123818,  2.21118808, ..., -1.94570577,\n",
       "          -1.44277203,  1.17842233]],\n",
       "\n",
       "        [[ 2.56175303,  2.22877884,  2.72884774, ..., -2.40302658,\n",
       "          -1.77935147,  1.45781732]],\n",
       "\n",
       "        [[ 2.77643704,  2.41570234,  2.95556736, ..., -2.60177588,\n",
       "          -1.92640305,  1.58192146]]],\n",
       "\n",
       "\n",
       "       [[[ 0.68411112,  0.5826242 ,  0.72484797, ..., -0.63207465,\n",
       "          -0.47040081,  0.39061105]],\n",
       "\n",
       "        [[ 1.34414136,  1.1608721 ,  1.43275785, ..., -1.25721765,\n",
       "          -0.93490183,  0.7647264 ]],\n",
       "\n",
       "        [[ 2.07360387,  1.80123818,  2.21118808, ..., -1.94570577,\n",
       "          -1.44277203,  1.17842233]],\n",
       "\n",
       "        [[ 2.56175303,  2.22877884,  2.72884774, ..., -2.40302658,\n",
       "          -1.77935147,  1.45781732]],\n",
       "\n",
       "        [[ 2.77643704,  2.41570234,  2.95556736, ..., -2.60177588,\n",
       "          -1.92640305,  1.58192146]]],\n",
       "\n",
       "\n",
       "       [[[ 0.68411112,  0.5826242 ,  0.72484797, ..., -0.63207465,\n",
       "          -0.47040081,  0.39061105]],\n",
       "\n",
       "        [[ 1.34414136,  1.1608721 ,  1.43275785, ..., -1.25721765,\n",
       "          -0.93490183,  0.7647264 ]],\n",
       "\n",
       "        [[ 2.07360387,  1.80123818,  2.21118808, ..., -1.94570577,\n",
       "          -1.44277203,  1.17842233]],\n",
       "\n",
       "        [[ 2.56175303,  2.22877884,  2.72884774, ..., -2.40302658,\n",
       "          -1.77935147,  1.45781732]],\n",
       "\n",
       "        [[ 2.77643704,  2.41570234,  2.95556736, ..., -2.60177588,\n",
       "          -1.92640305,  1.58192146]]]], dtype=float32)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1, 50)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEACAYAAABbMHZzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fca3ce04350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(testCont(sessionsMinibatch)[2][-1].squeeze(), 'b', testCont(sessionsMinibatch)[3][-1].squeeze(), 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 18,  18,  19,  19,  24,  24,  25,  25,  37,  37,  38,  38,  39,\n",
       "         39,  40,  40,  41,  41,  42,  42,  43,  43,  44,  44,  45,  45,\n",
       "         50,  50,  51,  51,  58,  58,  59,  59,  60,  60,  61,  61,  64,\n",
       "         64,  65,  65,  66,  66,  67,  67,  68,  68,  69,  69,  70,  70,\n",
       "         71,  71,  72,  72,  73,  73,  74,  74,  75,  75,  76,  76,  77,\n",
       "         77,  78,  78,  79,  79,  80,  80,  81,  81,  82,  82,  83,  83,\n",
       "         84,  84,  85,  85,  86,  86,  87,  87,  88,  88,  89,  89,  90,\n",
       "         90,  91,  91,  92,  92,  93,  93,  94,  94,  95,  95,  96,  96,\n",
       "         97,  97,  98,  98,  99,  99, 100, 100, 101, 101, 102, 102, 103,\n",
       "        103, 104, 104, 105, 105, 106, 106, 107, 107, 108, 108, 109, 109,\n",
       "        110, 110, 111, 111, 112, 112, 113, 113, 114, 114, 115, 115, 116,\n",
       "        116, 117, 117, 118, 118, 119, 119, 120, 120, 121, 121, 122, 122,\n",
       "        123, 123, 124, 124, 125, 125, 126, 126, 127, 127, 128, 128, 129,\n",
       "        129, 130, 130, 131, 131, 132, 132, 133, 133, 134, 134, 135, 135,\n",
       "        136, 136, 137, 137, 138, 138, 139, 139, 140, 140, 141, 141, 142,\n",
       "        142, 143, 143, 144, 144, 145, 145, 146, 146, 147, 147, 148, 148]),\n",
       " array([ 18, 247,  14, 151,  77, 104,   1, 137, 114, 115,  80, 225,  24,\n",
       "        138, 107, 175,  39, 105,  79, 235,  39, 169, 129, 224,  46, 221,\n",
       "         33,  51,  78, 125, 181, 191,   8, 160, 145, 182, 113, 222,  55,\n",
       "         56, 138, 171, 130, 207,   3, 141, 173, 183,   5, 191, 104, 108,\n",
       "         42, 177, 185, 242,  11, 112,  79, 172, 178, 223,  34, 159, 169,\n",
       "        189,  40, 108,  39, 164,  30,  33, 178, 220,  70, 133,  14,  44,\n",
       "         11, 131,  13, 212,  49, 109,  24, 192,  68, 231, 125, 247, 129,\n",
       "        227, 109, 151,  78, 144,  68, 224, 167, 223,  18, 130,  37, 184,\n",
       "          3,  92,  61, 109,  17,  70,  50, 215,  97, 160, 223, 230,  73,\n",
       "        239,  34, 133,  15, 222,  17,  44,  16,  78,  88, 211,  10, 126,\n",
       "        108, 175,  79, 231, 130, 247, 134, 253,  32, 232,  97, 161, 165,\n",
       "        208,  28, 167,  66,  74, 116, 236, 112, 235,  42, 104, 100, 136,\n",
       "        102, 112,  15, 144, 138, 152,  98, 171,  21, 214, 242, 252,  85,\n",
       "        139,  20, 125,  12,  13,  19,  77,  53, 183,  82, 231,  52, 191,\n",
       "         99, 168,   9, 111, 150, 207,  47, 152, 103, 134, 171, 248,  66,\n",
       "        146, 172, 184,  24, 130,  33, 239, 126, 224,  75, 113,  21, 224]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where((sessionsMinibatch[1][-1] == sessionsMinibatch[2][-1]) == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.996108949416 0.0\n",
      "0.993411154345 0.0\n",
      "1.0 1.0\n",
      "0.993411154345 0.0\n",
      "0.995577172503 0.0\n",
      "0.996108949416 0.0\n",
      "0.996108949416 0.0\n",
      "0.996108949416 0.0\n",
      "0.993437094682 0.0\n",
      "0.993488975357 0.0\n",
      "0.996108949416 0.0\n",
      "0.996108949416 0.0\n",
      "0.995577172503 0.0\n",
      "0.993411154345 0.0\n",
      "0.996108949416 0.0\n",
      "0.993566796368 0.0\n",
      "0.996108949416 0.0\n",
      "0.993488975357 0.0\n",
      "0.993385214008 0.0\n",
      "0.993411154345 0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print np.mean(sessionsMinibatch[2][-1]==sessionsMinibatch[i][-1]), np.mean(testCont(sessionsMinibatch)[2][-1]==testCont(sessionsMinibatch)[i][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = T.mean(scanOut)\n",
    "#cost = BinaryCrossEntropy().apply(Y, softout)\n",
    "cg = ComputationGraph([cost])\n",
    "learning_rate = 0.0001\n",
    "params = VariableFilter(roles = [PARAMETER])(cg.variables)\n",
    "updates = Adam(params, cost, learning_rate, c=5) #c is gradient clipping parameter\n",
    "#updates = RMSprop(cost, params, learning_rate, c=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "gradients = T.grad(cost, params)\n",
    "gradients = clip_norms(gradients, 1)\n",
    "gradientFun = theano.function([X], gradients, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling you beautiful person\n",
      "finished compiling\n"
     ]
    }
   ],
   "source": [
    "print \"compiling you beautiful person\"\n",
    "train = theano.function([X], cost, updates = updates, allow_input_downcast=True)\n",
    "#predict = theano.function([X], softout, allow_input_downcast=True)\n",
    "print \"finished compiling\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Epoch:  0\n",
      "Epoch cost average:  854.477\n",
      " \n",
      "Epoch:  2\n",
      "Epoch cost average:  849.821\n",
      " \n",
      "Epoch:  4\n",
      "Epoch cost average:  836.611\n",
      " \n",
      "Epoch:  6\n",
      "Epoch cost average:  825.634\n",
      " \n",
      "Epoch:  8\n",
      "Epoch cost average:  817.098\n"
     ]
    }
   ],
   "source": [
    "#Eventually we will need to do all of the transformations on the fly so we can pull from disc\n",
    "hexSessionsKeys = hexSessions.keys()\n",
    "#random.shuffle(hexSessionsKeys)\n",
    "trainPercent = 0.9\n",
    "trainIndex = int(len(hexSessionsKeys)*trainPercent)\n",
    "\n",
    "runname = 'hred'\n",
    "epochCost = []\n",
    "gradNorms = []\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 20\n",
    "iteration = 0\n",
    "\n",
    "\n",
    "for epoch in xrange(epochs):\n",
    "    costCollect = []\n",
    "\n",
    "    for start, end in zip(range(0, trainIndex,batch_size), range(batch_size, trainIndex, batch_size)):\n",
    "        #build a 4d array of oneHot sessions\n",
    "        \n",
    "        trainingSessions = []\n",
    "        for trainKey in range(start, end):\n",
    "            sessionForEncoding = hexSessions[hexSessions.keys()[trainKey]]\n",
    "            oneHotSes = oneSessionEncoder(sessionForEncoding, packetReverse=packetReverse, \n",
    "                                          padOldTimeSteps = packetReverse,\n",
    "                                          hexDict = hexDict,\n",
    "                                          maxPackets = maxPackets, packetTimeSteps = packetTimeSteps)\n",
    "            trainingSessions.append(oneHotSes)\n",
    "            \n",
    "        sessionsMinibatch = np.asarray(trainingSessions)\n",
    "        \n",
    "    \n",
    "        costfun = train(sessionsMinibatch)\n",
    "        costCollect.append(costfun)\n",
    "                \n",
    "        iteration+=1\n",
    "        \n",
    "    ####SAVE COST TO FILE  \n",
    "    if epoch%2 == 0:\n",
    "        print(' ')\n",
    "        print 'Epoch: ', epoch\n",
    "        epochCost.append(np.mean(costCollect))\n",
    "        print 'Epoch cost average: ', epochCost[-1]\n",
    "        #grads = gradientFun(inputs, outputs)\n",
    "        #for gra in grads:\n",
    "        #    print '  gradient norms: ', np.linalg.norm(gra)\n",
    "        \n",
    "    np.savetxt(runname+\"_COST.csv\", epochCost, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y = T.matrix('targets')\n",
    "batch_sizeClass = 20\n",
    "numClasses = 2\n",
    "clippings = 5\n",
    "learning_rateClass = 0.001\n",
    "classifierWts = IsotropicGaussian(0.08, 0)\n",
    "\n",
    "bmlp = BatchNormalizedMLP(activations=[Tanh(),Tanh()], \n",
    "           dims=[dimDec, dimDec, 2],\n",
    "           weights_init=classifierWts,\n",
    "           biases_init=Constant(0.0) )\n",
    "\n",
    "\n",
    "bmlp.initialize()\n",
    "\n",
    "def oneStep(X):\n",
    "    ###ENCODER\n",
    "    \n",
    "    data1class, data2class = fork.apply(X) #reusing the weights from fork, rnn, and rnnContext\n",
    "    \n",
    "    if rnnType == 'gru':\n",
    "        hEncclass = rnn.apply(data1class, data2class)[:,-1] #the [:,-1] gets the last hidden state for \n",
    "                                                            #each obs in minibatch i.e. the last state \n",
    "                                                            #for each sentence\n",
    "    else:\n",
    "        hinitclass, _ = rnn.apply(data2class)\n",
    "        hEncclass = hinitclass[:,-1]\n",
    "    \n",
    "    hEncclass = T.reshape(hEncclass,(maxPackets, 1, dim))\n",
    "    \n",
    "    data3class, data4class = forkContext.apply(hEncclass)\n",
    "    \n",
    "    if rnnType == 'gru':\n",
    "        hContextClass = rnnContext.apply(data3class, data4class)\n",
    "    else:\n",
    "        hinitContextClass, _ = rnnContext.apply(data4class)\n",
    "        hContextClass = hinitContextClass\n",
    "    \n",
    "    if bidirectional:\n",
    "        \n",
    "        data3classRev = data3class[::-1]\n",
    "        data4classRev = data4class[::-1]\n",
    "        \n",
    "        if rnnType == 'gru':\n",
    "            hContextRev = rnnContextRev.apply(data3classRev, data4classRev)\n",
    "        else:\n",
    "            hinitContextRevClass, _ = rnnContextRev.apply(data4classRev)\n",
    "            hContextRevClass = hinitContextRevClass\n",
    "        \n",
    "        hContextClass = T.concatenate((hContextClass, hContextRevClass), axis=2)\n",
    "        \n",
    "    return hContextClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hContextClass, _ = theano.scan(fn = oneStep, sequences=[X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#newhContextClass = hContextClass[:, -1].reshape((batch_sizeClass, dimDec)) \n",
    "newhContextClass = T.mean(hContextClass, axis = (1,2))\n",
    "pyx = bmlp.apply(newhContextClass)\n",
    "softmax = Softmax()\n",
    "softoutClass = softmax.apply(pyx)\n",
    "costClass = T.mean(BinaryCrossEntropy().apply(Y, softoutClass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cgClass = ComputationGraph([costClass])\n",
    "paramsClass = VariableFilter(roles = [PARAMETER])(cgClass.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "updatesClass = Adam(paramsClass, costClass, learning_rateClass, c=clippings) \n",
    "#updatesClass = RMSprop(costClass, paramsClass, learning_rateClass, c=clippings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'grad compiling'\n",
    "gradients = T.grad(costClass, paramsClass)\n",
    "gradients = clip_norms(gradients, clippings)\n",
    "gradientFun = theano.function([X,Y], gradients, allow_input_downcast=True)\n",
    "print 'finish with grads'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print 'compiling functions you talented soul'\n",
    "classifierTrain = theano.function([X,Y], [softoutClass,costClass], updates=updatesClass, allow_input_downcast=True)\n",
    "#classifierPredict = theano.function([X], softoutClass, allow_input_downcast=True)\n",
    "print 'finished compiling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "runname = 'hredClassify'\n",
    "epochCost = []\n",
    "gradNorms = []\n",
    "\n",
    "epochs = 300\n",
    "iteration = 0\n",
    "\n",
    "normalTarget = np.array([0,1], dtype=theano.config.floatX)\n",
    "abbyTarget = np.array([1,0], dtype=theano.config.floatX)\n",
    "\n",
    "for epoch in xrange(epochs):\n",
    "    costCollect = []\n",
    "\n",
    "    for start, end in zip(range(0, trainIndex,batch_sizeClass/2),\n",
    "                          range(batch_sizeClass/2, trainIndex, batch_sizeClass/2)):\n",
    "\n",
    "        #build a 4d array of oneHot sessions\n",
    "        \n",
    "        trainingTargets = []\n",
    "        trainingSessions = []\n",
    "        for trainKey in range(start, end):\n",
    "            sessionForEncoding = hexSessions[hexSessions.keys()[trainKey]]\n",
    "            \n",
    "            #encode a normal session\n",
    "            oneHotSes = oneSessionEncoder(sessionForEncoding,hexDict = hexDict, packetReverse=packetReverse, \n",
    "                                          padOldTimeSteps = packetReverse, maxPackets = maxPackets,\n",
    "                                          packetTimeSteps = packetTimeSteps)\n",
    "            trainingSessions.append(oneHotSes)\n",
    "            trainingTargets.append(normalTarget)\n",
    "            \n",
    "            #encode an abby normal session\n",
    "            abbyHexSession = oneIpDirSwitcher(sessionForEncoding)\n",
    "            abbyOneHotSes = oneSessionEncoder(abbyHexSession,hexDict = hexDict,packetReverse=packetReverse, \n",
    "                                          padOldTimeSteps = packetReverse, maxPackets = maxPackets, \n",
    "                                              packetTimeSteps = packetTimeSteps)\n",
    "            trainingSessions.append(abbyOneHotSes)\n",
    "            trainingTargets.append(abbyTarget)\n",
    "             \n",
    "        sessionsMinibatch = np.asarray(trainingSessions, dtype=theano.config.floatX)\n",
    "        targetsMinibatch = np.asarray(trainingTargets, dtype=theano.config.floatX)\n",
    "    \n",
    "        costfun = classifierTrain(sessionsMinibatch, targetsMinibatch)\n",
    "        costCollect.append(costfun[1])\n",
    "                \n",
    "        iteration+=1\n",
    "        \n",
    "    ####SAVE COST TO FILE  \n",
    "    if epoch%2 == 0:\n",
    "        print(' ')\n",
    "        print 'Epoch: ', epoch\n",
    "        epochCost.append(np.mean(costCollect))\n",
    "        print 'Epoch cost average: ', epochCost[-1]\n",
    "        grads = gradientFun(sessionsMinibatch, targetsMinibatch)\n",
    "        print 'pyx: ', costfun[0]\n",
    "        for gra in grads:\n",
    "            print '  gradient norms: ', np.linalg.norm(gra)\n",
    "        \n",
    "    np.savetxt(runname+\"_COST.csv\", epochCost, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advesarial Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a dictionary of packet/session stats\n",
    "\n",
    "#switch ips\n",
    "#switch ports\n",
    "#swap out and replace one ip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tests to show effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#jack up ip field and see diff in prediction before and after\n",
    "#jack up checksum and see diff in prediciton before and after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim = 257 #original data dimension/timesteps/columns\n",
    "rnnType = 'gru' #gru or lstm\n",
    "bidirectional = True\n",
    "X = T.tensor3('inputs')\n",
    "Xrev = T.matrix('reversed_inputs')\n",
    "linewt_init = Uniform(width=0.08)\n",
    "rnnwt_init = IsotropicGaussian(0.05)\n",
    "rnnbias_init = Constant(0.0)\n",
    "\n",
    "if rnnType == 'gru':\n",
    "    rnn = GatedRecurrent(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'gru')\n",
    "    dimMultiplier = 2\n",
    "else:\n",
    "    rnn = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'lstm')\n",
    "    dimMultiplier = 4\n",
    "\n",
    "###RECURRENT LAYER\n",
    "\n",
    "#To use or not to use that is the question\n",
    "fork = Fork(output_names=['linear', 'gates'],\n",
    "            name='fork', input_dim=dim, output_dims=[dim, dim * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = rnnbias_init)\n",
    "data1, data2 = fork.apply(X)\n",
    "\n",
    "###for raw inputs\n",
    "#data1 = X\n",
    "#data2 = T.concatenate([X]*dimMultiplier, axis=2)\n",
    "\n",
    "if rnnType == 'gru':\n",
    "    hEnc = rnn.apply(data1, data2)[:,-1] #the [:,-1] gets the last hidden state for each obs in minibatch\n",
    "                                         #i.e. the last state for each sentence\n",
    "else:\n",
    "    hinit, _ = rnn.apply(data2)\n",
    "    hEnc = hinit[:,-1]\n",
    "\n",
    "hEnc = T.reshape(hEnc,(maxPackets, 1, dim))\n",
    "#get weights initialized. without weights are nans.\n",
    "fork.initialize()\n",
    "rnn.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TEST Encoder will return a maxPackets x packet length matrix\n",
    "encoder = theano.function([X], hEnc, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TEST ENCODED PACKETS shape = (maxPackets, 1, dim)\n",
    "encoder(sessions[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "X = T.tensor4('testInputs')\n",
    "Y = T.vector('targets')\n",
    "dim = 257 #original data dimension/timesteps/columns\n",
    "dimDec = dim*2\n",
    "rnnType = 'gru' #gru or lstm\n",
    "bidirectional = True\n",
    "linewt_init = Uniform(width=0.05)\n",
    "rnnwt_init = IsotropicGaussian(0.02)\n",
    "rnnbias_init = Constant(0.0)\n",
    "\n",
    "def oneStep(X):\n",
    "    ###ENCODER\n",
    "    if rnnType == 'gru':\n",
    "        rnn = GatedRecurrent(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'gru')\n",
    "        dimMultiplier = 2\n",
    "    else:\n",
    "        rnn = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'lstm')\n",
    "        dimMultiplier = 4\n",
    "    \n",
    "    #To use (fork) or not to use that is the question\n",
    "    fork = Fork(output_names=['linear', 'gates'],\n",
    "                name='fork', input_dim=dim, output_dims=[dim, dim * dimMultiplier], \n",
    "                weights_init = linewt_init, biases_init = rnnbias_init)\n",
    "    data1, data2 = fork.apply(X)\n",
    "\n",
    "    ###for raw inputs... for not using fork\n",
    "    #data1 = X\n",
    "    #data2 = T.concatenate([X]*dimMultiplier, axis=2)\n",
    "\n",
    "    if rnnType == 'gru':\n",
    "        hEnc = rnn.apply(data1, data2)[:,-1] #the [:,-1] gets the last hidden state for each obs in minibatch\n",
    "                                             #i.e. the last state for each sentence\n",
    "    else:\n",
    "        hinit, _ = rnn.apply(data2)\n",
    "        hEnc = hinit[:,-1]\n",
    "\n",
    "    hEnc = T.reshape(hEnc,(maxPackets, 1, dim))\n",
    "\n",
    "    fork.initialize()\n",
    "    rnn.initialize()\n",
    "\n",
    "    \n",
    "    ###CONTEXT\n",
    "    if rnnType == 'gru':\n",
    "        rnnContext = GatedRecurrent(dim=dim, weights_init = rnnwt_init, \n",
    "                                    biases_init = rnnbias_init, name = 'gruContext')\n",
    "        dimMultiplier = 2\n",
    "    else:\n",
    "        rnnContext = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, \n",
    "                          name = 'lstmContext')\n",
    "        dimMultiplier = 4\n",
    "\n",
    "    ###RECURRENT LAYER\n",
    "    forkContext = Fork(output_names=['linearContext', 'gatesContext'],\n",
    "                name='forkContext', input_dim=dim, output_dims=[dim, dim * dimMultiplier], \n",
    "                weights_init = linewt_init, biases_init = rnnbias_init)\n",
    "    data3, data4 = forkContext.apply(hEnc)\n",
    "\n",
    "    if rnnType == 'gru':\n",
    "        hContext = rnnContext.apply(data3, data4)\n",
    "    else:\n",
    "        hinitContext, _ = rnnContext.apply(data4)\n",
    "        hContext = hinitContext\n",
    "\n",
    "    #THINK ABOUT ADDING L2 POOLING BEFORE CAT\n",
    "    if bidirectional:\n",
    "\n",
    "        data3 = data3[::-1]\n",
    "        data4 = data4[::-1]\n",
    "\n",
    "        if rnnType == 'gru':\n",
    "            rnnContextRev = GatedRecurrent(dim=dim, weights_init = rnnwt_init, \n",
    "                                           biases_init = rnnbias_init, name = 'gruContextRev')\n",
    "            hContextRev = rnnContextRev.apply(data3, data4)\n",
    "        else:\n",
    "            rnnContextRev = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init,\n",
    "                                 name = 'lstmContextRev')\n",
    "            hinitContext, _ = rnnContextRev.apply(data4)\n",
    "            hContextRev = hinitContext\n",
    "\n",
    "\n",
    "        hContext = T.concatenate((hContext, hContextRev), axis=2)\n",
    "        rnnContextRev.initialize()\n",
    "\n",
    "    forkContext.initialize()\n",
    "    rnnContext.initialize()\n",
    "\n",
    "    \n",
    "    ###DECODER\n",
    "    \n",
    "    if rnnType == 'gru':\n",
    "        rnnDec = GatedRecurrent(dim=dim, weights_init = rnnwt_init, \n",
    "                                biases_init = rnnbias_init, name = 'gruDecoder')\n",
    "        dimMultiplier = 2\n",
    "    else:\n",
    "        rnnDec = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'lstmDecoder')\n",
    "        dimMultiplier = 4\n",
    "\n",
    "\n",
    "    forkDec = Fork(output_names=['linear', 'gates'],\n",
    "                name='forkDec', input_dim=dimDec, output_dims=[dim, dim*dimMultiplier], \n",
    "                weights_init = linewt_init, biases_init = rnnbias_init)\n",
    "\n",
    "    forkFinal = Fork(output_names=['linear', 'gates'],\n",
    "                name='forkDec', input_dim=dim, output_dims=[dim, dim*dimMultiplier], \n",
    "                weights_init = linewt_init, biases_init = rnnbias_init)\n",
    "\n",
    "    data5, data6 = forkDec.apply(hContext)#reduce dimension of bidirectLSTM\n",
    "\n",
    "    #decoding data needs to be one timestep (next packet in session) ahead, thus data1 we ignore the first packet\n",
    "    #and the last hidden state of the context RNN.\n",
    "    data7 = T.concatenate((data5[:-1,:,:], data1[1:, :-1, :]), axis=1) #data1 is the original embedding of X\n",
    "\n",
    "    #data8 = T.concatenate((data7, data5), axis = 2)\n",
    "    data8, data9 = forkFinal.apply(data7)\n",
    "\n",
    "\n",
    "    if rnnType == 'gru':\n",
    "        hDec = rnnDec.apply(data8, data9) \n",
    "    else:\n",
    "        hinit, _ = rnnDec.apply(data9)\n",
    "        hDec = hinit\n",
    "\n",
    "    #Smooth out the probabilities of hDec\n",
    "    softmax = NDimensionalSoftmax()\n",
    "    softout = softmax.apply(hDec, extra_ndim = 1)\n",
    "\n",
    "\n",
    "    precost = X[1:, :, :]*np.log(softout) + (1-X[1:, :, :])*np.log(1-softout)\n",
    "    precost2 = -T.sum(T.sum(precost, axis = 2), axis = 1)\n",
    "    #precost2 = -T.mean(T.sum(T.sum(precost, axis = 2), axis = 1))\n",
    "    \n",
    "    #cost = BinaryCrossEntropy().apply(X[1:, :, :], softout)\n",
    "\n",
    "    forkDec.initialize()\n",
    "    forkFinal.initialize()\n",
    "    rnnDec.initialize()\n",
    "    \n",
    "    return precost2, hEnc, hContext, hDec, softout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "if rnnType == 'gru':\n",
    "    rnnContext = GatedRecurrent(dim=dim, weights_init = rnnwt_init, \n",
    "                                biases_init = rnnbias_init, name = 'gruContext')\n",
    "    dimMultiplier = 2\n",
    "else:\n",
    "    rnnContext = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, \n",
    "                      name = 'lstmContext')\n",
    "    dimMultiplier = 4\n",
    "\n",
    "\n",
    "###ICLR suggestion -> don't use bias in RNNs\n",
    "\n",
    "###RECURRENT LAYER\n",
    "forkContext = Fork(output_names=['linearContext', 'gatesContext'],\n",
    "            name='forkContext', input_dim=dim, output_dims=[dim, dim * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = rnnbias_init)\n",
    "data3, data4 = forkContext.apply(hEnc)\n",
    "\n",
    "if rnnType == 'gru':\n",
    "    hContext = rnnContext.apply(data3, data4)\n",
    "else:\n",
    "    hinitContext, _ = rnnContext.apply(data4)\n",
    "    hContext = hinitContext\n",
    "\n",
    "#THINK ABOUT ADDING L2 POOLING BEFORE CAT\n",
    "if bidirectional:\n",
    "    \n",
    "    data3 = data3[::-1]\n",
    "    data4 = data4[::-1]\n",
    "    \n",
    "    if rnnType == 'gru':\n",
    "        rnnContextRev = GatedRecurrent(dim=dim, weights_init = rnnwt_init, \n",
    "                                       biases_init = rnnbias_init, name = 'gruContextRev')\n",
    "        hContextRev = rnnContextRev.apply(data3, data4)\n",
    "    else:\n",
    "        rnnContextRev = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init,\n",
    "                             name = 'lstmContextRev')\n",
    "        hinitContext, _ = rnnContextRev.apply(data4)\n",
    "        hContextRev = hinitContext\n",
    "    \n",
    "    \n",
    "    hContext = T.concatenate((hContext, hContextRev), axis=2)\n",
    "    rnnContextRev.initialize()\n",
    "    \n",
    "#get weights initialized. without weights are nans.\n",
    "forkContext.initialize()\n",
    "rnnContext.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TEST output shape = (maxPackets, 1, dim*2)\n",
    "context = theano.function([X], hContext, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TEST\n",
    "context(sessions[1]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dimDec = dim*2\n",
    "\n",
    "if rnnType == 'gru':\n",
    "    rnnDec = GatedRecurrent(dim=dim, weights_init = rnnwt_init, \n",
    "                            biases_init = rnnbias_init, name = 'gruDecoder')\n",
    "    dimMultiplier = 2\n",
    "else:\n",
    "    rnnDec = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'lstmDecoder')\n",
    "    dimMultiplier = 4\n",
    "\n",
    "\n",
    "forkDec = Fork(output_names=['linear', 'gates'],\n",
    "            name='forkDec', input_dim=dimDec, output_dims=[dim, dim*dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = rnnbias_init)\n",
    "\n",
    "forkFinal = Fork(output_names=['linear', 'gates'],\n",
    "            name='forkDec', input_dim=dim, output_dims=[dim, dim*dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = rnnbias_init)\n",
    "\n",
    "data5, data6 = forkDec.apply(hContext)#reduce dimension of bidirectLSTM\n",
    "\n",
    "#decoding data needs to be one timestep (next packet in session) ahead, thus data1 we ignore the first packet\n",
    "#and the last hidden state of the context RNN.\n",
    "data7 = T.concatenate((data5[:-1,:,:], data1[1:, :-1, :]), axis=1) #data1 is the original embedding of X\n",
    "\n",
    "#data8 = T.concatenate((data7, data5), axis = 2)\n",
    "data8, data9 = forkFinal.apply(data7)\n",
    "\n",
    "\n",
    "if rnnType == 'gru':\n",
    "    hDec = rnnDec.apply(data8, data9) \n",
    "else:\n",
    "    hinit, _ = rnnDec.apply(data9)\n",
    "    hDec = hinit\n",
    "\n",
    "#Smooth out the probabilities of hDec\n",
    "softmax = NDimensionalSoftmax()\n",
    "softout = softmax.apply(hDec, extra_ndim = 1)\n",
    "    \n",
    "\n",
    "precost = X[1:, :, :]*np.log(softout) + (1-X[1:, :, :])*np.log(1-softout)\n",
    "cost = -T.mean(T.sum(T.sum(precost, axis = 2), axis = 1))\n",
    "#cost = BinaryCrossEntropy().apply(X[1:, :, :], softout)\n",
    "\n",
    "#get weights initialized\n",
    "forkDec.initialize()\n",
    "forkFinal.initialize()\n",
    "rnnDec.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TEST\n",
    "decoder = theano.function([X], cost, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X = T.tensor4('input')\n",
    "decoderTest = theano.function([X], cost, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO: make a training function\n",
    "runname = 'firstRun'\n",
    "epochCost = []\n",
    "gradNorms = []\n",
    "\n",
    "epochs = 200000\n",
    "batch_size = 64\n",
    "iteration = 0\n",
    "\n",
    "for epoch in xrange(epochs):\n",
    "    \n",
    "    costCollect = []\n",
    "\n",
    "    for start, end in zip(range(0, len(trainData),batch_size), range(batch_size, len(trainData), batch_size)):\n",
    "        \n",
    "        inputs = trainData[start:end]\n",
    "        outputs = targetMaker(inputs)\n",
    "        costfun = train(inputs, outputs)\n",
    "        \n",
    "        \n",
    "        costCollect.append(costfun)\n",
    "                \n",
    "        iteration+=1\n",
    "        \n",
    "    ####SAVE COST TO FILE  \n",
    "    if epoch%30 == 0:\n",
    "        print(' ')\n",
    "        print 'Epoch: ', epoch\n",
    "        epochCost.append(np.mean(costCollect))\n",
    "        print 'Epoch cost average: ', epochCost[-1]\n",
    "        grads = gradientFun(inputs, outputs)\n",
    "        for gra in grads:\n",
    "            print '  gradient norms: ', np.linalg.norm(gra)\n",
    "        \n",
    "    \n",
    "    np.savetxt(runname+\"_COST.csv\", epochCost, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting to CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GPU TO CPU conversion\n",
    "#Now get the weights from the test function. These weights will be numpy arrays\n",
    "w1 = test.get_shared()[0].get_value()\n",
    "\n",
    "#Here the weights are going to be set to the numpy arrays taken from the GPU predict function\n",
    "input_linear.parameters[0].set_value(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.get_shared()[2].get_value().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = '1234567890abcdefghijklmnopqrstuvwxyz'\n",
    "words = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#we add 256 on the end to signify the end of the packet ('EOP')\n",
    "\n",
    "maxPackets = 10 #limit the number of packets\n",
    "tokSessions = []\n",
    "oneHotSessions = []\n",
    "\n",
    "for ses in hexSessions.keys():    \n",
    "    tokPacket = []\n",
    "    oneHotPacket = []\n",
    "    for p in hexSessions[ses][:maxPackets]:\n",
    "        tokP = [hexDict[p[i:i+2]] for i in xrange(0,len(p)-2+1,2)]+[256] #takes hexstring and tokenizes hex pairs\n",
    "        tokPacket.append(tokP)\n",
    "        oneHotPacket.append(oneHot(tokP))\n",
    "\n",
    "    tokSessions.append(tokPacket)\n",
    "    oneHotSessions.append(oneHotPacket)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###ALT RNN LAYER\n",
    "def initialize(to_init):\n",
    "    for bricks in to_init:\n",
    "        bricks.weights_init = initialization.Uniform(width=0.08)\n",
    "        bricks.biases_init = initialization.Constant(0)\n",
    "        bricks.initialize()\n",
    "\n",
    "def gru_layer(dim, h, n):\n",
    "    fork = Fork(output_names=['linear' + str(n), 'gates' + str(n)],\n",
    "                name='fork' + str(n), input_dim=dim, output_dims=[dim, dim * 2])\n",
    "    gru = GatedRecurrent(dim=dim, name='gru' + str(n))\n",
    "    initialize([fork, gru])\n",
    "    linear, gates = fork.apply(h)\n",
    "    return gru.apply(linear, gates)\n",
    "\n",
    "\n",
    "def lstm_layer(dim, h, n):\n",
    "    linear = Linear(input_dim=dim, output_dim=dim * 4, name='linear' + str(n))\n",
    "    lstm = LSTM(dim=dim, name='lstm' + str(n))\n",
    "    initialize([linear, lstm])\n",
    "    return lstm.apply(linear.apply(h))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
