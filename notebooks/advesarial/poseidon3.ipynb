{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#how data are represented at each level (forward, backward, forward with padding on top) needs a little\n",
    "    #experimentation to determine the best representation\n",
    "    #also, is encoding at each layer really the best way? or just feeding the raw through?\n",
    "    \n",
    "#Outside web ips are going to be a problem/messy/noisy. Start by categorizing all outside ips by <OUTSIDE_IP>\n",
    "    #instead of the ip address, or another 4 digit symbol to insert into the hex string.\n",
    "    \n",
    "#to help the models generalize more, for a given source ip address with probability p (say p = 0.1) \n",
    "    #use the token <OTHER_MACHINE>\n",
    "    \n",
    "#should we remove random parts of the header, i.e. checksum\n",
    "\n",
    "#should I take out bias for RNNs?\n",
    "\n",
    "#for the decoder,does the fork encoding need to happen ?\n",
    "    #do we simply cat the hContext with the next words?\n",
    "    \n",
    "#Should the architecture just be encode, context and then prediction???\n",
    "\n",
    "#Input data, should it have character and hex pair encoding as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ['THEANO_FLAGS'] = 'floatX=float32,device=gpu,optimizer=fast_compile'\n",
    "\n",
    "import sys\n",
    "import binascii\n",
    "import multiprocessing as mp\n",
    "from itertools import chain\n",
    "from scapy.all import *\n",
    "sys.path.append('hed-dlg/')\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.stats import itemfreq\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import blocks\n",
    "from blocks.bricks import Linear, Softmax, Softplus, NDimensionalSoftmax, BatchNormalizedMLP, \\\n",
    "                                Rectifier, Logistic, Tanh, MLP\n",
    "from blocks.bricks.recurrent import GatedRecurrent, Fork, LSTM\n",
    "from blocks.initialization import Constant, IsotropicGaussian, Identity, Uniform\n",
    "from blocks.bricks.cost import BinaryCrossEntropy, CategoricalCrossEntropy\n",
    "from blocks.filter import VariableFilter\n",
    "from blocks.roles import PARAMETER\n",
    "from blocks.graph import ComputationGraph\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "\n",
    "###These warnings do not impede progress\n",
    "#WARNING: Failed to execute tcpdump. Check it is installed and in the PATH\n",
    "#WARNING: No route found for IPv6 destination :: (no default route?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataPath = '/data/fs4/datasets/pcaps/smallFlows.pcap'\n",
    "pcaps = rdpcap(dataPath)\n",
    "sessionPrep = pcaps.sessions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ipComs(listOsessions, hexOut = False):\n",
    "    '''\n",
    "    takes scapy sessions\n",
    "    \n",
    "    returns a dictionary of source ips and the ips they talk to, and a list of \n",
    "    all of the unique ip addresses in the data\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    ipAddressDict = {}\n",
    "    uniqIPs = []\n",
    "    \n",
    "    for k,v in listOsessions.items():\n",
    "        for p in v:\n",
    "            sourceIP = p.payload.fields['src']\n",
    "            destIP = p.payload.fields['dst']\n",
    "            \n",
    "            #if source ip is not in dictionary, then add it with dest ip as list\n",
    "            if sourceIP not in ipAddressDict:\n",
    "                ipAddressDict[sourceIP] = [destIP]\n",
    "            \n",
    "            else:\n",
    "                ipAddressDict[sourceIP] = list(set(ipAddressDict[sourceIP]) | set([destIP]))\n",
    "            \n",
    "            uniqIPs = list(set(uniqIPs) | set([destIP, sourceIP]))\n",
    "            \n",
    "    return ipAddressDict, uniqIPs\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sessionDict, uniqips = ipComs(sessionPrep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def oneIpDirSwitcher(normHexSessionList):\n",
    "    '''\n",
    "    input is a list of packets from ONE session\n",
    "    '''\n",
    "    \n",
    "    session = []\n",
    "        \n",
    "    for p in normHexSessionList:\n",
    "        sourceIP = p[52:60]\n",
    "        destIP = p[60:68]\n",
    "\n",
    "        session.append(p[:52]+destIP+sourceIP+p[68:])\n",
    "\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ipDirectionSwitcher(hexSessionsDict):\n",
    "    '''\n",
    "    input is a dictionary of many sessions\n",
    "    '''\n",
    "    badSessions = {}\n",
    "    \n",
    "    for k in hexSessionsDict.keys():\n",
    "        \n",
    "        session = oneIpDirSwitcher(k)\n",
    "        \n",
    "        badSessions[k] = session\n",
    "            \n",
    "    return badSessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#turns the sessions into a dictionary key = session_number, val = list of packages in hex\n",
    "\n",
    "i=0\n",
    "hexSessions = {}\n",
    "\n",
    "for k,v in sessionPrep.items(): # v is the session\n",
    "    #for attr, value in v.__dict__.iteritems(): THIS IS TO GET DICT OF VALUES\n",
    "    #    print attr, value\n",
    "    #if i == 2:\n",
    "    #    break\n",
    "    scpcaps = []    \n",
    "    for p in v: #p is the individual packet in the session\n",
    "        \n",
    "        try:\n",
    "            rawindex = len(p[Raw])\n",
    "            payloadLens.append(rawindex)\n",
    "            scpcaps.append(binascii.hexlify(str(p.original)[:-rawindex])) #turn it into hex\n",
    "        except:\n",
    "            scpcaps.append(binascii.hexlify(p.original))\n",
    "        #for attr, value in p.payload.__dict__.iteritems():#this give the fields that are accessable\n",
    "        #    print attr, value\n",
    "        \n",
    "        #print len(binascii.hexlify(p.original))\n",
    "    hexSessions['session_' + str(i)] = scpcaps\n",
    "    \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Making the hex dictionary\n",
    "hexstring = '0,\t1,\t2,\t3,\t4,\t5,\t6,\t7,\t8,\t9,\tA,\tB,\tC,\tD,\tE,\tF,\t10,\t11,\t12,\t13,\t14,\t15,\t16,\t17,\t18,\t19\\\n",
    ",\t1A,\t1B,\t1C,\t1D,\t1E,\t1F,\t20,\t21,\t22,\t23,\t24,\t25,\t26,\t27,\t28,\t29,\t2A,\t2B,\t2C,\t2D,\t2E,\t2F,\t30,\t31,\t32,\t33,\t34,\t35\\\n",
    ",\t36,\t37,\t38,\t39,\t3A,\t3B,\t3C,\t3D,\t3E,\t3F,\t40,\t41,\t42,\t43,\t44,\t45,\t46,\t47,\t48,\t49,\t4A,\t4B,\t4C,\t4D,\t4E,\t4F,\t50,\t51\\\n",
    ",\t52,\t53,\t54,\t55,\t56,\t57,\t58,\t59,\t5A,\t5B,\t5C,\t5D,\t5E,\t5F,\t60,\t61,\t62,\t63,\t64,\t65,\t66,\t67,\t68,\t69,\t6A,\t6B,\t6C,\t6D\\\n",
    ",\t6E,\t6F,\t70,\t71,\t72,\t73,\t74,\t75,\t76,\t77,\t78,\t79,\t7A,\t7B,\t7C,\t7D,\t7E,\t7F,\t80,\t81,\t82,\t83,\t84,\t85,\t86,\t87,\t88,\t89\\\n",
    ",\t8A,\t8B,\t8C,\t8D,\t8E,\t8F,\t90,\t91,\t92,\t93,\t94,\t95,\t96,\t97,\t98,\t99,\t9A,\t9B,\t9C,\t9D,\t9E,\t9F,\tA0,\tA1,\tA2,\tA3,\tA4,\tA5\\\n",
    ",\tA6,\tA7,\tA8,\tA9,\tAA,\tAB,\tAC,\tAD,\tAE,\tAF,\tB0,\tB1,\tB2,\tB3,\tB4,\tB5,\tB6,\tB7,\tB8,\tB9,\tBA,\tBB,\tBC,\tBD,\tBE,\tBF,\tC0,\tC1\\\n",
    ",\tC2,\tC3,\tC4,\tC5,\tC6,\tC7,\tC8,\tC9,\tCA,\tCB,\tCC,\tCD,\tCE,\tCF,\tD0,\tD1,\tD2,\tD3,\tD4,\tD5,\tD6,\tD7,\tD8,\tD9,\tDA,\tDB,\tDC,\tDD\\\n",
    ",\tDE,\tDF,\tE0,\tE1,\tE2,\tE3,\tE4,\tE5,\tE6,\tE7,\tE8,\tE9,\tEA,\tEB,\tEC,\tED,\tEE,\tEF,\tF0,\tF1,\tF2,\tF3,\tF4,\tF5,\tF6,\tF7,\tF8,\tF9\\\n",
    ",\tFA,\tFB,\tFC,\tFD,\tFE,\tFF'.replace('\\t', '')\n",
    "\n",
    "hexList = hexstring.lower().split(',')\n",
    "hexList.append('<EOP>') #End Of Packet token\n",
    "hexDict = {}\n",
    "    \n",
    "for key, val in enumerate(hexList):\n",
    "    if len(val) == 1:\n",
    "        val = '0'+val\n",
    "    hexDict[val] = key  #dictionary k=hex, v=int  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def oneHot(index, granular = 'hex'):\n",
    "    if granular == 'hex':\n",
    "        vecLen = 257\n",
    "    else:\n",
    "        vecLen = 17\n",
    "    \n",
    "    zeroVec = np.zeros(vecLen)\n",
    "    zeroVec[index] = 1.0\n",
    "    \n",
    "    return zeroVec\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxPackets = 2\n",
    "packetTimeSteps = 66\n",
    "def oneSessionEncoder(sessionPackets, hexDict, maxPackets, packetTimeSteps,\n",
    "                       packetReverse = True, charLevel = False, padOldTimeSteps = True):    \n",
    "            \n",
    "    sessionCollect = []\n",
    "    \n",
    "    if charLevel:\n",
    "        vecLen = 17\n",
    "    else:\n",
    "        vecLen = 257\n",
    "    \n",
    "    if len(sessionPackets) > maxPackets: #crop the number of sessions to maxPackets\n",
    "        sessionList = sessionPackets[:maxPackets]\n",
    "    else:\n",
    "        sessionList = sessionPackets\n",
    "\n",
    "    for packet in sessionList:\n",
    "        packet = [hexDict[packet[i:i+2]] for i in xrange(0,len(packet)-2+1,2)]\n",
    "\n",
    "        if len(packet) >= packetTimeSteps: #crop packet to length packetTimeSteps\n",
    "            packet = packet[:packetTimeSteps-1]\n",
    "\n",
    "        packet = packet+[256] #add <EOP> end of packet token\n",
    "\n",
    "        pacMat = np.array([oneHot(x) for x in packet]) #one hot encoding of packet into a matrix\n",
    "        pacMatLen = len(pacMat)\n",
    "        \n",
    "        #padding packet\n",
    "        if packetReverse:\n",
    "            pacMat = pacMat[::-1]\n",
    "\n",
    "        if pacMatLen < packetTimeSteps:\n",
    "            #pad by stacking zeros on top of data so that earlier timesteps do not have information\n",
    "            #padding the packet such that zeros are after the actual info for better translation\n",
    "            if padOldTimeSteps:\n",
    "                pacMat = np.vstack( ( np.zeros((packetTimeSteps-pacMatLen,vecLen)), pacMat) ) \n",
    "            else:\n",
    "                pacMat = np.vstack( (pacMat, np.zeros((packetTimeSteps-pacMatLen,vecLen))) ) \n",
    "\n",
    "        if pacMatLen > packetTimeSteps:\n",
    "            pacMat = pacMat[:packetTimeSteps, :]\n",
    "\n",
    "        sessionCollect.append(pacMat)\n",
    "\n",
    "    #padding session\n",
    "    sessionCollect = np.asarray(sessionCollect, dtype=theano.config.floatX)\n",
    "    numPacketsInSession = sessionCollect.shape[0]\n",
    "    if numPacketsInSession < maxPackets:\n",
    "        #pad sessions to fit the \n",
    "        sessionCollect = np.vstack( (sessionCollect,np.zeros((maxPackets-numPacketsInSession, \n",
    "                                                             packetTimeSteps, vecLen))) )\n",
    "    return sessionCollect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hexKeys = hexSessions.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hexLens = []\n",
    "pacLens = []\n",
    "for key in hexKeys:\n",
    "    hexLens.append(len(hexSessions[key]))\n",
    "    for pac in hexSessions[key]:\n",
    "        pacLens.append(len(pac)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(pacLens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "#input is a 4d tensor (numMiniBatch, session, packetRow, packetCol)\n",
    "sessions = oneHotSessions(hexSessions)\n",
    "#badSessions = oneHotSessions(badHexSessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def dropout(X, p=0.):\n",
    "    if p != 0:\n",
    "        retain_prob = 1 - p\n",
    "        X = X / retain_prob * srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX)\n",
    "    return X\n",
    "\n",
    "# Gradient clipping\n",
    "def clip_norm(g, c, n): \n",
    "    '''n is the norm, c is the threashold, and g is the gradient'''\n",
    "    \n",
    "    if c > 0: \n",
    "        g = T.switch(T.ge(n, c), g*c/n, g) \n",
    "    return g\n",
    "\n",
    "def clip_norms(gs, c):\n",
    "    norm = T.sqrt(sum([T.sum(g**2) for g in gs]))\n",
    "    return [clip_norm(g, c, norm) for g in gs]\n",
    "\n",
    "# Regularizers\n",
    "def max_norm(p, maxnorm = 0.):\n",
    "    if maxnorm > 0:\n",
    "        norms = T.sqrt(T.sum(T.sqr(p), axis=0))\n",
    "        desired = T.clip(norms, 0, maxnorm)\n",
    "        p = p * (desired/ (1e-7 + norms))\n",
    "    return p\n",
    "\n",
    "def gradient_regularize(p, g, l1 = 0., l2 = 0.):\n",
    "    g += p * l2\n",
    "    g += T.sgn(p) * l1\n",
    "    return g\n",
    "\n",
    "def weight_regularize(p, maxnorm = 0.):\n",
    "    p = max_norm(p, maxnorm)\n",
    "    return p\n",
    "\n",
    "def Adam(params, cost, lr=0.0002, b1=0.1, b2=0.001, e=1e-8, l1 = 0., l2 = 0., maxnorm = 0., c = 8):\n",
    "    \n",
    "    updates = []\n",
    "    grads = T.grad(cost, params)\n",
    "    grads = clip_norms(grads, c)\n",
    "    \n",
    "    i = theano.shared(floatX(0.))\n",
    "    i_t = i + 1.\n",
    "    fix1 = 1. - b1**(i_t)\n",
    "    fix2 = 1. - b2**(i_t)\n",
    "    lr_t = lr * (T.sqrt(fix2) / fix1)\n",
    "    \n",
    "    for p, g in zip(params, grads):\n",
    "        m = theano.shared(p.get_value() * 0.)\n",
    "        v = theano.shared(p.get_value() * 0.)\n",
    "        m_t = (b1 * g) + ((1. - b1) * m)\n",
    "        v_t = (b2 * T.sqr(g)) + ((1. - b2) * v)\n",
    "        g_t = m_t / (T.sqrt(v_t) + e)\n",
    "        g_t = gradient_regularize(p, g_t, l1=l1, l2=l2)\n",
    "        p_t = p - (lr_t * g_t)\n",
    "        p_t = weight_regularize(p_t, maxnorm=maxnorm)\n",
    "        \n",
    "        updates.append((m, m_t))\n",
    "        updates.append((v, v_t))\n",
    "        updates.append((p, p_t))\n",
    "    \n",
    "    updates.append((i, i_t))\n",
    "    return updates\n",
    "\n",
    "def RMSprop(cost, params, lr = 0.001, l1 = 0., l2 = 0., maxnorm = 0., rho=0.9, epsilon=1e-6, c = 8):\n",
    "    \n",
    "    grads = T.grad(cost, params)\n",
    "    grads = clip_norms(grads, c)\n",
    "    updates = []\n",
    "    \n",
    "    for p, g in zip(params, grads):\n",
    "        g = gradient_regularize(p, g, l1 = l1, l2 = l2)\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        updates.append((acc, acc_new))\n",
    "        \n",
    "        updated_p = p - lr * (g / T.sqrt(acc_new + epsilon))\n",
    "        updated_p = weight_regularize(updated_p, maxnorm = maxnorm)\n",
    "        updates.append((p, updated_p))\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#makes output by shifting inputs down in time one step and then copying the last time step to the end.\n",
    "#def targetModifier(targetArray):\n",
    "#    newTarget = np.vstack((targetArray[1:, :], targetArray[-1,:]))\n",
    "#    return newTarget\n",
    "\n",
    "#def targetMaker(listOinputs):\n",
    "    #TODO: do this with arrays\n",
    "#    outputs = []\n",
    "#    for inp in listOinputs:\n",
    "#        outputs.append(targetModifier(inp))\n",
    "#    outputs = np.asarray(outputs)\n",
    "#    \n",
    "#    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = T.tensor4('inputs')\n",
    "Y = T.matrix('targets')\n",
    "\n",
    "dimIn = 257 #hex has 256 characters + the <EOP> character\n",
    "dim = 128 #dimension reduction size\n",
    "rnnType = 'lstm' #gru or lstm\n",
    "bidirectional = False\n",
    "linewt_init = Uniform(width=0.08)\n",
    "line_bias = Constant(0.0)\n",
    "rnnwt_init = IsotropicGaussian(0.08)\n",
    "rnnbias_init = Constant(0.0)\n",
    "packetReverse = False\n",
    "\n",
    "###ENCODER\n",
    "if rnnType == 'gru':\n",
    "    rnn = GatedRecurrent(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'gru')\n",
    "    dimMultiplier = 2\n",
    "else:\n",
    "    rnn = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'lstm')\n",
    "    dimMultiplier = 4\n",
    "\n",
    "fork = Fork(output_names=['linear', 'gates'],\n",
    "            name='fork', input_dim=dimIn, output_dims=[dim, dim * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "\n",
    "###CONTEXT\n",
    "if rnnType == 'gru':\n",
    "    rnnContext = GatedRecurrent(dim=dim, weights_init = rnnwt_init, \n",
    "                                biases_init = rnnbias_init, name = 'gruContext')\n",
    "else:\n",
    "    rnnContext = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, \n",
    "                      name = 'lstmContext')\n",
    "\n",
    "forkContext = Fork(output_names=['linearContext', 'gatesContext'],\n",
    "            name='forkContext', input_dim=dim, output_dims=[dim, dim * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "if bidirectional:\n",
    "    dimDec = dim*2\n",
    "    \n",
    "    if rnnType == 'gru':\n",
    "        rnnContextRev = GatedRecurrent(dim=dim, weights_init = rnnwt_init, \n",
    "                                       biases_init = rnnbias_init, name = 'gruContextRev')\n",
    "        \n",
    "    else:\n",
    "        rnnContextRev = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init,\n",
    "                             name = 'lstmContextRev')\n",
    "    \n",
    "    rnnContextRev.initialize()\n",
    "\n",
    "else:\n",
    "    dimDec = dim\n",
    "\n",
    "\n",
    "###DECODER\n",
    "if rnnType == 'gru':\n",
    "    rnnDec = GatedRecurrent(dim=dimIn, weights_init = rnnwt_init, \n",
    "                            biases_init = rnnbias_init, name = 'gruDecoder')\n",
    "else:\n",
    "    rnnDec = LSTM(dim=dimIn, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'lstmDecoder')\n",
    "\n",
    "\n",
    "forkDec = Fork(output_names=['linear', 'gates'],\n",
    "            name='forkDec', input_dim=dimDec, output_dims=[dim, dim*dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "forkFinal = Fork(output_names=['linear', 'gates'],\n",
    "            name='forkFinal', input_dim=dim, output_dims=[dimIn, dimIn*dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "#reduce dimension of bidirectLSTM\n",
    "\n",
    "fork.initialize()\n",
    "rnn.initialize()\n",
    "\n",
    "forkContext.initialize()\n",
    "rnnContext.initialize()\n",
    "\n",
    "forkDec.initialize()\n",
    "forkFinal.initialize()\n",
    "rnnDec.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "  \n",
    "def oneStep(X):\n",
    "    ###ENCODER\n",
    "    i = 0\n",
    "    data1, data2 = fork.apply(X)\n",
    "\n",
    "    if rnnType == 'gru':\n",
    "        hEnc = rnn.apply(data1, data2)[:,-1] #the [:,-1] gets the last hidden state for each obs in minibatch\n",
    "                                             #i.e. the last state for each sentence\n",
    "    else:\n",
    "        hinit, _ = rnn.apply(data2)\n",
    "        hEnc = hinit[:,-1]\n",
    "    \n",
    "    hEnc = T.reshape(hEnc,(maxPackets, 1, dim))\n",
    "    \n",
    "    data3, data4 = forkContext.apply(hEnc)\n",
    "    \n",
    "    if rnnType == 'gru':\n",
    "        hContext = rnnContext.apply(data3, data4)\n",
    "    else:\n",
    "        hinitContext, _ = rnnContext.apply(data4)\n",
    "        hContext = hinitContext\n",
    "    \n",
    "    if bidirectional:\n",
    "        data3 = data3[::-1]\n",
    "        data4 = data4[::-1]\n",
    "        \n",
    "        if rnnType == 'gru':\n",
    "            hContextRev = rnnContextRev.apply(data3, data4)\n",
    "        else:\n",
    "            hinitContext, _ = rnnContextRev.apply(data4)\n",
    "            hContextRev = hinitContext\n",
    "        \n",
    "        hContext = T.concatenate((hContext, hContextRev), axis=2)\n",
    "            \n",
    "    data5, data6 = forkDec.apply(hContext)\n",
    "    \n",
    "    #decoding data needs to be one timestep (next packet in session) ahead, thus data1 we ignore the first packet\n",
    "    #and the last hidden state of the context RNN.\n",
    "    #Think about L2 pooling before cat\n",
    "    if packetReverse:\n",
    "        data1 = data1[:,::-1]\n",
    "        \n",
    "    data7 = T.concatenate((data5[:-1,:,:], data1[1:, :-1, :]), axis=1) #data1 is the original embedding of X\n",
    "\n",
    "    #data8 = T.concatenate((data7, data5), axis = 2)\n",
    "    data8, data9 = forkFinal.apply(data7)\n",
    "\n",
    "\n",
    "    if rnnType == 'gru':\n",
    "        hDec = rnnDec.apply(data8, data9) \n",
    "    else:\n",
    "        hinit, _ = rnnDec.apply(data9)\n",
    "        hDec = hinit\n",
    "    \n",
    "    softmax = NDimensionalSoftmax()\n",
    "    softout = softmax.apply(hDec, extra_ndim = 1)\n",
    "\n",
    "    precost = X[1:, :, :]*np.log(softout) + (1-X[1:, :, :])*np.log(1-softout)\n",
    "    precost2 = -T.sum(T.sum(precost, axis = 2), axis = 1)\n",
    "    #precost2 = -T.mean(T.sum(T.sum(precost, axis = 2), axis = 1))\n",
    "    i+=1\n",
    "    #precost2 = BinaryCrossEntropy().apply(X[1:, :, :], softout)\n",
    "\n",
    "    #return data4\n",
    "    return precost2, softout, hContext, data5, data7, data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#hContext, _ = theano.scan(fn = oneStep, sequences=[X])\n",
    "[scanOut, softout, hContext, data5, data7, data1], _ = theano.scan(fn = oneStep, sequences=[X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testCont = theano.function([X], [hContext, data7, data5, data1], allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainingSessions = []\n",
    "hexTests = []\n",
    "for trainKey in range(0, 20):\n",
    "    sessionForEncoding = hexSessions[hexSessions.keys()[trainKey]]\n",
    "    hexTests.append(sessionForEncoding)\n",
    "    oneHotSes = oneSessionEncoder(sessionForEncoding,packetReverse=packetReverse, hexDict = hexDict, \n",
    "                                  padOldTimeSteps = packetReverse,\n",
    "                                  maxPackets = maxPackets, \n",
    "                                  packetTimeSteps = packetTimeSteps)\n",
    "    trainingSessions.append(oneHotSes)\n",
    "\n",
    "sessionsMinibatch = np.asarray(trainingSessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sessionsMinibatch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testCont(sessionsMinibatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(testCont(sessionsMinibatch)[0][2][-1].squeeze(), 'b', testCont(sessionsMinibatch)[0][3][-1].squeeze(), 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.where((sessionsMinibatch[1][-1] == sessionsMinibatch[2][-1]) == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    print np.mean(sessionsMinibatch[2][-1]==sessionsMinibatch[i][-1]), np.mean(testCont(sessionsMinibatch)[0][2][-1]==testCont(sessionsMinibatch)[0][i][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = T.mean(scanOut)\n",
    "#cost = BinaryCrossEntropy().apply(Y, softout)\n",
    "cg = ComputationGraph([cost])\n",
    "learning_rate = 0.0001\n",
    "params = VariableFilter(roles = [PARAMETER])(cg.variables)\n",
    "updates = Adam(params, cost, learning_rate, c=5) #c is gradient clipping parameter\n",
    "#updates = RMSprop(cost, params, learning_rate, c=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "gradients = T.grad(cost, params)\n",
    "gradients = clip_norms(gradients, 1)\n",
    "gradientFun = theano.function([X], gradients, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"compiling you beautiful person\"\n",
    "train = theano.function([X], cost, updates = updates, allow_input_downcast=True)\n",
    "#predict = theano.function([X], softout, allow_input_downcast=True)\n",
    "print \"finished compiling\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Eventually we will need to do all of the transformations on the fly so we can pull from disc\n",
    "hexSessionsKeys = hexSessions.keys()\n",
    "#random.shuffle(hexSessionsKeys)\n",
    "trainPercent = 0.9\n",
    "trainIndex = int(len(hexSessionsKeys)*trainPercent)\n",
    "\n",
    "runname = 'hred'\n",
    "epochCost = []\n",
    "gradNorms = []\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 20\n",
    "iteration = 0\n",
    "\n",
    "\n",
    "for epoch in xrange(epochs):\n",
    "    costCollect = []\n",
    "\n",
    "    for start, end in zip(range(0, trainIndex,batch_size), range(batch_size, trainIndex, batch_size)):\n",
    "        #build a 4d array of oneHot sessions\n",
    "        \n",
    "        trainingSessions = []\n",
    "        for trainKey in range(start, end):\n",
    "            sessionForEncoding = hexSessions[hexSessions.keys()[trainKey]]\n",
    "            oneHotSes = oneSessionEncoder(sessionForEncoding, packetReverse=packetReverse, \n",
    "                                          padOldTimeSteps = packetReverse,\n",
    "                                          hexDict = hexDict,\n",
    "                                          maxPackets = maxPackets, packetTimeSteps = packetTimeSteps)\n",
    "            trainingSessions.append(oneHotSes)\n",
    "            \n",
    "        sessionsMinibatch = np.asarray(trainingSessions)\n",
    "        \n",
    "    \n",
    "        costfun = train(sessionsMinibatch)\n",
    "        costCollect.append(costfun)\n",
    "                \n",
    "        iteration+=1\n",
    "        \n",
    "    ####SAVE COST TO FILE  \n",
    "    if epoch%2 == 0:\n",
    "        print(' ')\n",
    "        print 'Epoch: ', epoch\n",
    "        epochCost.append(np.mean(costCollect))\n",
    "        print 'Epoch cost average: ', epochCost[-1]\n",
    "        #grads = gradientFun(inputs, outputs)\n",
    "        #for gra in grads:\n",
    "        #    print '  gradient norms: ', np.linalg.norm(gra)\n",
    "        \n",
    "    np.savetxt(runname+\"_COST.csv\", epochCost, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y = T.matrix('targets')\n",
    "batch_sizeClass = 20\n",
    "numClasses = 2\n",
    "clippings = 5\n",
    "learning_rateClass = 0.001\n",
    "classifierWts = IsotropicGaussian(0.08, 0)\n",
    "\n",
    "bmlp = BatchNormalizedMLP(activations=[Tanh(),Tanh()], \n",
    "           dims=[dimDec, dimDec, 2],\n",
    "           weights_init=classifierWts,\n",
    "           biases_init=Constant(0.0) )\n",
    "\n",
    "\n",
    "bmlp.initialize()\n",
    "\n",
    "def oneStep(X):\n",
    "    ###ENCODER\n",
    "    \n",
    "    data1class, data2class = fork.apply(X) #reusing the weights from fork, rnn, and rnnContext\n",
    "    \n",
    "    if rnnType == 'gru':\n",
    "        hEncclass = rnn.apply(data1class, data2class)[:,-1] #the [:,-1] gets the last hidden state for \n",
    "                                                            #each obs in minibatch i.e. the last state \n",
    "                                                            #for each sentence\n",
    "    else:\n",
    "        hinitclass, _ = rnn.apply(data2class)\n",
    "        hEncclass = hinitclass[:,-1]\n",
    "    \n",
    "    hEncclass = T.reshape(hEncclass,(maxPackets, 1, dim))\n",
    "    \n",
    "    data3class, data4class = forkContext.apply(hEncclass)\n",
    "    \n",
    "    if rnnType == 'gru':\n",
    "        hContextClass = rnnContext.apply(data3class, data4class)\n",
    "    else:\n",
    "        hinitContextClass, _ = rnnContext.apply(data4class)\n",
    "        hContextClass = hinitContextClass\n",
    "    \n",
    "    if bidirectional:\n",
    "        \n",
    "        data3classRev = data3class[::-1]\n",
    "        data4classRev = data4class[::-1]\n",
    "        \n",
    "        if rnnType == 'gru':\n",
    "            hContextRev = rnnContextRev.apply(data3classRev, data4classRev)\n",
    "        else:\n",
    "            hinitContextRevClass, _ = rnnContextRev.apply(data4classRev)\n",
    "            hContextRevClass = hinitContextRevClass\n",
    "        \n",
    "        hContextClass = T.concatenate((hContextClass, hContextRevClass), axis=2)\n",
    "        \n",
    "    return hContextClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hContextClass, _ = theano.scan(fn = oneStep, sequences=[X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#newhContextClass = hContextClass[:, -1].reshape((batch_sizeClass, dimDec)) \n",
    "newhContextClass = T.mean(hContextClass, axis = (1,2))\n",
    "pyx = bmlp.apply(newhContextClass)\n",
    "softmax = Softmax()\n",
    "softoutClass = softmax.apply(pyx)\n",
    "costClass = T.mean(BinaryCrossEntropy().apply(Y, softoutClass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cgClass = ComputationGraph([costClass])\n",
    "paramsClass = VariableFilter(roles = [PARAMETER])(cgClass.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "updatesClass = Adam(paramsClass, costClass, learning_rateClass, c=clippings) \n",
    "#updatesClass = RMSprop(costClass, paramsClass, learning_rateClass, c=clippings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print 'grad compiling'\n",
    "gradients = T.grad(costClass, paramsClass)\n",
    "gradients = clip_norms(gradients, clippings)\n",
    "gradientFun = theano.function([X,Y], gradients, allow_input_downcast=True)\n",
    "print 'finish with grads'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print 'compiling functions you talented soul'\n",
    "classifierTrain = theano.function([X,Y], [softoutClass,costClass], updates=updatesClass, allow_input_downcast=True)\n",
    "#classifierPredict = theano.function([X], softoutClass, allow_input_downcast=True)\n",
    "print 'finished compiling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "runname = 'hredClassify'\n",
    "epochCost = []\n",
    "gradNorms = []\n",
    "\n",
    "epochs = 300\n",
    "iteration = 0\n",
    "\n",
    "normalTarget = np.array([0,1], dtype=theano.config.floatX)\n",
    "abbyTarget = np.array([1,0], dtype=theano.config.floatX)\n",
    "\n",
    "for epoch in xrange(epochs):\n",
    "    costCollect = []\n",
    "\n",
    "    for start, end in zip(range(0, trainIndex,batch_sizeClass/2),\n",
    "                          range(batch_sizeClass/2, trainIndex, batch_sizeClass/2)):\n",
    "\n",
    "        #build a 4d array of oneHot sessions\n",
    "        \n",
    "        trainingTargets = []\n",
    "        trainingSessions = []\n",
    "        for trainKey in range(start, end):\n",
    "            sessionForEncoding = hexSessions[hexSessions.keys()[trainKey]]\n",
    "            \n",
    "            #encode a normal session\n",
    "            oneHotSes = oneSessionEncoder(sessionForEncoding,hexDict = hexDict, packetReverse=packetReverse, \n",
    "                                          padOldTimeSteps = packetReverse, maxPackets = maxPackets,\n",
    "                                          packetTimeSteps = packetTimeSteps)\n",
    "            trainingSessions.append(oneHotSes)\n",
    "            trainingTargets.append(normalTarget)\n",
    "            \n",
    "            #encode an abby normal session\n",
    "            abbyHexSession = oneIpDirSwitcher(sessionForEncoding)\n",
    "            abbyOneHotSes = oneSessionEncoder(abbyHexSession,hexDict = hexDict,packetReverse=packetReverse, \n",
    "                                          padOldTimeSteps = packetReverse, maxPackets = maxPackets, \n",
    "                                              packetTimeSteps = packetTimeSteps)\n",
    "            trainingSessions.append(abbyOneHotSes)\n",
    "            trainingTargets.append(abbyTarget)\n",
    "             \n",
    "        sessionsMinibatch = np.asarray(trainingSessions, dtype=theano.config.floatX)\n",
    "        targetsMinibatch = np.asarray(trainingTargets, dtype=theano.config.floatX)\n",
    "    \n",
    "        costfun = classifierTrain(sessionsMinibatch, targetsMinibatch)\n",
    "        costCollect.append(costfun[1])\n",
    "                \n",
    "        iteration+=1\n",
    "        \n",
    "    ####SAVE COST TO FILE  \n",
    "    if epoch%2 == 0:\n",
    "        print(' ')\n",
    "        print 'Epoch: ', epoch\n",
    "        epochCost.append(np.mean(costCollect))\n",
    "        print 'Epoch cost average: ', epochCost[-1]\n",
    "        #grads = gradientFun(sessionsMinibatch, targetsMinibatch)\n",
    "        print 'pyx: ', costfun[0]\n",
    "        #for gra in grads:\n",
    "        #    print '  gradient norms: ', np.linalg.norm(gra)\n",
    "        \n",
    "    np.savetxt(runname+\"_COST.csv\", epochCost, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advesarial Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a dictionary of packet/session stats\n",
    "\n",
    "#switch ips\n",
    "#switch ports\n",
    "#swap out and replace one ip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tests to show effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#jack up ip field and see diff in prediction before and after\n",
    "#jack up checksum and see diff in prediciton before and after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim = 257 #original data dimension/timesteps/columns\n",
    "rnnType = 'gru' #gru or lstm\n",
    "bidirectional = True\n",
    "X = T.tensor3('inputs')\n",
    "Xrev = T.matrix('reversed_inputs')\n",
    "linewt_init = Uniform(width=0.08)\n",
    "rnnwt_init = IsotropicGaussian(0.05)\n",
    "rnnbias_init = Constant(0.0)\n",
    "\n",
    "if rnnType == 'gru':\n",
    "    rnn = GatedRecurrent(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'gru')\n",
    "    dimMultiplier = 2\n",
    "else:\n",
    "    rnn = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'lstm')\n",
    "    dimMultiplier = 4\n",
    "\n",
    "###RECURRENT LAYER\n",
    "\n",
    "#To use or not to use that is the question\n",
    "fork = Fork(output_names=['linear', 'gates'],\n",
    "            name='fork', input_dim=dim, output_dims=[dim, dim * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = rnnbias_init)\n",
    "data1, data2 = fork.apply(X)\n",
    "\n",
    "###for raw inputs\n",
    "#data1 = X\n",
    "#data2 = T.concatenate([X]*dimMultiplier, axis=2)\n",
    "\n",
    "if rnnType == 'gru':\n",
    "    hEnc = rnn.apply(data1, data2)[:,-1] #the [:,-1] gets the last hidden state for each obs in minibatch\n",
    "                                         #i.e. the last state for each sentence\n",
    "else:\n",
    "    hinit, _ = rnn.apply(data2)\n",
    "    hEnc = hinit[:,-1]\n",
    "\n",
    "hEnc = T.reshape(hEnc,(maxPackets, 1, dim))\n",
    "#get weights initialized. without weights are nans.\n",
    "fork.initialize()\n",
    "rnn.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TEST Encoder will return a maxPackets x packet length matrix\n",
    "encoder = theano.function([X], hEnc, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TEST ENCODED PACKETS shape = (maxPackets, 1, dim)\n",
    "encoder(sessions[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "X = T.tensor4('testInputs')\n",
    "Y = T.vector('targets')\n",
    "dim = 257 #original data dimension/timesteps/columns\n",
    "dimDec = dim*2\n",
    "rnnType = 'gru' #gru or lstm\n",
    "bidirectional = True\n",
    "linewt_init = Uniform(width=0.05)\n",
    "rnnwt_init = IsotropicGaussian(0.02)\n",
    "rnnbias_init = Constant(0.0)\n",
    "\n",
    "def oneStep(X):\n",
    "    ###ENCODER\n",
    "    if rnnType == 'gru':\n",
    "        rnn = GatedRecurrent(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'gru')\n",
    "        dimMultiplier = 2\n",
    "    else:\n",
    "        rnn = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'lstm')\n",
    "        dimMultiplier = 4\n",
    "    \n",
    "    #To use (fork) or not to use that is the question\n",
    "    fork = Fork(output_names=['linear', 'gates'],\n",
    "                name='fork', input_dim=dim, output_dims=[dim, dim * dimMultiplier], \n",
    "                weights_init = linewt_init, biases_init = rnnbias_init)\n",
    "    data1, data2 = fork.apply(X)\n",
    "\n",
    "    ###for raw inputs... for not using fork\n",
    "    #data1 = X\n",
    "    #data2 = T.concatenate([X]*dimMultiplier, axis=2)\n",
    "\n",
    "    if rnnType == 'gru':\n",
    "        hEnc = rnn.apply(data1, data2)[:,-1] #the [:,-1] gets the last hidden state for each obs in minibatch\n",
    "                                             #i.e. the last state for each sentence\n",
    "    else:\n",
    "        hinit, _ = rnn.apply(data2)\n",
    "        hEnc = hinit[:,-1]\n",
    "\n",
    "    hEnc = T.reshape(hEnc,(maxPackets, 1, dim))\n",
    "\n",
    "    fork.initialize()\n",
    "    rnn.initialize()\n",
    "\n",
    "    \n",
    "    ###CONTEXT\n",
    "    if rnnType == 'gru':\n",
    "        rnnContext = GatedRecurrent(dim=dim, weights_init = rnnwt_init, \n",
    "                                    biases_init = rnnbias_init, name = 'gruContext')\n",
    "        dimMultiplier = 2\n",
    "    else:\n",
    "        rnnContext = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, \n",
    "                          name = 'lstmContext')\n",
    "        dimMultiplier = 4\n",
    "\n",
    "    ###RECURRENT LAYER\n",
    "    forkContext = Fork(output_names=['linearContext', 'gatesContext'],\n",
    "                name='forkContext', input_dim=dim, output_dims=[dim, dim * dimMultiplier], \n",
    "                weights_init = linewt_init, biases_init = rnnbias_init)\n",
    "    data3, data4 = forkContext.apply(hEnc)\n",
    "\n",
    "    if rnnType == 'gru':\n",
    "        hContext = rnnContext.apply(data3, data4)\n",
    "    else:\n",
    "        hinitContext, _ = rnnContext.apply(data4)\n",
    "        hContext = hinitContext\n",
    "\n",
    "    #THINK ABOUT ADDING L2 POOLING BEFORE CAT\n",
    "    if bidirectional:\n",
    "\n",
    "        data3 = data3[::-1]\n",
    "        data4 = data4[::-1]\n",
    "\n",
    "        if rnnType == 'gru':\n",
    "            rnnContextRev = GatedRecurrent(dim=dim, weights_init = rnnwt_init, \n",
    "                                           biases_init = rnnbias_init, name = 'gruContextRev')\n",
    "            hContextRev = rnnContextRev.apply(data3, data4)\n",
    "        else:\n",
    "            rnnContextRev = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init,\n",
    "                                 name = 'lstmContextRev')\n",
    "            hinitContext, _ = rnnContextRev.apply(data4)\n",
    "            hContextRev = hinitContext\n",
    "\n",
    "\n",
    "        hContext = T.concatenate((hContext, hContextRev), axis=2)\n",
    "        rnnContextRev.initialize()\n",
    "\n",
    "    forkContext.initialize()\n",
    "    rnnContext.initialize()\n",
    "\n",
    "    \n",
    "    ###DECODER\n",
    "    \n",
    "    if rnnType == 'gru':\n",
    "        rnnDec = GatedRecurrent(dim=dim, weights_init = rnnwt_init, \n",
    "                                biases_init = rnnbias_init, name = 'gruDecoder')\n",
    "        dimMultiplier = 2\n",
    "    else:\n",
    "        rnnDec = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'lstmDecoder')\n",
    "        dimMultiplier = 4\n",
    "\n",
    "\n",
    "    forkDec = Fork(output_names=['linear', 'gates'],\n",
    "                name='forkDec', input_dim=dimDec, output_dims=[dim, dim*dimMultiplier], \n",
    "                weights_init = linewt_init, biases_init = rnnbias_init)\n",
    "\n",
    "    forkFinal = Fork(output_names=['linear', 'gates'],\n",
    "                name='forkDec', input_dim=dim, output_dims=[dim, dim*dimMultiplier], \n",
    "                weights_init = linewt_init, biases_init = rnnbias_init)\n",
    "\n",
    "    data5, data6 = forkDec.apply(hContext)#reduce dimension of bidirectLSTM\n",
    "\n",
    "    #decoding data needs to be one timestep (next packet in session) ahead, thus data1 we ignore the first packet\n",
    "    #and the last hidden state of the context RNN.\n",
    "    data7 = T.concatenate((data5[:-1,:,:], data1[1:, :-1, :]), axis=1) #data1 is the original embedding of X\n",
    "\n",
    "    #data8 = T.concatenate((data7, data5), axis = 2)\n",
    "    data8, data9 = forkFinal.apply(data7)\n",
    "\n",
    "\n",
    "    if rnnType == 'gru':\n",
    "        hDec = rnnDec.apply(data8, data9) \n",
    "    else:\n",
    "        hinit, _ = rnnDec.apply(data9)\n",
    "        hDec = hinit\n",
    "\n",
    "    #Smooth out the probabilities of hDec\n",
    "    softmax = NDimensionalSoftmax()\n",
    "    softout = softmax.apply(hDec, extra_ndim = 1)\n",
    "\n",
    "\n",
    "    precost = X[1:, :, :]*np.log(softout) + (1-X[1:, :, :])*np.log(1-softout)\n",
    "    precost2 = -T.sum(T.sum(precost, axis = 2), axis = 1)\n",
    "    #precost2 = -T.mean(T.sum(T.sum(precost, axis = 2), axis = 1))\n",
    "    \n",
    "    #cost = BinaryCrossEntropy().apply(X[1:, :, :], softout)\n",
    "\n",
    "    forkDec.initialize()\n",
    "    forkFinal.initialize()\n",
    "    rnnDec.initialize()\n",
    "    \n",
    "    return precost2, hEnc, hContext, hDec, softout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "if rnnType == 'gru':\n",
    "    rnnContext = GatedRecurrent(dim=dim, weights_init = rnnwt_init, \n",
    "                                biases_init = rnnbias_init, name = 'gruContext')\n",
    "    dimMultiplier = 2\n",
    "else:\n",
    "    rnnContext = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, \n",
    "                      name = 'lstmContext')\n",
    "    dimMultiplier = 4\n",
    "\n",
    "\n",
    "###ICLR suggestion -> don't use bias in RNNs\n",
    "\n",
    "###RECURRENT LAYER\n",
    "forkContext = Fork(output_names=['linearContext', 'gatesContext'],\n",
    "            name='forkContext', input_dim=dim, output_dims=[dim, dim * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = rnnbias_init)\n",
    "data3, data4 = forkContext.apply(hEnc)\n",
    "\n",
    "if rnnType == 'gru':\n",
    "    hContext = rnnContext.apply(data3, data4)\n",
    "else:\n",
    "    hinitContext, _ = rnnContext.apply(data4)\n",
    "    hContext = hinitContext\n",
    "\n",
    "#THINK ABOUT ADDING L2 POOLING BEFORE CAT\n",
    "if bidirectional:\n",
    "    \n",
    "    data3 = data3[::-1]\n",
    "    data4 = data4[::-1]\n",
    "    \n",
    "    if rnnType == 'gru':\n",
    "        rnnContextRev = GatedRecurrent(dim=dim, weights_init = rnnwt_init, \n",
    "                                       biases_init = rnnbias_init, name = 'gruContextRev')\n",
    "        hContextRev = rnnContextRev.apply(data3, data4)\n",
    "    else:\n",
    "        rnnContextRev = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init,\n",
    "                             name = 'lstmContextRev')\n",
    "        hinitContext, _ = rnnContextRev.apply(data4)\n",
    "        hContextRev = hinitContext\n",
    "    \n",
    "    \n",
    "    hContext = T.concatenate((hContext, hContextRev), axis=2)\n",
    "    rnnContextRev.initialize()\n",
    "    \n",
    "#get weights initialized. without weights are nans.\n",
    "forkContext.initialize()\n",
    "rnnContext.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TEST output shape = (maxPackets, 1, dim*2)\n",
    "context = theano.function([X], hContext, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TEST\n",
    "context(sessions[1]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dimDec = dim*2\n",
    "\n",
    "if rnnType == 'gru':\n",
    "    rnnDec = GatedRecurrent(dim=dim, weights_init = rnnwt_init, \n",
    "                            biases_init = rnnbias_init, name = 'gruDecoder')\n",
    "    dimMultiplier = 2\n",
    "else:\n",
    "    rnnDec = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'lstmDecoder')\n",
    "    dimMultiplier = 4\n",
    "\n",
    "\n",
    "forkDec = Fork(output_names=['linear', 'gates'],\n",
    "            name='forkDec', input_dim=dimDec, output_dims=[dim, dim*dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = rnnbias_init)\n",
    "\n",
    "forkFinal = Fork(output_names=['linear', 'gates'],\n",
    "            name='forkDec', input_dim=dim, output_dims=[dim, dim*dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = rnnbias_init)\n",
    "\n",
    "data5, data6 = forkDec.apply(hContext)#reduce dimension of bidirectLSTM\n",
    "\n",
    "#decoding data needs to be one timestep (next packet in session) ahead, thus data1 we ignore the first packet\n",
    "#and the last hidden state of the context RNN.\n",
    "data7 = T.concatenate((data5[:-1,:,:], data1[1:, :-1, :]), axis=1) #data1 is the original embedding of X\n",
    "\n",
    "#data8 = T.concatenate((data7, data5), axis = 2)\n",
    "data8, data9 = forkFinal.apply(data7)\n",
    "\n",
    "\n",
    "if rnnType == 'gru':\n",
    "    hDec = rnnDec.apply(data8, data9) \n",
    "else:\n",
    "    hinit, _ = rnnDec.apply(data9)\n",
    "    hDec = hinit\n",
    "\n",
    "#Smooth out the probabilities of hDec\n",
    "softmax = NDimensionalSoftmax()\n",
    "softout = softmax.apply(hDec, extra_ndim = 1)\n",
    "    \n",
    "\n",
    "precost = X[1:, :, :]*np.log(softout) + (1-X[1:, :, :])*np.log(1-softout)\n",
    "cost = -T.mean(T.sum(T.sum(precost, axis = 2), axis = 1))\n",
    "#cost = BinaryCrossEntropy().apply(X[1:, :, :], softout)\n",
    "\n",
    "#get weights initialized\n",
    "forkDec.initialize()\n",
    "forkFinal.initialize()\n",
    "rnnDec.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "   \n",
    "'''def oneHotSessions(sessionDict, hexDict = hexDict, maxPackets = maxPackets, packetTimeSteps = packetTimeSteps,\n",
    "                   reverse = False, charLevel = False):\n",
    "    \"\"\"\n",
    "    This takes a list of int tokens and onehot encodes them, pads sessions with zero tensors according to maxPackets\n",
    "    and packet according to packetTimeSteps\n",
    "    \n",
    "    sessionDict = dict of lists of key = sessions and value = list of packets\n",
    "    timeSteps = maximum len of packet. it will be padded with zero vectors is packet is too short.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    listOsessions = []\n",
    "\n",
    "    if charLevel:\n",
    "        vecLen = 17\n",
    "    else:\n",
    "        vecLen = 257\n",
    "    \n",
    "    sessionKeys = sessionDict.keys()\n",
    "    \n",
    "    for session in sessionKeys:\n",
    "        \n",
    "        sessionCollect = oneSessionEncoder(session, hexDict = hexDict, maxPackets = maxPackets, \n",
    "                                           packetTimeSteps = packetTimeSteps, reverse = reverse, \n",
    "                                           charLevel = charLevel )\n",
    "        \n",
    "        listOsessions.append(sessionCollect)\n",
    "        \n",
    "    return listOsessions'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TEST\n",
    "decoder = theano.function([X], cost, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X = T.tensor4('input')\n",
    "decoderTest = theano.function([X], cost, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO: make a training function\n",
    "runname = 'firstRun'\n",
    "epochCost = []\n",
    "gradNorms = []\n",
    "\n",
    "epochs = 200000\n",
    "batch_size = 64\n",
    "iteration = 0\n",
    "\n",
    "for epoch in xrange(epochs):\n",
    "    \n",
    "    costCollect = []\n",
    "\n",
    "    for start, end in zip(range(0, len(trainData),batch_size), range(batch_size, len(trainData), batch_size)):\n",
    "        \n",
    "        inputs = trainData[start:end]\n",
    "        outputs = targetMaker(inputs)\n",
    "        costfun = train(inputs, outputs)\n",
    "        \n",
    "        \n",
    "        costCollect.append(costfun)\n",
    "                \n",
    "        iteration+=1\n",
    "        \n",
    "    ####SAVE COST TO FILE  \n",
    "    if epoch%30 == 0:\n",
    "        print(' ')\n",
    "        print 'Epoch: ', epoch\n",
    "        epochCost.append(np.mean(costCollect))\n",
    "        print 'Epoch cost average: ', epochCost[-1]\n",
    "        grads = gradientFun(inputs, outputs)\n",
    "        for gra in grads:\n",
    "            print '  gradient norms: ', np.linalg.norm(gra)\n",
    "        \n",
    "    \n",
    "    np.savetxt(runname+\"_COST.csv\", epochCost, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting to CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GPU TO CPU conversion\n",
    "#Now get the weights from the test function. These weights will be numpy arrays\n",
    "w1 = test.get_shared()[0].get_value()\n",
    "\n",
    "#Here the weights are going to be set to the numpy arrays taken from the GPU predict function\n",
    "input_linear.parameters[0].set_value(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.get_shared()[2].get_value().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = '1234567890abcdefghijklmnopqrstuvwxyz'\n",
    "words = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#we add 256 on the end to signify the end of the packet ('EOP')\n",
    "\n",
    "maxPackets = 10 #limit the number of packets\n",
    "tokSessions = []\n",
    "oneHotSessions = []\n",
    "\n",
    "for ses in hexSessions.keys():    \n",
    "    tokPacket = []\n",
    "    oneHotPacket = []\n",
    "    for p in hexSessions[ses][:maxPackets]:\n",
    "        tokP = [hexDict[p[i:i+2]] for i in xrange(0,len(p)-2+1,2)]+[256] #takes hexstring and tokenizes hex pairs\n",
    "        tokPacket.append(tokP)\n",
    "        oneHotPacket.append(oneHot(tokP))\n",
    "\n",
    "    tokSessions.append(tokPacket)\n",
    "    oneHotSessions.append(oneHotPacket)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###ALT RNN LAYER\n",
    "def initialize(to_init):\n",
    "    for bricks in to_init:\n",
    "        bricks.weights_init = initialization.Uniform(width=0.08)\n",
    "        bricks.biases_init = initialization.Constant(0)\n",
    "        bricks.initialize()\n",
    "\n",
    "def gru_layer(dim, h, n):\n",
    "    fork = Fork(output_names=['linear' + str(n), 'gates' + str(n)],\n",
    "                name='fork' + str(n), input_dim=dim, output_dims=[dim, dim * 2])\n",
    "    gru = GatedRecurrent(dim=dim, name='gru' + str(n))\n",
    "    initialize([fork, gru])\n",
    "    linear, gates = fork.apply(h)\n",
    "    return gru.apply(linear, gates)\n",
    "\n",
    "\n",
    "def lstm_layer(dim, h, n):\n",
    "    linear = Linear(input_dim=dim, output_dim=dim * 4, name='linear' + str(n))\n",
    "    lstm = LSTM(dim=dim, name='lstm' + str(n))\n",
    "    initialize([linear, lstm])\n",
    "    return lstm.apply(linear.apply(h))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
