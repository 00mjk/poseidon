{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#how data are represented at each level (forward, backward, forward with padding on top) needs a little\n",
    "    #experimentation to determine the best representation\n",
    "    #also, is encoding at each layer really the best way? or just feeding the raw through?\n",
    "    \n",
    "#Outside web ips are going to be a problem/messy/noisy. Start by categorizing all outside ips by <OUTSIDE_IP>\n",
    "    #instead of the ip address, or another 4 digit symbol to insert into the hex string.\n",
    "    \n",
    "#to help the models generalize more, for a given source ip address with probability p (say p = 0.1) \n",
    "    #use the token <OTHER_MACHINE>\n",
    "    \n",
    "#should we remove random parts of the header, i.e. checksum\n",
    "\n",
    "#should I take out bias for RNNs?\n",
    "\n",
    "#for the decoder,does the fork encoding need to happen ?\n",
    "    #do we simply cat the hContext with the next words?\n",
    "    \n",
    "#Should the architecture just be encode, context and then prediction???\n",
    "\n",
    "#Input data, should it have character and hex pair encoding as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: No route found for IPv6 destination :: (no default route?)\n",
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled, CuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ['THEANO_FLAGS'] = 'floatX=float32,device=gpu'#,optimizer=fast_compile'\n",
    "\n",
    "import sys\n",
    "import binascii\n",
    "import multiprocessing as mp\n",
    "from itertools import chain\n",
    "from scapy.all import *\n",
    "sys.path.append('hed-dlg/')\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.stats import itemfreq\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import copy\n",
    "\n",
    "import blocks\n",
    "from blocks.bricks import Linear, Softmax, Softplus, NDimensionalSoftmax, BatchNormalizedMLP, \\\n",
    "                                Rectifier, Logistic, Tanh, MLP\n",
    "from blocks.bricks.recurrent import GatedRecurrent, Fork, LSTM\n",
    "from blocks.initialization import Constant, IsotropicGaussian, Identity, Uniform\n",
    "from blocks.bricks.cost import BinaryCrossEntropy, CategoricalCrossEntropy\n",
    "from blocks.filter import VariableFilter\n",
    "from blocks.roles import PARAMETER\n",
    "from blocks.graph import ComputationGraph\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "\n",
    "###These warnings do not impede progress\n",
    "#WARNING: Failed to execute tcpdump. Check it is installed and in the PATH\n",
    "#WARNING: No route found for IPv6 destination :: (no default route?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataPath = '/data/fs4/datasets/pcaps/smallFlows.pcap'\n",
    "pcaps = rdpcap(dataPath)\n",
    "sessionPrep = pcaps.sessions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def parse_header(line):\n",
    "    ret_dict = {}\n",
    "    h = line.split()\n",
    "    #ret_dict['direction'] = \" \".join(h[3:6])\n",
    "\n",
    "    if h[2] == 'IP6':\n",
    "        \"\"\"\n",
    "        Conditional formatting based on ethernet type.\n",
    "        IPv4 format: 0.0.0.0.port\n",
    "        IPv6 format (one of many): 0:0:0:0:0:0.port\n",
    "        \"\"\"\n",
    "        ret_dict['src_port'] = h[3].split('.')[-1]\n",
    "        ret_dict['src_ip'] = h[3].split('.')[0]\n",
    "\n",
    "        ret_dict['dest_port'] = h[5].split('.')[-1].split(':')[0]\n",
    "        ret_dict['dest_ip'] = h[5].split('.')[0]\n",
    "    else:\n",
    "        if len(h[3].split('.')) > 4:\n",
    "            ret_dict['src_port'] = h[3].split('.')[-1]\n",
    "            ret_dict['src_ip'] = '.'.join(h[3].split('.')[:-1])\n",
    "        else:\n",
    "            ret_dict['src_ip'] = h[3]\n",
    "            ret_dict['src_port'] = ''\n",
    "\n",
    "        if len(h[5].split('.')) > 4:\n",
    "            ret_dict['dest_port'] = h[5].split('.')[-1].split(':')[0]\n",
    "            ret_dict['dest_ip'] = '.'.join(h[5].split('.')[:-1])\n",
    "        else:\n",
    "            ret_dict['dest_ip'] = h[5].split(':')[0]\n",
    "            ret_dict['dest_port'] = ''\n",
    "\n",
    "    try:\n",
    "        if ret_dict['src_port'] == '53' or ret_dict['dst_port'] == '53':\n",
    "            ret_dict['length'] = int(h[-1][1:-1])\n",
    "        else:\n",
    "            try:\n",
    "                ret_dict['length'] = int(line.split(' length ')[1].split(':')[0])\n",
    "            except:\n",
    "                ret_dict['length'] = 0\n",
    "    except:\n",
    "        try:\n",
    "            ret_dict['length'] = int(line.split(' length ')[1].split(':')[0])\n",
    "        except:\n",
    "            ret_dict['length'] = 0\n",
    "\n",
    "    return ret_dict\n",
    "\n",
    "def parse_data(line, length):\n",
    "    ret_str = ''\n",
    "    h, d = line.split(':', 1)\n",
    "    ret_str = d.strip().replace(' ', '')\n",
    "    if length != 0:\n",
    "        ret_str = ret_str[:-(2 * length)]\n",
    "    return ret_str\n",
    "\n",
    "def process_packet(output):\n",
    "    # TODO!! throws away the first packet!\n",
    "    try:\n",
    "        ret_header = {}\n",
    "        ret_dict = {}\n",
    "        ret_data = ''\n",
    "        for line in output:\n",
    "            line = line.strip()\n",
    "            if line.startswith('0x'):\n",
    "                data = parse_data(line, ret_header['length'])\n",
    "                ret_data = ret_data + data\n",
    "            else:\n",
    "                ret_header = parse_header(line)\n",
    "                ret_dict.update(ret_header)\n",
    "                if ret_data != '':\n",
    "                    ret_dict['data'] = ret_data\n",
    "                    ret_data = ''\n",
    "                yield ret_dict\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "def read_pcap(path):\n",
    "    hex_sessions = {}\n",
    "    proc = subprocess.Popen('tcpdump -nn -tttt -xx -r '+path,\n",
    "                            shell=True,\n",
    "                            stdout=subprocess.PIPE)\n",
    "    for packet in process_packet(proc.stdout):\n",
    "        a = str(packet)\n",
    "    \n",
    "        a = ast.literal_eval(a)\n",
    "        if 'data' in a:\n",
    "            key = (a['src_ip']+\":\"+a['src_port'], a['dest_ip']+\":\"+a['dest_port'])\n",
    "            keys = [k for k in hex_sessions if (k[0] == key[0] and k[1] == key[1]) or (k[0] == key[1] and k[1] == key[0])]\n",
    "            if len(keys) > 0:\n",
    "                #hex_sessions[keys[0]].append(a['direction']+a['data'])\n",
    "                hex_sessions[keys[0]].append(a['data'])\n",
    "            else:\n",
    "                #hex_sessions[key] = [a['direction']+a['data']]\n",
    "                hex_sessions[key] = [a['data']]\n",
    "    return hex_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hexSessions = read_pcap(dataPath)\n",
    "\n",
    "for ses in hexSessions.keys():\n",
    "    paclens = []\n",
    "    for pac in hexSessions[ses]:\n",
    "        paclens.append(len(pac))\n",
    "    if np.min(paclens)<80:\n",
    "        del hexSessions[ses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#turns the sessions into a dictionary key = session_number, val = list of packages in hex\n",
    "minPackets = 2\n",
    "\n",
    "def hexSessionDictCreator(scapySessions, minPackets = 2):\n",
    "\n",
    "    hexSessions = OrderedDict()\n",
    "    i=0\n",
    "    for k,v in sessionPrep.items(): # v is the session\n",
    "        \n",
    "        if len(v) < minPackets:\n",
    "            pass\n",
    "        \n",
    "        else:\n",
    "            scpcaps = []    \n",
    "            \n",
    "            for p in v: #p is the individual packet in the session  \n",
    "                \n",
    "                try: #getting rid of payload\n",
    "                    rawindex = len(p[Raw]) \n",
    "                    #payloadLens.append(rawindex)\n",
    "                    scpcaps.append(binascii.hexlify(str(p.original)[:-rawindex])) #turn it into hex\n",
    "                \n",
    "                except: #if no payload\n",
    "                    scpcaps.append(binascii.hexlify(str(p.original)))\n",
    "    \n",
    "            hexSessions['session_' + str(i)] = scpcaps\n",
    "            i+=1\n",
    "    #assert that all sessions have len of at least minPackets\n",
    "    return hexSessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hexSessionstest = hexSessionDictCreator(sessionPrep, minPackets=minPackets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Making the hex dictionary\n",
    "def hexTokenizer():\n",
    "    hexstring = '0,\t1,\t2,\t3,\t4,\t5,\t6,\t7,\t8,\t9,\tA,\tB,\tC,\tD,\tE,\tF,\t10,\t11,\t12,\t13,\t14,\t15,\t16,\t17,\t18,\t19\\\n",
    "    ,\t1A,\t1B,\t1C,\t1D,\t1E,\t1F,\t20,\t21,\t22,\t23,\t24,\t25,\t26,\t27,\t28,\t29,\t2A,\t2B,\t2C,\t2D,\t2E,\t2F,\t30,\t31,\t32,\t33,\t34,\t35\\\n",
    "    ,\t36,\t37,\t38,\t39,\t3A,\t3B,\t3C,\t3D,\t3E,\t3F,\t40,\t41,\t42,\t43,\t44,\t45,\t46,\t47,\t48,\t49,\t4A,\t4B,\t4C,\t4D,\t4E,\t4F,\t50,\t51\\\n",
    "    ,\t52,\t53,\t54,\t55,\t56,\t57,\t58,\t59,\t5A,\t5B,\t5C,\t5D,\t5E,\t5F,\t60,\t61,\t62,\t63,\t64,\t65,\t66,\t67,\t68,\t69,\t6A,\t6B,\t6C,\t6D\\\n",
    "    ,\t6E,\t6F,\t70,\t71,\t72,\t73,\t74,\t75,\t76,\t77,\t78,\t79,\t7A,\t7B,\t7C,\t7D,\t7E,\t7F,\t80,\t81,\t82,\t83,\t84,\t85,\t86,\t87,\t88,\t89\\\n",
    "    ,\t8A,\t8B,\t8C,\t8D,\t8E,\t8F,\t90,\t91,\t92,\t93,\t94,\t95,\t96,\t97,\t98,\t99,\t9A,\t9B,\t9C,\t9D,\t9E,\t9F,\tA0,\tA1,\tA2,\tA3,\tA4,\tA5\\\n",
    "    ,\tA6,\tA7,\tA8,\tA9,\tAA,\tAB,\tAC,\tAD,\tAE,\tAF,\tB0,\tB1,\tB2,\tB3,\tB4,\tB5,\tB6,\tB7,\tB8,\tB9,\tBA,\tBB,\tBC,\tBD,\tBE,\tBF,\tC0,\tC1\\\n",
    "    ,\tC2,\tC3,\tC4,\tC5,\tC6,\tC7,\tC8,\tC9,\tCA,\tCB,\tCC,\tCD,\tCE,\tCF,\tD0,\tD1,\tD2,\tD3,\tD4,\tD5,\tD6,\tD7,\tD8,\tD9,\tDA,\tDB,\tDC,\tDD\\\n",
    "    ,\tDE,\tDF,\tE0,\tE1,\tE2,\tE3,\tE4,\tE5,\tE6,\tE7,\tE8,\tE9,\tEA,\tEB,\tEC,\tED,\tEE,\tEF,\tF0,\tF1,\tF2,\tF3,\tF4,\tF5,\tF6,\tF7,\tF8,\tF9\\\n",
    "    ,\tFA,\tFB,\tFC,\tFD,\tFE,\tFF'.replace('\\t', '')\n",
    "\n",
    "    hexList = [x.strip() for x in hexstring.lower().split(',')]\n",
    "    hexList.append('<EOP>') #End Of Packet token\n",
    "    #EOS token??????\n",
    "    hexDict = {}\n",
    "\n",
    "    for key, val in enumerate(hexList):\n",
    "        if len(val) == 1:\n",
    "            val = '0'+val\n",
    "        hexDict[val] = key  #dictionary k=hex, v=int  \n",
    "    \n",
    "    return hexDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hexDict = hexTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary of IP communications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def srcIpDict(hexSessionDict):\n",
    "    ''' \n",
    "    input: dictionary of key = sessions, value = list of HEX HEADERS of packets in session\n",
    "    output: dictionary of key = source IP, value/subkey = dictionary of destination IPs, \n",
    "                                           subvalue = [[sport], [dport], [plen], [protocol]]\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    srcIpDict = {}   \n",
    "    uniqIPs = [] #some ips are dest only. this will collect all ips, not just srcIpDict.keys()\n",
    "    \n",
    "    for session in hexSessionDict.keys():\n",
    "        \n",
    "        for rawpacket in hexSessionDict[session]:\n",
    "            packet = copy(rawpacket)\n",
    "            \n",
    "            dstIpSubDict = {}\n",
    "\n",
    "            srcip = packet[52:60]\n",
    "            dstip = packet[60:68]\n",
    "            sport = packet[68:72]\n",
    "            dport = packet[72:76]\n",
    "            plen = packet[32:36]\n",
    "            protocol = packet[46:48]\n",
    "            \n",
    "            if len(srcip) == 8 and len(dstip) == 8:\n",
    "                uniqIPs = list(set(uniqIPs) | set([dstip, srcip]))\n",
    "\n",
    "                if srcip not in srcIpDict:\n",
    "                    dstIpSubDict[dstip] = [[sport], [dport], [plen], [protocol]]\n",
    "                    srcIpDict[srcip] = dstIpSubDict\n",
    "\n",
    "                if dstip not in srcIpDict[srcip]:    \n",
    "                    srcIpDict[srcip][dstip] = [[sport], [dport], [plen], [protocol]]\n",
    "                else:\n",
    "                    srcIpDict[srcip][dstip][0].append(sport)\n",
    "                    srcIpDict[srcip][dstip][1].append(dport)\n",
    "                    srcIpDict[srcip][dstip][2].append(plen)\n",
    "                    srcIpDict[srcip][dstip][3].append(protocol)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    return srcIpDict, uniqIPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def srcIpDict(hexSessionDict):\n",
    "    ''' \n",
    "    input: dictionary of key = sessions, value = list of HEX HEADERS of packets in session\n",
    "    output: dictionary of key = source IP, value/subkey = dictionary of destination IPs, \n",
    "                                           subvalue = [[sport], [dport], [plen], [protocol]]\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    srcIpDict = OrderedDict()   \n",
    "    uniqIPs = [] #some ips are dest only. this will collect all ips, not just srcIpDict.keys()\n",
    "    \n",
    "    for session in hexSessionDict.keys():\n",
    "        \n",
    "        for rawpacket in hexSessionDict[session]:\n",
    "            packet = copy(rawpacket)\n",
    "            \n",
    "            dstIpSubDict = {}\n",
    "\n",
    "            srcip = packet[52:60]\n",
    "            dstip = packet[60:68]\n",
    "            sport = packet[68:72]\n",
    "            dport = packet[72:76]\n",
    "            plen = packet[32:36]\n",
    "            protocol = packet[46:48]\n",
    "            \n",
    "            uniqIPs = list(set(uniqIPs) | set([dstip, srcip]))\n",
    "\n",
    "            if srcip not in srcIpDict:\n",
    "                dstIpSubDict[dstip] = [[sport], [dport], [plen], [protocol]]\n",
    "                srcIpDict[srcip] = dstIpSubDict\n",
    "\n",
    "            if dstip not in srcIpDict[srcip]:    \n",
    "                srcIpDict[srcip][dstip] = [[sport], [dport], [plen], [protocol]]\n",
    "            else:\n",
    "                srcIpDict[srcip][dstip][0].append(sport)\n",
    "                srcIpDict[srcip][dstip][1].append(dport)\n",
    "                srcIpDict[srcip][dstip][2].append(plen)\n",
    "                srcIpDict[srcip][dstip][3].append(protocol)\n",
    "\n",
    "    return srcIpDict, uniqIPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dictUniquerizer(dictOdictsOlistOlists):\n",
    "    '''\n",
    "    input: dictionary of dictionaries that have a list of lists \n",
    "           ex. srcIpDict[srcip][dstip] = [[sport], [dport], [plen], [protocol]]\n",
    "    output: dictionary of dictionaries with list of lists with unique items in the final sublist\n",
    "    \n",
    "    WARNING: will overwrite your input dictionary. Make a copy if you want to preserve dictOdictsOlistOlists.\n",
    "    '''\n",
    "    #dictCopy\n",
    "    for key in dictOdictsOlistOlists.keys():\n",
    "        for subkey in dictOdictsOlistOlists[key].keys():\n",
    "            for sublist in xrange(len(dictOdictsOlistOlists[key][subkey])):\n",
    "                dictOdictsOlistOlists[key][subkey][sublist] = list(set(dictOdictsOlistOlists[key][subkey][sublist]))\n",
    "    \n",
    "    return dictOdictsOlistOlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test\n",
    "#comsDicttest, uniqIPstest = srcIpDict(hexSessionstest)\n",
    "comsDict, uniqIPs = srcIpDict(hexSessions)\n",
    "#test\n",
    "comsDict = dictUniquerizer(comsDict)\n",
    "#comsDicttest = dictUniquerizer(comsDicttest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directionality adversary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ipDirSwitcher(hexSessionList):\n",
    "    '''\n",
    "    input is a list of packets from ONE session\n",
    "    '''\n",
    "    \n",
    "    ipdirsession = []\n",
    "        \n",
    "    for p in hexSessionList:\n",
    "        sourceIP = p[52:60]\n",
    "        destIP = p[60:68]\n",
    "\n",
    "        ipdirsession.append(p[:52]+destIP+sourceIP+p[68:])\n",
    "\n",
    "    return ipdirsession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def portDirSwitcher(hexSessionList):\n",
    "    '''\n",
    "    input is a list of packets from ONE session\n",
    "    '''\n",
    "    \n",
    "    portdirsession = []\n",
    "    \n",
    "    for p in hexSessionList:\n",
    "        sport = p[68:72]\n",
    "        dport = p[72:76]\n",
    "\n",
    "        portdirsession.append(p[:68]+dport+sport+p[76:])\n",
    "\n",
    "    return portdirsession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave one swap one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dstIpSwapOut(hexSessionList, dictOcoms, listOuniqIPs):\n",
    "    #srcIpDict[srcip][dstip] = [[sport], [dport], [plen], [protocol]]\n",
    "    \n",
    "    swapSession = []\n",
    "    srcip = hexSessionList[0][52:60] #assumes first packet contains true initial direction\n",
    "    dstip = hexSessionList[0][60:68]\n",
    "    normDstIps = dictOcoms[srcip].keys()+[srcip] #get list of dstIPs that srcIP talks to\n",
    "    abbynormIps = copy(listOuniqIPs)\n",
    "    \n",
    "    for normIp in normDstIps:\n",
    "        abbynormIps.remove(normIp) #remove itself and know dstIPs from list of consideration.\n",
    "    \n",
    "    abbynormDestIp = random.sample(abbynormIps, 1)[0] #get random ip that srcip doesn't talk to\n",
    "\n",
    "    for rawpacket in hexSessionList:\n",
    "        packet = copy(rawpacket)\n",
    "        \n",
    "        if packet[60:68] == dstip:\n",
    "            packet = packet[:60] + abbynormDestIp + packet[68:] #\n",
    "        elif packet[61:69] == srcip:\n",
    "            packet = packet[:52] + abbynormDestIp + packet[60:] #in case direction switches for packet in session\n",
    "            \n",
    "        swapSession.append(packet)\n",
    "\n",
    "    return swapSession\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dstPortSwapOneOut(hexSessionList):\n",
    "    #THINK THROUGH\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def oneHot(index, granular = 'hex'):\n",
    "    if granular == 'hex':\n",
    "        vecLen = 257\n",
    "    else:\n",
    "        vecLen = 17\n",
    "    \n",
    "    zeroVec = np.zeros(vecLen)\n",
    "    zeroVec[index] = 1.0\n",
    "    \n",
    "    return zeroVec\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def oneSessionTokenized(sessionDict, hexDict, packetTimeSteps=50):\n",
    "\n",
    "    dstMac = []\n",
    "    srcMac = []\n",
    "    firstType = []\n",
    "    ihl = []\n",
    "    tos = []\n",
    "    firLen = []\n",
    "    firId = []\n",
    "    frag = []\n",
    "    ttl = []\n",
    "    protocol = []\n",
    "    firCheck = []\n",
    "    srcip = []\n",
    "    dstip = []\n",
    "    sport = []\n",
    "    dport = []\n",
    "    \n",
    "    i=0\n",
    "    for key in sessionDict.keys():\n",
    "        sessionPackets = sessionDict[key]\n",
    "        \n",
    "        #if len(sessionPackets) > maxPackets: #crop the number of sessions to maxPackets\n",
    "        #    sessionList = sessionPackets[:maxPackets]\n",
    "        #else:\n",
    "        #    sessionList = sessionPackets\n",
    "\n",
    "        for packet in sessionPackets:\n",
    "            \n",
    "            dstMac.append(packet[0:12]) \n",
    "            srcMac.append(packet[12:24]) \n",
    "            firstType.append(packet[24:28])\n",
    "            ihl.append(packet[28:30]) \n",
    "            tos.append(packet[30:32])\n",
    "            firLen.append(packet[32:36])\n",
    "            firId.append(packet[36:40]) \n",
    "            frag.append(packet[40:44]) \n",
    "            ttl.append(packet[44:46])\n",
    "            protocol.append(packet[46:48])\n",
    "            firCheck.append(packet[48:52]) \n",
    "            srcip.append(packet[52:60])\n",
    "            dstip.append(packet[60:68])\n",
    "            sport.append(packet[68:72])\n",
    "            dport.append(packet[72:76])\n",
    "            \n",
    "            i+=1\n",
    "    print i\n",
    "    return np.asarray(dstMac), np.asarray(srcMac),np.asarray(firstType),np.asarray(ihl),np.asarray(tos),\\\n",
    "           np.asarray(firLen),np.asarray(firId),np.asarray(frag),np.asarray(ttl),np.asarray(protocol),\\\n",
    "           np.asarray(firCheck),np.asarray(srcip),np.asarray(dstip), np.asarray(sport), np.asarray(dport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7538\n"
     ]
    }
   ],
   "source": [
    "netstats  = oneSessionTokenized(hexSessions, hexDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dstMac 8\n",
      "srcMac 7\n",
      "firstType 3\n",
      "ihl 13\n",
      "tos 22\n",
      "firLen 36\n",
      "firId 3506\n",
      "frag 25\n",
      "ttl 43\n",
      "protocol 10\n",
      "firCheck 3313\n",
      "srcip 99\n",
      "dstip 123\n"
     ]
    }
   ],
   "source": [
    "indx = ['dstMac', 'srcMac', 'firstType', 'ihl', 'tos', 'firLen', 'firId', 'frag', 'ttl', 'protocol', 'firCheck',\n",
    "       'srcip', 'dstip', 'sport', 'dport']\n",
    "for num in range(13):\n",
    "    print indx[num], len(np.unique(netstats[num]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "packet = hexSessions[hexSessions.keys()[1]][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coll = []\n",
    "for key in hexSessions.keys():\n",
    "    sessionPackets = hexSessions[key]\n",
    "    for packet in sessionPackets:\n",
    "        coll.append(packet[32:36]+packet[44:46]+packet[46:48]+packet[52:60]+packet[60:68]+packet[68:70]+packet[70:72]+\n",
    "                   packet[72:74])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.unique(coll).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxPackets = 2\n",
    "packetTimeSteps = 100\n",
    "\n",
    "def oneSessionEncoder(sessionPackets, hexDict, maxPackets = 2, packetTimeSteps = 100,\n",
    "                       packetReverse = False, charLevel = False, padOldTimeSteps = True):    \n",
    "            \n",
    "    sessionCollect = []\n",
    "    packetCollect = []\n",
    "    \n",
    "    if charLevel:\n",
    "        vecLen = 17\n",
    "    else:\n",
    "        vecLen = 257\n",
    "    \n",
    "    if len(sessionPackets) > maxPackets: #crop the number of sessions to maxPackets\n",
    "        sessionList = copy(sessionPackets[:maxPackets])\n",
    "    else:\n",
    "        sessionList = copy(sessionPackets)\n",
    "\n",
    "    for rawpacket in sessionList:\n",
    "        packet = copy(rawpacket)\n",
    "        #packet = packet[32:36]+packet[44:46]+packet[46:48]+packet[52:60]+packet[60:68]+\\\n",
    "        #         packet[68:70]+packet[70:72]+packet[72:74]\n",
    "        packet = [hexDict[packet[i:i+2]] for i in xrange(0,len(packet)-2+1,2)]\n",
    "        \n",
    "        #print np.asarray(packet)\n",
    "            \n",
    "        if len(packet) >= packetTimeSteps: #crop packet to length packetTimeSteps\n",
    "            packet = packet[:packetTimeSteps]\n",
    "            packet = packet+[256] #add <EOP> end of packet token\n",
    "        else:\n",
    "            packet = packet+[256] #add <EOP> end of packet token\n",
    "        \n",
    "        packetCollect.append(packet)\n",
    "        \n",
    "        pacMat = np.array([oneHot(x) for x in packet]) #one hot encoding of packet into a matrix\n",
    "        pacMatLen = len(pacMat)\n",
    "        \n",
    "        #padding packet\n",
    "        if packetReverse:\n",
    "            pacMat = pacMat[::-1]\n",
    "\n",
    "        if pacMatLen < packetTimeSteps:\n",
    "            #pad by stacking zeros on top of data so that earlier timesteps do not have information\n",
    "            #padding the packet such that zeros are after the actual info for better translation\n",
    "            if padOldTimeSteps:\n",
    "                pacMat = np.vstack( ( np.zeros((packetTimeSteps-pacMatLen,vecLen)), pacMat) ) \n",
    "            else:\n",
    "                pacMat = np.vstack( (pacMat, np.zeros((packetTimeSteps-pacMatLen,vecLen))) ) \n",
    "\n",
    "        if pacMatLen > packetTimeSteps:\n",
    "            pacMat = pacMat[:packetTimeSteps, :]\n",
    "\n",
    "        sessionCollect.append(pacMat)\n",
    "\n",
    "    #padding session\n",
    "    sessionCollect = np.asarray(sessionCollect, dtype=theano.config.floatX)\n",
    "    numPacketsInSession = sessionCollect.shape[0]\n",
    "    if numPacketsInSession < maxPackets:\n",
    "        #pad sessions to fit the \n",
    "        sessionCollect = np.vstack( (sessionCollect,np.zeros((maxPackets-numPacketsInSession, \n",
    "                                                             packetTimeSteps, vecLen))) )\n",
    "    \n",
    "    return sessionCollect, packetCollect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def dropout(X, p=0.):\n",
    "    if p != 0:\n",
    "        retain_prob = 1 - p\n",
    "        X = X / retain_prob * srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX)\n",
    "    return X\n",
    "\n",
    "# Gradient clipping\n",
    "def clip_norm(g, c, n): \n",
    "    '''n is the norm, c is the threashold, and g is the gradient'''\n",
    "    \n",
    "    if c > 0: \n",
    "        g = T.switch(T.ge(n, c), g*c/n, g) \n",
    "    return g\n",
    "\n",
    "def clip_norms(gs, c):\n",
    "    norm = T.sqrt(sum([T.sum(g**2) for g in gs]))\n",
    "    return [clip_norm(g, c, norm) for g in gs]\n",
    "\n",
    "# Regularizers\n",
    "def max_norm(p, maxnorm = 0.):\n",
    "    if maxnorm > 0:\n",
    "        norms = T.sqrt(T.sum(T.sqr(p), axis=0))\n",
    "        desired = T.clip(norms, 0, maxnorm)\n",
    "        p = p * (desired/ (1e-7 + norms))\n",
    "    return p\n",
    "\n",
    "def gradient_regularize(p, g, l1 = 0., l2 = 0.):\n",
    "    g += p * l2\n",
    "    g += T.sgn(p) * l1\n",
    "    return g\n",
    "\n",
    "def weight_regularize(p, maxnorm = 0.):\n",
    "    p = max_norm(p, maxnorm)\n",
    "    return p\n",
    "\n",
    "def Adam(params, cost, lr=0.0002, b1=0.1, b2=0.001, e=1e-8, l1 = 0., l2 = 0., maxnorm = 0., c = 8):\n",
    "    \n",
    "    updates = []\n",
    "    grads = T.grad(cost, params)\n",
    "    grads = clip_norms(grads, c)\n",
    "    \n",
    "    i = theano.shared(floatX(0.))\n",
    "    i_t = i + 1.\n",
    "    fix1 = 1. - b1**(i_t)\n",
    "    fix2 = 1. - b2**(i_t)\n",
    "    lr_t = lr * (T.sqrt(fix2) / fix1)\n",
    "    \n",
    "    for p, g in zip(params, grads):\n",
    "        m = theano.shared(p.get_value() * 0.)\n",
    "        v = theano.shared(p.get_value() * 0.)\n",
    "        m_t = (b1 * g) + ((1. - b1) * m)\n",
    "        v_t = (b2 * T.sqr(g)) + ((1. - b2) * v)\n",
    "        g_t = m_t / (T.sqrt(v_t) + e)\n",
    "        g_t = gradient_regularize(p, g_t, l1=l1, l2=l2)\n",
    "        p_t = p - (lr_t * g_t)\n",
    "        p_t = weight_regularize(p_t, maxnorm=maxnorm)\n",
    "        \n",
    "        updates.append((m, m_t))\n",
    "        updates.append((v, v_t))\n",
    "        updates.append((p, p_t))\n",
    "    \n",
    "    updates.append((i, i_t))\n",
    "    return updates\n",
    "\n",
    "def RMSprop(cost, params, lr = 0.001, l1 = 0., l2 = 0., maxnorm = 0., rho=0.9, epsilon=1e-6, c = 8):\n",
    "    \n",
    "    grads = T.grad(cost, params)\n",
    "    grads = clip_norms(grads, c)\n",
    "    updates = []\n",
    "    \n",
    "    for p, g in zip(params, grads):\n",
    "        g = gradient_regularize(p, g, l1 = l1, l2 = l2)\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        updates.append((acc, acc_new))\n",
    "        \n",
    "        updated_p = p - lr * (g / T.sqrt(acc_new + epsilon))\n",
    "        updated_p = weight_regularize(updated_p, maxnorm = maxnorm)\n",
    "        updates.append((p, updated_p))\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#makes output by shifting inputs down in time one step and then copying the last time step to the end.\n",
    "#def targetModifier(targetArray):\n",
    "#    newTarget = np.vstack((targetArray[1:, :], targetArray[-1,:]))\n",
    "#    return newTarget\n",
    "\n",
    "#def targetMaker(listOinputs):\n",
    "    #TODO: do this with arrays\n",
    "#    outputs = []\n",
    "#    for inp in listOinputs:\n",
    "#        outputs.append(targetModifier(inp))\n",
    "#    outputs = np.asarray(outputs)\n",
    "#    \n",
    "#    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised feature extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization for both the unsupervised net and the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = T.tensor4('inputs')\n",
    "Y = T.matrix('targets')\n",
    "\n",
    "wtstd = 0.3\n",
    "dimIn = 257 #hex has 256 characters + the <EOP> character\n",
    "dim = 100 #dimension reduction size\n",
    "rnnType = 'gru' #gru or lstm\n",
    "bidirectional = False\n",
    "linewt_init = IsotropicGaussian(wtstd)\n",
    "line_bias = Constant(1.0)\n",
    "rnnwt_init = IsotropicGaussian(wtstd)\n",
    "rnnbias_init = Constant(0.0)\n",
    "packetReverse = False\n",
    "\n",
    "###ENCODER\n",
    "if rnnType == 'gru':\n",
    "    rnn = GatedRecurrent(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'gru')\n",
    "    dimMultiplier = 2\n",
    "else:\n",
    "    rnn = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'lstm')\n",
    "    dimMultiplier = 4\n",
    "\n",
    "fork = Fork(output_names=['linear', 'gates'],\n",
    "            name='fork', input_dim=dimIn, output_dims=[dim, dim * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "\n",
    "###CONTEXT\n",
    "if rnnType == 'gru':\n",
    "    rnnContext = GatedRecurrent(dim=dim, weights_init = rnnwt_init, \n",
    "                                biases_init = rnnbias_init, name = 'gruContext')\n",
    "else:\n",
    "    rnnContext = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, \n",
    "                      name = 'lstmContext')\n",
    "\n",
    "forkContext = Fork(output_names=['linearContext', 'gatesContext'],\n",
    "            name='forkContext', input_dim=dim, output_dims=[dim, dim * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "if bidirectional:\n",
    "    dimDec = dim*2\n",
    "    \n",
    "    if rnnType == 'gru':\n",
    "        rnnContextRev = GatedRecurrent(dim=dim, weights_init = rnnwt_init, \n",
    "                                       biases_init = rnnbias_init, name = 'gruContextRev')\n",
    "        \n",
    "    else:\n",
    "        rnnContextRev = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init,\n",
    "                             name = 'lstmContextRev')\n",
    "    \n",
    "    rnnContextRev.initialize()\n",
    "\n",
    "else:\n",
    "    dimDec = dim\n",
    "\n",
    "\n",
    "###DECODER\n",
    "'''if rnnType == 'gru':\n",
    "    rnnDec = GatedRecurrent(dim=dimIn, weights_init = rnnwt_init, \n",
    "                            biases_init = rnnbias_init, name = 'gruDecoder')\n",
    "else:\n",
    "    rnnDec = LSTM(dim=dimIn, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'lstmDecoder')'''\n",
    "\n",
    "\n",
    "forkDec = Fork(output_names=['linear', 'gates'],\n",
    "            name='forkDec', input_dim=dimDec, output_dims=[dim, dim*dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "#forkFinal = Fork(output_names=['linear', 'gates'],\n",
    "#            name='forkFinal', input_dim=dim, output_dims=[dimIn, dimIn*dimMultiplier], \n",
    "#            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "#initialize the weights in all the functions\n",
    "fork.initialize()\n",
    "rnn.initialize()\n",
    "\n",
    "forkContext.initialize()\n",
    "rnnContext.initialize()\n",
    "forkDec.initialize()\n",
    "\n",
    "#forkFinal.initialize()\n",
    "#rnnDec.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This section can be skipped if you want to go directly to the classifier\n",
    "\n",
    "batch_size = 20\n",
    "maxPackets = 3\n",
    "\n",
    "def onestepEnc(X):\n",
    "\n",
    "    data1, data2 = fork.apply(X)\n",
    "    \n",
    "    if rnnType == 'gru':\n",
    "        hEnc = rnn.apply(data1, data2) \n",
    "    else:\n",
    "        hEnc, _ = rnn.apply(data2)\n",
    "    \n",
    "    return hEnc, data1\n",
    "\n",
    "[hEnc, data1], _ = theano.scan(onestepEnc, X)\n",
    "hEncReshape = T.reshape(hEnc[:,-1], (batch_size, maxPackets, 1, dim))#take last representation of packet\n",
    "\n",
    "def onestepContext(hEncReshape):\n",
    "    \n",
    "    data3, data4 = forkContext.apply(hEncReshape)\n",
    "    \n",
    "    if rnnType == 'gru':\n",
    "        hContext = rnnContext.apply(data3, data4)\n",
    "    else:\n",
    "        hinitContext, _ = rnnContext.apply(data4)\n",
    "        hContext = hinitContext \n",
    "    \n",
    "    return hContext\n",
    "\n",
    "hContext, _ = theano.scan(onestepContext, hEncReshape) #shape = (batch_size,maxPackets,1,dim)\n",
    "#hContextReshape = T.reshape(hContext[:,-1], (batch_sizeClass, dim))\n",
    "\n",
    "data5, _ = forkDec.apply(hContext) #this fork makes data5 same dim as data1 (the orig word embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testones = np.ones((60, 100, 1, 257))\n",
    "testones[0] = testones[0]+500\n",
    "testfun = theano.function([X], [data1, hContext], allow_input_downcast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-0.80759811,  0.95638108,  0.0306921 , ...,  0.02286468,\n",
       "           0.90007931,  0.12608609]],\n",
       "\n",
       "        [[-0.99241495,  0.340267  ,  0.02675632, ...,  0.51105827,\n",
       "           0.99152482,  0.85750473]],\n",
       "\n",
       "        [[-0.99920672,  0.58101737,  0.01561151, ...,  0.6756627 ,\n",
       "           0.99920058,  0.91132247]]],\n",
       "\n",
       "\n",
       "       [[[-0.97815973,  0.13022809, -0.00345091, ...,  0.31171903,\n",
       "           0.87581617,  0.89926767]],\n",
       "\n",
       "        [[-0.98689735,  0.08709375, -0.02774348, ...,  0.49410865,\n",
       "           0.96862364,  0.96230948]],\n",
       "\n",
       "        [[-0.99790287,  0.24172701, -0.05585832, ...,  0.84049654,\n",
       "           0.98190516,  0.97334903]]],\n",
       "\n",
       "\n",
       "       [[[-0.97815973,  0.13022809, -0.00345091, ...,  0.31171903,\n",
       "           0.87581617,  0.89926767]],\n",
       "\n",
       "        [[-0.98689735,  0.08709375, -0.02774348, ...,  0.49410865,\n",
       "           0.96862364,  0.96230948]],\n",
       "\n",
       "        [[-0.99790287,  0.24172701, -0.05585832, ...,  0.84049654,\n",
       "           0.98190516,  0.97334903]]],\n",
       "\n",
       "\n",
       "       ..., \n",
       "       [[[-0.97815973,  0.13022809, -0.00345091, ...,  0.31171903,\n",
       "           0.87581617,  0.89926767]],\n",
       "\n",
       "        [[-0.98689735,  0.08709375, -0.02774348, ...,  0.49410865,\n",
       "           0.96862364,  0.96230948]],\n",
       "\n",
       "        [[-0.99790287,  0.24172701, -0.05585832, ...,  0.84049654,\n",
       "           0.98190516,  0.97334903]]],\n",
       "\n",
       "\n",
       "       [[[-0.97815973,  0.13022809, -0.00345091, ...,  0.31171903,\n",
       "           0.87581617,  0.89926767]],\n",
       "\n",
       "        [[-0.98689735,  0.08709375, -0.02774348, ...,  0.49410865,\n",
       "           0.96862364,  0.96230948]],\n",
       "\n",
       "        [[-0.99790287,  0.24172701, -0.05585832, ...,  0.84049654,\n",
       "           0.98190516,  0.97334903]]],\n",
       "\n",
       "\n",
       "       [[[-0.97815973,  0.13022809, -0.00345091, ...,  0.31171903,\n",
       "           0.87581617,  0.89926767]],\n",
       "\n",
       "        [[-0.98689735,  0.08709375, -0.02774348, ...,  0.49410865,\n",
       "           0.96862364,  0.96230948]],\n",
       "\n",
       "        [[-0.99790287,  0.24172701, -0.05585832, ...,  0.84049654,\n",
       "           0.98190516,  0.97334903]]]], dtype=float32)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testfun(testones)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ -8.07598114e-01,   9.56381083e-01,   3.06920968e-02, ...,\n",
       "            2.28646826e-02,   9.00079310e-01,   1.26086086e-01],\n",
       "         [  3.43906665e+03,  -2.02095227e+03,  -1.77620093e+03, ...,\n",
       "            1.61368762e+03,   1.74679663e+03,  -1.51070190e+02],\n",
       "         [  3.43906665e+03,  -2.02095227e+03,  -1.77620093e+03, ...,\n",
       "            1.61368762e+03,   1.74679663e+03,  -1.51070190e+02],\n",
       "         ..., \n",
       "         [  3.43906665e+03,  -2.02095227e+03,  -1.77620093e+03, ...,\n",
       "            1.61368762e+03,   1.74679663e+03,  -1.51070190e+02],\n",
       "         [  3.43906665e+03,  -2.02095227e+03,  -1.77620093e+03, ...,\n",
       "            1.61368762e+03,   1.74679663e+03,  -1.51070190e+02],\n",
       "         [  3.43906665e+03,  -2.02095227e+03,  -1.77620093e+03, ...,\n",
       "            1.61368762e+03,   1.74679663e+03,  -1.51070190e+02]],\n",
       "\n",
       "        [[ -9.92414951e-01,   3.40267003e-01,   2.67563164e-02, ...,\n",
       "            5.11058271e-01,   9.91524816e-01,   8.57504725e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         ..., \n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01]],\n",
       "\n",
       "        [[ -9.99206722e-01,   5.81017375e-01,   1.56115089e-02, ...,\n",
       "            6.75662696e-01,   9.99200583e-01,   9.11322474e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         ..., \n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01]]],\n",
       "\n",
       "\n",
       "       [[[ -9.78159726e-01,   1.30228087e-01,  -3.45091242e-03, ...,\n",
       "            3.11719030e-01,   8.75816166e-01,   8.99267673e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         ..., \n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01]],\n",
       "\n",
       "        [[ -9.86897349e-01,   8.70937482e-02,  -2.77434774e-02, ...,\n",
       "            4.94108647e-01,   9.68623638e-01,   9.62309480e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         ..., \n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01]],\n",
       "\n",
       "        [[ -9.97902870e-01,   2.41727009e-01,  -5.58583178e-02, ...,\n",
       "            8.40496540e-01,   9.81905162e-01,   9.73349035e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         ..., \n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01]]],\n",
       "\n",
       "\n",
       "       [[[ -9.78159726e-01,   1.30228087e-01,  -3.45091242e-03, ...,\n",
       "            3.11719030e-01,   8.75816166e-01,   8.99267673e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         ..., \n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01]],\n",
       "\n",
       "        [[ -9.86897349e-01,   8.70937482e-02,  -2.77434774e-02, ...,\n",
       "            4.94108647e-01,   9.68623638e-01,   9.62309480e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         ..., \n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01]],\n",
       "\n",
       "        [[ -9.97902870e-01,   2.41727009e-01,  -5.58583178e-02, ...,\n",
       "            8.40496540e-01,   9.81905162e-01,   9.73349035e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         ..., \n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01]]],\n",
       "\n",
       "\n",
       "       ..., \n",
       "       [[[ -9.78159726e-01,   1.30228087e-01,  -3.45091242e-03, ...,\n",
       "            3.11719030e-01,   8.75816166e-01,   8.99267673e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         ..., \n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01]],\n",
       "\n",
       "        [[ -9.86897349e-01,   8.70937482e-02,  -2.77434774e-02, ...,\n",
       "            4.94108647e-01,   9.68623638e-01,   9.62309480e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         ..., \n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01]],\n",
       "\n",
       "        [[ -9.97902870e-01,   2.41727009e-01,  -5.58583178e-02, ...,\n",
       "            8.40496540e-01,   9.81905162e-01,   9.73349035e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         ..., \n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01]]],\n",
       "\n",
       "\n",
       "       [[[ -9.78159726e-01,   1.30228087e-01,  -3.45091242e-03, ...,\n",
       "            3.11719030e-01,   8.75816166e-01,   8.99267673e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         ..., \n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01]],\n",
       "\n",
       "        [[ -9.86897349e-01,   8.70937482e-02,  -2.77434774e-02, ...,\n",
       "            4.94108647e-01,   9.68623638e-01,   9.62309480e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         ..., \n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01]],\n",
       "\n",
       "        [[ -9.97902870e-01,   2.41727009e-01,  -5.58583178e-02, ...,\n",
       "            8.40496540e-01,   9.81905162e-01,   9.73349035e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         ..., \n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01]]],\n",
       "\n",
       "\n",
       "       [[[ -9.78159726e-01,   1.30228087e-01,  -3.45091242e-03, ...,\n",
       "            3.11719030e-01,   8.75816166e-01,   8.99267673e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         ..., \n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01]],\n",
       "\n",
       "        [[ -9.86897349e-01,   8.70937482e-02,  -2.77434774e-02, ...,\n",
       "            4.94108647e-01,   9.68623638e-01,   9.62309480e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         ..., \n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01]],\n",
       "\n",
       "        [[ -9.97902870e-01,   2.41727009e-01,  -5.58583178e-02, ...,\n",
       "            8.40496540e-01,   9.81905162e-01,   9.73349035e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         ..., \n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01],\n",
       "         [  7.86240435e+00,  -3.03583336e+00,  -2.54730773e+00, ...,\n",
       "            4.21893549e+00,   4.48462439e+00,   6.96465850e-01]]]], dtype=float32)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#20,3,1,100\n",
    "np.concatenate((testfun(testones)[1], testfun(testones)[0].reshape(20, 3, 100, 100)[:,:,1:]), axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 3, 99, 100)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(60, 100, 1, 100)\n",
    "testfun(testones)[0].reshape(20, 3, 100, 100)[:,:,1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#decoding data needs to be one timestep (next packet in session) ahead, thus data1 we ignore the first packet\n",
    "#and the last hidden state of the context RNN.\n",
    "#THINK about L2 pooling before cat\n",
    "#THINK should we concatenate with X instead of data5\n",
    "#if packetReverse:\n",
    "#    data1 = data1[:,::-1]\n",
    "#do we need data5??\n",
    "\n",
    "data7 = T.concatenate((T.reshape(data5[:,:-1],(batch_size*(maxPackets-1),1,dim)), \n",
    "                       T.reshape(T.reshape(data1,(batch_size, maxPackets, packetTimeSteps, dim))[:,1:,1:,:], \n",
    "                                 (batch_size*(maxPackets-1),packetTimeSteps - 1,dim))), axis=1) \n",
    "                      \n",
    "                      #data1 is the original embedding of X, data5 is transformed context output\n",
    "                      #get rid of first packet in data 5\n",
    "                      #get rid of last context vector\n",
    "                      \n",
    "data8, data9 = forkFinal.apply(data7) #forkFinal transforms back to original dimIn\n",
    "\n",
    "if rnnType == 'gru':\n",
    "    hDec = rnnDec.apply(data8, data9) \n",
    "else:\n",
    "    hinit, _ = rnnDec.apply(data9)\n",
    "    hDec = hinit #hDec shape = (batch_size*(maxPackets-1), packetTimeSteps, 257)\n",
    "\n",
    "softmax = NDimensionalSoftmax()\n",
    "softout = softmax.apply(hDec, extra_ndim = 1)\n",
    "predX = T.reshape(T.reshape(X,(batch_size, maxPackets, packetTimeSteps, dimIn))[:,1:,:,:], \n",
    "                  (batch_size*(maxPackets-1), packetTimeSteps, 257))\n",
    "\n",
    "precost = predX*T.log(softout) + (1-predX)*T.log(1-softout)\n",
    "precost2 = -T.sum(T.sum(precost, axis = 2), axis = 1)\n",
    "#precost2 = -T.mean(T.sum(T.sum(precost, axis = 2), axis = 1))\n",
    "\n",
    "#cost = T.mean(precost2)\n",
    "cost = T.mean(BinaryCrossEntropy().apply(predX, softout))\n",
    "cg = ComputationGraph([cost])\n",
    "learning_rate = 0.0001\n",
    "params = VariableFilter(roles = [PARAMETER])(cg.variables)\n",
    "updates = Adam(params, cost, learning_rate, c=5) #c is gradient clipping parameter\n",
    "#updates = RMSprop(cost, params, learning_rate, c=5)\n",
    "\n",
    "#gradients = T.grad(cost, params)\n",
    "#gradients = clip_norms(gradients, 1)\n",
    "#gradientFun = theano.function([X], gradients, allow_input_downcast=True)\n",
    "\n",
    "print \"compiling you beautiful person\"\n",
    "train = theano.function([X], [cost, hContext], updates = updates, allow_input_downcast=True)\n",
    "predict = theano.function([X], softout, allow_input_downcast=True)\n",
    "print \"finished compiling\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#This section can be skipped if you want to go directly to the classifier\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "data1, data2 = fork.apply(X) \n",
    "\n",
    "if rnnType == 'gru':\n",
    "    hEnc = rnn.apply(data1, data2)[:,-1] #the [:,-1] gets the last hidden state for each obs in minibatch\n",
    "                                         #i.e. the last state for each sentence\n",
    "else:\n",
    "    hinit, _ = rnn.apply(data2)\n",
    "    hEnc = hinit[:,-1] #hEnc shape = (batch_size*maxPackets, dim) \n",
    "\n",
    "hEnc = T.reshape(hEnc,(batch_size, maxPackets, dim))\n",
    "\n",
    "data3, data4 = forkContext.apply(hEnc)\n",
    "\n",
    "if rnnType == 'gru':\n",
    "    hContext = rnnContext.apply(data3, data4)\n",
    "else:\n",
    "    hinitContext, _ = rnnContext.apply(data4)\n",
    "    hContext = hinitContext #hContext shape = (batch_size, maxPackets, dim)\n",
    "\n",
    "#TODO:test bidirectional and make work\n",
    "\n",
    "'''if bidirectional:\n",
    "    data3 = data3[::-1]\n",
    "    data4 = data4[::-1]\n",
    "\n",
    "    if rnnType == 'gru':\n",
    "        hContextRev = rnnContextRev.apply(data3, data4)\n",
    "    else:\n",
    "        hinitContext, _ = rnnContextRev.apply(data4)\n",
    "        hContextRev = hinitContext\n",
    "\n",
    "    hContext = T.concatenate((hContext, hContextRev), axis=2)\n",
    "'''\n",
    "\n",
    "data5, _ = forkDec.apply(hContext) #this fork makes data5 same dim as data1 (the orig word embedding)\n",
    "\n",
    "#decoding data needs to be one timestep (next packet in session) ahead, thus data1 we ignore the first packet\n",
    "#and the last hidden state of the context RNN.\n",
    "#THINK about L2 pooling before cat\n",
    "#THINK should we concatenate with X instead of data5\n",
    "#if packetReverse:\n",
    "#    data1 = data1[:,::-1]\n",
    "\n",
    "data7 = T.concatenate((T.reshape(data5[:,:-1],(batch_size*(maxPackets-1),1,dim)), \n",
    "                       T.reshape(T.reshape(data1,(batch_size, maxPackets, packetTimeSteps, dim))[:,1:,1:,:], \n",
    "                                 (batch_size*(maxPackets-1),packetTimeSteps - 1,dim))), axis=1) \n",
    "                      \n",
    "                      #data1 is the original embedding of X, data5 is transformed context output\n",
    "                      #get rid of first packet in data 5\n",
    "                      #get rid of last context vector\n",
    "                      \n",
    "data8, data9 = forkFinal.apply(data7) #forkFinal transforms back to original dimIn\n",
    "\n",
    "if rnnType == 'gru':\n",
    "    hDec = rnnDec.apply(data8, data9) \n",
    "else:\n",
    "    hinit, _ = rnnDec.apply(data9)\n",
    "    hDec = hinit #hDec shape = (batch_size*(maxPackets-1), packetTimeSteps, 257)\n",
    "\n",
    "softmax = NDimensionalSoftmax()\n",
    "softout = softmax.apply(hDec, extra_ndim = 1)\n",
    "predX = T.reshape(T.reshape(X,(batch_size, maxPackets, packetTimeSteps, dimIn))[:,1:,:,:], \n",
    "                  (batch_size*(maxPackets-1), packetTimeSteps, 257))\n",
    "\n",
    "precost = predX*T.log(softout) + (1-predX)*T.log(1-softout)\n",
    "precost2 = -T.sum(T.sum(precost, axis = 2), axis = 1)\n",
    "#precost2 = -T.mean(T.sum(T.sum(precost, axis = 2), axis = 1))\n",
    "\n",
    "#cost = T.mean(precost2)\n",
    "cost = T.mean(BinaryCrossEntropy().apply(predX, softout))\n",
    "cg = ComputationGraph([cost])\n",
    "learning_rate = 0.0001\n",
    "params = VariableFilter(roles = [PARAMETER])(cg.variables)\n",
    "updates = Adam(params, cost, learning_rate, c=5) #c is gradient clipping parameter\n",
    "#updates = RMSprop(cost, params, learning_rate, c=5)\n",
    "\n",
    "#gradients = T.grad(cost, params)\n",
    "#gradients = clip_norms(gradients, 1)\n",
    "#gradientFun = theano.function([X], gradients, allow_input_downcast=True)\n",
    "\n",
    "print \"compiling you beautiful person\"\n",
    "train = theano.function([X], [cost, hContext], updates = updates, allow_input_downcast=True)\n",
    "predict = theano.function([X], softout, allow_input_downcast=True)\n",
    "print \"finished compiling\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#randomize data\n",
    "hexSessionsKeys = hexSessions.keys()\n",
    "random.shuffle(hexSessionsKeys)\n",
    "trainPercent = 0.9\n",
    "trainIndex = int(len(hexSessionsKeys)*trainPercent)\n",
    "\n",
    "runname = 'hred'\n",
    "epochCost = []\n",
    "gradNorms = []\n",
    "contextCollect = []\n",
    "\n",
    "epochs = 10\n",
    "iteration = 0\n",
    "\n",
    "for epoch in xrange(epochs):\n",
    "    costCollect = []\n",
    "    \n",
    "    for start, end in zip(range(0, trainIndex,batch_size), range(batch_size, trainIndex, batch_size)):\n",
    "        \n",
    "        trainingSessions = []\n",
    "        \n",
    "        for trainKey in range(start, end):\n",
    "            sessionForEncoding = hexSessions[hexSessions.keys()[trainKey]]\n",
    "            oneHotSes = oneSessionEncoder(sessionForEncoding, packetReverse=packetReverse, \n",
    "                                          padOldTimeSteps = packetReverse,\n",
    "                                          hexDict = hexDict,\n",
    "                                          maxPackets = maxPackets, packetTimeSteps = packetTimeSteps)\n",
    "            trainingSessions.append(oneHotSes[0])\n",
    "        \n",
    "        trainingSessions = [item for sublist in trainingSessions for item in sublist]\n",
    "        sessionsMinibatch = np.asarray(trainingSessions)\n",
    "        \n",
    "    \n",
    "        costfun = train(sessionsMinibatch)\n",
    "        costCollect.append(costfun[0])\n",
    "        \n",
    "            \n",
    "        iteration+=1\n",
    "        \n",
    "        '''if iteration%20:\n",
    "            print '   iteration: ', iteration\n",
    "            print '   intermed cost: ', np.mean(costCollect)'''\n",
    "    ####SAVE COST TO FILE  \n",
    "    if epoch%2 == 0:\n",
    "        print(' ')\n",
    "        print 'Epoch: ', epoch\n",
    "        epochCost.append(np.mean(costCollect))\n",
    "        contextCollect.append(costfun[1][:4])\n",
    "        print 'Epoch cost average: ', epochCost[-1]\n",
    "        #grads = gradientFun(inputs, outputs)\n",
    "        #for gra in grads:\n",
    "        #    print '  gradient norms: ', np.linalg.norm(gra)\n",
    "        \n",
    "    np.savetxt(runname+\"_COST.csv\", epochCost, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Can be used with or without unsupervised learning\n",
    "\n",
    "#X = T.matrix('classtest')\n",
    "#Y = T.matrix('targets')\n",
    "batch_sizeClass = 20\n",
    "numClasses = 2\n",
    "clippings = 1\n",
    "learning_rateClass = 0.0001\n",
    "classifierWts = IsotropicGaussian(wtstd)\n",
    "\n",
    "bmlp = BatchNormalizedMLP(activations=[Logistic(),Logistic()], \n",
    "           dims=[dim, dim, numClasses],\n",
    "           weights_init=classifierWts,\n",
    "           biases_init=Constant(0.0001) )\n",
    "\n",
    "\n",
    "bmlp.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling functions you talented soul\n",
      "finished compiling\n"
     ]
    }
   ],
   "source": [
    "def onestepEnc(X):\n",
    "    data1, data2 = fork.apply(X) \n",
    "\n",
    "    if rnnType == 'gru':\n",
    "        hEnc = rnn.apply(data1, data2) \n",
    "    else:\n",
    "        hEnc, _ = rnn.apply(data2)\n",
    "        #hinit, _ = rnn.apply(data2)\n",
    "        #hEnc = hinit[:,-1] #hEnc shape = (batch_size, maxPackets, dim) \n",
    "\n",
    "    #hEnc2 = hEnc[-1]#T.reshape(hEnc[:,-1],(batch_sizeClass, maxPackets, dim))#the [:,-1] gets the last hidden state for \n",
    "                                                                    #each obs in minibatch\n",
    "                                                                    #i.e. the last state for each sentence\n",
    "    return hEnc\n",
    "\n",
    "hEnc, _ = theano.scan(onestepEnc, X)\n",
    "hEncReshape = T.reshape(hEnc[:,-1], (batch_sizeClass,maxPackets,1,dim))\n",
    "\n",
    "def onestepContext(hEncReshape):\n",
    "    \n",
    "    data3, data4 = forkContext.apply(hEncReshape)\n",
    "    \n",
    "    if rnnType == 'gru':\n",
    "        hContext = rnnContext.apply(data3, data4)\n",
    "    else:\n",
    "        hinitContext, _ = rnnContext.apply(data4)\n",
    "        hContext = hinitContext #hContext shape = (batch_size, maxPackets, dim)\n",
    "    \n",
    "    return hContext\n",
    "\n",
    "hContext, _ = theano.scan(onestepContext, hEncReshape)\n",
    "hContextReshape = T.reshape(hContext[:,-1], (batch_sizeClass,dim))\n",
    "\n",
    "data5, _ = forkDec.apply(hContextReshape)\n",
    "\n",
    "pyx = bmlp.apply(data5)\n",
    "softmax = Softmax()\n",
    "softoutClass = softmax.apply(pyx)\n",
    "costClass = T.mean(CategoricalCrossEntropy().apply(Y, softoutClass))\n",
    "\n",
    "cgClass = ComputationGraph([costClass])\n",
    "paramsClass = VariableFilter(roles = [PARAMETER])(cgClass.variables)\n",
    "\n",
    "updatesClass = Adam(paramsClass, costClass, learning_rateClass, c=clippings) \n",
    "#updatesClass = RMSprop(costClass, paramsClass, learning_rateClass, c=clippings)\n",
    "\n",
    "#print 'grad compiling'\n",
    "#gradients = T.grad(costClass, paramsClass)\n",
    "#gradients = clip_norms(gradients, clippings)\n",
    "#gradientFun = theano.function([X,Y], gradients, allow_input_downcast=True)\n",
    "#print 'finish with grads'\n",
    "\n",
    "print 'compiling functions you talented soul'\n",
    "classifierTrain = theano.function([X,Y], [costClass, hEnc, hContext, pyx, softoutClass], \n",
    "                                  updates=updatesClass, allow_input_downcast=True)\n",
    "classifierPredict = theano.function([X], softoutClass, allow_input_downcast=True)\n",
    "print 'finished compiling'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Epoch:  0\n",
      "Epoch cost average:  0.698986\n",
      "TRAIN accuracy:  0.5\n",
      "TEST accuracy:  0.525\n",
      "\n",
      " \n",
      "Epoch:  2\n",
      "Epoch cost average:  0.69327\n",
      "TRAIN accuracy:  0.503846153846\n",
      " \n",
      "Epoch:  4\n",
      "Epoch cost average:  0.691868\n",
      "TRAIN accuracy:  0.519230769231\n",
      "TEST accuracy:  0.6125\n",
      "\n",
      " \n",
      "Epoch:  6\n",
      "Epoch cost average:  0.66688\n",
      "TRAIN accuracy:  0.594871794872\n",
      " \n",
      "Epoch:  8\n",
      "Epoch cost average:  0.659588\n",
      "TRAIN accuracy:  0.605128205128\n",
      " \n",
      "Epoch:  10\n",
      "Epoch cost average:  0.641178\n",
      "TRAIN accuracy:  0.648717948718\n",
      "TEST accuracy:  0.6125\n",
      "\n",
      " \n",
      "Epoch:  12\n",
      "Epoch cost average:  0.647893\n",
      "TRAIN accuracy:  0.648717948718\n",
      " \n",
      "Epoch:  14\n",
      "Epoch cost average:  0.60033\n",
      "TRAIN accuracy:  0.702564102564\n",
      "TEST accuracy:  0.6875\n",
      "\n",
      " \n",
      "Epoch:  16\n",
      "Epoch cost average:  0.584161\n",
      "TRAIN accuracy:  0.710256410256\n",
      " \n",
      "Epoch:  18\n",
      "Epoch cost average:  0.568242\n",
      "TRAIN accuracy:  0.737179487179\n",
      " \n",
      "Epoch:  20\n",
      "Epoch cost average:  0.537879\n",
      "TRAIN accuracy:  0.770512820513\n",
      "TEST accuracy:  0.825\n",
      "\n",
      " \n",
      "Epoch:  22\n",
      "Epoch cost average:  0.511585\n",
      "TRAIN accuracy:  0.796153846154\n",
      " \n",
      "Epoch:  24\n",
      "Epoch cost average:  0.515518\n",
      "TRAIN accuracy:  0.798717948718\n",
      "TEST accuracy:  0.775\n",
      "\n",
      " \n",
      "Epoch:  26\n",
      "Epoch cost average:  0.504887\n",
      "TRAIN accuracy:  0.805128205128\n",
      " \n",
      "Epoch:  28\n",
      "Epoch cost average:  0.490133\n",
      "TRAIN accuracy:  0.821794871795\n",
      " \n",
      "Epoch:  30\n",
      "Epoch cost average:  0.496421\n",
      "TRAIN accuracy:  0.808974358974\n",
      "TEST accuracy:  0.8125\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-80581f789692>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mtargetsMinibatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingTargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mcostfun\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifierTrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msessionsMinibatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargetsMinibatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[0mcostCollect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcostfun\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mtrainCollect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcostfun\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtargetsMinibatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/envs/python2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/envs/python2/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[0;32m    949\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[0;32m    950\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m--> 951\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    952\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/envs/python2/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(node, args, outs)\u001b[0m\n\u001b[0;32m    938\u001b[0m                         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m                         \u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m                         self, node)\n\u001b[0m\u001b[0;32m    941\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mtheano/scan_module/scan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/home/bradh/.theano/compiledir_Linux-4.2--generic-x86_64-with-debian-jessie-sid-x86_64-2.7.12-64/scan_perform/mod.cpp:4193)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m/opt/conda/envs/python2/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[0;32m    949\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[0;32m    950\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m--> 951\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    952\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/envs/python2/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(node, args, outs)\u001b[0m\n\u001b[0;32m    938\u001b[0m                         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m                         \u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m                         self, node)\n\u001b[0m\u001b[0;32m    941\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#randomize data, if not already done in unsupervised portion\n",
    "hexSessionsKeys = hexSessions.keys()\n",
    "#random.shuffle(hexSessionsKeys)\n",
    "trainPercent = 0.9\n",
    "trainIndex = int(len(hexSessionsKeys)*trainPercent)\n",
    "packetReverse = False\n",
    "padOldTimeSteps = True\n",
    "\n",
    "runname = 'hredClassify'\n",
    "epochCost = []\n",
    "gradNorms = []\n",
    "trainAcc = []\n",
    "\n",
    "epochs = 1000\n",
    "iteration = 0\n",
    "\n",
    "normalTarget = np.array([0,1], dtype=theano.config.floatX)\n",
    "abbyTarget = np.array([1,0], dtype=theano.config.floatX)\n",
    "\n",
    "for epoch in xrange(epochs):\n",
    "    costCollect = []\n",
    "    trainCollect = []\n",
    "\n",
    "    for start, end in zip(range(0, trainIndex,batch_sizeClass/2),\n",
    "                          range(batch_sizeClass/2, trainIndex, batch_sizeClass/2)):\n",
    "        \n",
    "        trainingTargets = []\n",
    "        trainingSessions = []\n",
    "        for trainKey in range(start, end):\n",
    "            sessionForEncoding = list(hexSessions[hexSessions.keys()[trainKey]])\n",
    "            \n",
    "            #encode a normal session\n",
    "            oneHotSes = oneSessionEncoder(sessionForEncoding,hexDict = hexDict, packetReverse=packetReverse, \n",
    "                                          padOldTimeSteps = padOldTimeSteps, maxPackets = maxPackets,\n",
    "                                          packetTimeSteps = packetTimeSteps)\n",
    "            trainingSessions.append(oneHotSes[0])\n",
    "            trainingTargets.append(normalTarget)\n",
    "            \n",
    "            #encode an abby normal session\n",
    "            adversaryList = [dstIpSwapOut(sessionForEncoding, comsDict, uniqIPs), \n",
    "                             portDirSwitcher(sessionForEncoding), \n",
    "                             ipDirSwitcher(sessionForEncoding)]\n",
    "            abbyHexSession = random.sample(adversaryList, 1)[0]\n",
    "            #abbyHexSession = ipDirSwitcher(sessionForEncoding)\n",
    "            #abbyHexSession = dstIpSwapOut(sessionForEncoding, comsDict, uniqIPs)\n",
    "            #abbyHexSession = portDirSwitcher(sessionForEncoding)\n",
    "            abbyOneHotSes = oneSessionEncoder(abbyHexSession,hexDict = hexDict,packetReverse=packetReverse, \n",
    "                                              padOldTimeSteps = padOldTimeSteps, maxPackets = maxPackets, \n",
    "                                              packetTimeSteps = packetTimeSteps)\n",
    "            trainingSessions.append(abbyOneHotSes[0])\n",
    "            trainingTargets.append(abbyTarget)\n",
    "            \n",
    "        #trainingSessions2 = [item for sublist in trainingSessions for item in sublist] #FIX\n",
    "        sessionsMinibatch = np.asarray(trainingSessions).reshape((batch_sizeClass*maxPackets, packetTimeSteps, 1, 257))\n",
    "        targetsMinibatch = np.asarray(trainingTargets)\n",
    "    \n",
    "        costfun = classifierTrain(sessionsMinibatch, targetsMinibatch)\n",
    "        costCollect.append(costfun[0])\n",
    "        trainCollect.append(np.mean(np.argmax(costfun[-1],axis=1) == targetsMinibatch[:,1]))\n",
    "        \n",
    "        iteration+=1\n",
    "        \n",
    "    epochCost.append(np.mean(costCollect))\n",
    "    trainAcc.append(np.mean(trainCollect))\n",
    "    \n",
    "    #training accuracy\n",
    "    if epoch%2 == 0:\n",
    "        print(' ')\n",
    "        print 'Epoch: ', epoch\n",
    "        print 'Epoch cost average: ', epochCost[-1]\n",
    "        print 'TRAIN accuracy: ', trainAcc[-1]\n",
    "        #print costfun[-1]\n",
    "        #grads = gradientFun(sessionsMinibatch, targetsMinibatch)\n",
    "        #for gra in grads:\n",
    "        #    print '  gradient norms: ', np.linalg.norm(gra)\n",
    "        \n",
    "    #testing accuracy\n",
    "    if epoch%5 == 0:\n",
    "        testCollect = []\n",
    "        \n",
    "        #TODO: get accuracy for each adversarial type when they are all part of the training \n",
    "        #for ad in adversaryList:\n",
    "        \n",
    "        for start, end in zip(range(trainIndex, len(hexSessionsKeys), batch_sizeClass/2),\n",
    "                              range(trainIndex + batch_sizeClass/2, len(hexSessionsKeys), batch_sizeClass/2)):\n",
    "            trainingTargets = []\n",
    "            trainingSessions = []\n",
    "            for trainKey in range(start, end):\n",
    "                sessionForEncoding = hexSessions[hexSessions.keys()[trainKey]]\n",
    "                #encode a normal session\n",
    "                oneHotSes = oneSessionEncoder(sessionForEncoding,hexDict = hexDict, packetReverse=packetReverse, \n",
    "                                              padOldTimeSteps = padOldTimeSteps, maxPackets = maxPackets,\n",
    "                                              packetTimeSteps = packetTimeSteps)\n",
    "                trainingSessions.append(oneHotSes[0])\n",
    "                trainingTargets.append(normalTarget)\n",
    "\n",
    "                #encode an abby normal session\n",
    "                adversaryList = [dstIpSwapOut(sessionForEncoding, comsDict, uniqIPs), \n",
    "                                 portDirSwitcher(sessionForEncoding), \n",
    "                                 ipDirSwitcher(sessionForEncoding)]\n",
    "                abbyHexSession = random.sample(adversaryList, 1)[0]\n",
    "                #abbyHexSession = ipDirSwitcher(sessionForEncoding)\n",
    "                #abbyHexSession = dstIpSwapOut(sessionForEncoding, comsDict, uniqIPs)\n",
    "                #abbyHexSession = portDirSwitcher(sessionForEncoding)\n",
    "                abbyOneHotSes = oneSessionEncoder(abbyHexSession,hexDict = hexDict,packetReverse=packetReverse, \n",
    "                                                  padOldTimeSteps = padOldTimeSteps, maxPackets = maxPackets, \n",
    "                                                  packetTimeSteps = packetTimeSteps)\n",
    "                trainingSessions.append(abbyOneHotSes[0])\n",
    "                trainingTargets.append(abbyTarget)\n",
    "\n",
    "            #trainingSessions = [item for sublist in trainingSessions for item in sublist] #FIX in oneSessionEncoder\n",
    "            sessionsMinibatch = np.asarray(trainingSessions, dtype=theano.config.floatX)\\\n",
    "                                           .reshape((batch_sizeClass*maxPackets, packetTimeSteps, 1, 257))\n",
    "            targetsMinibatch = np.asarray(trainingTargets, dtype=theano.config.floatX)\n",
    "\n",
    "            predcostfun = classifierPredict(sessionsMinibatch)\n",
    "            testCollect.append(np.mean(np.argmax(predcostfun, axis=1) == targetsMinibatch[:,1]))\n",
    "\n",
    "        #print 'PortSwitching accruacy: ', \n",
    "        #print 'IpSwitching accuracy: ', \n",
    "        #print 'IpSwap accuracy: ', \n",
    "        print 'TEST accuracy: ', np.mean(testCollect)        \n",
    "        print\n",
    "    \n",
    "    \n",
    "    np.savetxt(runname+\"_COST.csv\", epochCost, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adversaryList = [dstIpSwapOut(sessionForEncoding, comsDict, uniqIPs), portDirSwitcher(sessionForEncoding), \n",
    "                 ipDirSwitcher(sessionForEncoding)]\n",
    "\n",
    "abbyHexSession = random.sample(adversaryList, 1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:  0.9625\n"
     ]
    }
   ],
   "source": [
    "#testing for generalization\n",
    "\n",
    "testCollect = []\n",
    "for start, end in zip(range(trainIndex, len(hexSessionsKeys), batch_sizeClass/2),\n",
    "                      range(trainIndex + batch_sizeClass/2, len(hexSessionsKeys), batch_sizeClass/2)):\n",
    "    trainingTargets = []\n",
    "    trainingSessions = []\n",
    "    for trainKey in range(start, end):\n",
    "        sessionForEncoding = hexSessions[hexSessions.keys()[trainKey]]\n",
    "\n",
    "        #encode a normal session\n",
    "        oneHotSes = oneSessionEncoder(sessionForEncoding,hexDict = hexDict, packetReverse=packetReverse, \n",
    "                                      padOldTimeSteps = padOldTimeSteps, maxPackets = maxPackets,\n",
    "                                      packetTimeSteps = packetTimeSteps)\n",
    "        trainingSessions.append(oneHotSes[0])\n",
    "        trainingTargets.append(normalTarget)\n",
    "\n",
    "        #encode an abby normal session\n",
    "        abbyHexSession = dstIpSwapOut(sessionForEncoding, comsDict, uniqIPs) #95%\n",
    "        #abbyHexSession = ipDirSwitcher(sessionForEncoding) #96%\n",
    "        #abbyHexSession = portDirSwitcher(sessionForEncoding) #96%\n",
    "        abbyOneHotSes = oneSessionEncoder(abbyHexSession,hexDict = hexDict,packetReverse=packetReverse, \n",
    "                                          padOldTimeSteps = padOldTimeSteps, maxPackets = maxPackets, \n",
    "                                          packetTimeSteps = packetTimeSteps)\n",
    "        trainingSessions.append(abbyOneHotSes[0])\n",
    "        trainingTargets.append(abbyTarget)\n",
    "\n",
    "    #trainingSessions = [item for sublist in trainingSessions for item in sublist] #FIX in oneSessionEncoder\n",
    "    sessionsMinibatch = np.asarray(trainingSessions, dtype=theano.config.floatX)\\\n",
    "                                   .reshape((batch_sizeClass*maxPackets, packetTimeSteps, 1, 257))\n",
    "    targetsMinibatch = np.asarray(trainingTargets, dtype=theano.config.floatX)\n",
    "\n",
    "    predcostfun = classifierPredict(sessionsMinibatch)\n",
    "    testCollect.append(np.mean(np.argmax(predcostfun, axis=1) == targetsMinibatch[:,1]))\n",
    "\n",
    "print 'test accuracy: ', np.mean(testCollect)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#[costClass,hEnc, hContext,pyx]\n",
    "costfun[1][0][0] == costfun[1][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifierPredict(sessionsMinibatch[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifierPredict(sessionsMinibatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#Can be used with or without unsupervised learning\n",
    "\n",
    "#X = T.matrix('classtest')\n",
    "#Y = T.matrix('targets')\n",
    "batch_sizeClass = 20\n",
    "numClasses = 2\n",
    "clippings = 5\n",
    "learning_rateClass = 0.0001\n",
    "classifierWts = IsotropicGaussian(0.03, 0)\n",
    "\n",
    "bmlp = BatchNormalizedMLP(activations=[Logistic(),Logistic()], \n",
    "           dims=[dim, dim, numClasses],\n",
    "           weights_init=classifierWts,\n",
    "           biases_init=Constant(0.0001) )\n",
    "\n",
    "\n",
    "bmlp.initialize()\n",
    "\n",
    "data1, data2 = fork.apply(X) #data1 shape = (batch_size, maxPackets, dimIn)\n",
    "\n",
    "if rnnType == 'gru':\n",
    "    hEnc = rnn.apply(data1, data2) \n",
    "else:\n",
    "    hEnc, _ = rnn.apply(data2, iterate = False)\n",
    "    #hinit, _ = rnn.apply(data2)\n",
    "    #hEnc = hinit[:,-1] #hEnc shape = (batch_size, maxPackets, dim) \n",
    "\n",
    "hEnc2 = T.reshape(hEnc[-1],(batch_sizeClass, maxPackets, dim))#the [:,-1] gets the last hidden state for \n",
    "                                                                #each obs in minibatch\n",
    "                                                                #i.e. the last state for each sentence\n",
    "\n",
    "data3, data4 = forkContext.apply(hEnc2)\n",
    "\n",
    "if rnnType == 'gru':\n",
    "    hContext = rnnContext.apply(data3, data4)\n",
    "else:\n",
    "    hinitContext, _ = rnnContext.apply(data4)\n",
    "    hContext = hinitContext #hContext shape = (batch_size, maxPackets, dim)\n",
    "\n",
    "data5, _ = forkDec.apply(hContext) \n",
    "\n",
    "#FIX bidirectional\n",
    "'''if bidirectional:\n",
    "\n",
    "    data3classRev = data3class[::-1]\n",
    "    data4classRev = data4class[::-1]\n",
    "\n",
    "    if rnnType == 'gru':\n",
    "        hContextRev = rnnContextRev.apply(data3classRev, data4classRev)\n",
    "    else:\n",
    "        hinitContextRevClass, _ = rnnContextRev.apply(data4classRev)\n",
    "        hContextRevClass = hinitContextRevClass\n",
    "\n",
    "    hContextClass = T.concatenate((hContextClass, hContextRevClass), axis=2)'''\n",
    "        \n",
    "\n",
    "pyx = bmlp.apply(hContext[:,-1])\n",
    "#softmax = Softmax()\n",
    "#softoutClass = softmax.apply(pyx)\n",
    "costClass = T.mean(CategoricalCrossEntropy().apply(Y, pyx))\n",
    "\n",
    "cgClass = ComputationGraph([costClass])\n",
    "paramsClass = VariableFilter(roles = [PARAMETER])(cgClass.variables)\n",
    "\n",
    "updatesClass = Adam(paramsClass, costClass, learning_rateClass, c=clippings) \n",
    "#updatesClass = RMSprop(costClass, paramsClass, learning_rateClass, c=clippings)\n",
    "\n",
    "#print 'grad compiling'\n",
    "#gradients = T.grad(costClass, paramsClass)\n",
    "#gradients = clip_norms(gradients, clippings)\n",
    "#gradientFun = theano.function([X,Y], gradients, allow_input_downcast=True)\n",
    "#print 'finish with grads'\n",
    "\n",
    "print 'compiling functions you talented soul'\n",
    "#classifierTrain = theano.function([X,Y], [costClass,hEnc, hEnc2, hContext,pyx], updates=updatesClass, allow_input_downcast=True)\n",
    "#classifierPredict = theano.function([X], pyx, allow_input_downcast=True)\n",
    "print 'finished compiling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
