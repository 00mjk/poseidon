{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to execute tcpdump. Check it is installed and in the PATH\n",
      "WARNING: No route found for IPv6 destination :: (no default route?)\n",
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled, cuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import binascii\n",
    "import multiprocessing as mp\n",
    "from itertools import chain\n",
    "from scapy.all import *\n",
    "sys.path.append('hed-dlg/')\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.stats import itemfreq\n",
    "\n",
    "import blocks\n",
    "from blocks.bricks import Linear, Softmax, Softplus, NDimensionalSoftmax\n",
    "from blocks.bricks.recurrent import GatedRecurrent, Fork, LSTM\n",
    "from blocks.initialization import Constant, IsotropicGaussian, Identity, Uniform\n",
    "from blocks.bricks.cost import BinaryCrossEntropy, CategoricalCrossEntropy\n",
    "from blocks.filter import VariableFilter\n",
    "from blocks.roles import PARAMETER\n",
    "from blocks.graph import ComputationGraph\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "\n",
    "###These warnings do not impede progress\n",
    "#WARNING: Failed to execute tcpdump. Check it is installed and in the PATH\n",
    "#WARNING: No route found for IPv6 destination :: (no default route?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataPath = '/data/bradsfirstpcaps.pcap'\n",
    "pcaps = rdpcap(dataPath)\n",
    "sessions = pcaps.sessions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#turn the sessions into a dictionary key = session_number, val = list of packages in hex\n",
    "\n",
    "i=0\n",
    "hexSessions = {}\n",
    "\n",
    "for k,v in sessions.items(): # v is the session\n",
    "    #for attr, value in v.__dict__.iteritems(): THIS IS TO GET DICT OF VALUES\n",
    "    #    print attr, value\n",
    "    #if i == 2:\n",
    "    #    break\n",
    "    scpcaps = []    \n",
    "    for p in v: #p is the individual packet in the session\n",
    "        \n",
    "        try:\n",
    "            rawindex = len(p[Raw])\n",
    "            payloadLens.append(rawindex)\n",
    "            scpcaps.append(binascii.hexlify(str(p.original)[:-rawindex])) #turn it into hex\n",
    "        except:\n",
    "            scpcaps.append(binascii.hexlify(p.original))\n",
    "        #for attr, value in p.payload.__dict__.iteritems():#this give the fields that are accessable\n",
    "        #    print attr, value\n",
    "        \n",
    "        #print len(binascii.hexlify(p.original))\n",
    "    hexSessions['session_' + str(i)] = scpcaps\n",
    "    \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Making the hex dictionary\n",
    "hexstring = '0,\t1,\t2,\t3,\t4,\t5,\t6,\t7,\t8,\t9,\tA,\tB,\tC,\tD,\tE,\tF,\t10,\t11,\t12,\t13,\t14,\t15,\t16,\t17,\t18,\t19\\\n",
    ",\t1A,\t1B,\t1C,\t1D,\t1E,\t1F,\t20,\t21,\t22,\t23,\t24,\t25,\t26,\t27,\t28,\t29,\t2A,\t2B,\t2C,\t2D,\t2E,\t2F,\t30,\t31,\t32,\t33,\t34,\t35\\\n",
    ",\t36,\t37,\t38,\t39,\t3A,\t3B,\t3C,\t3D,\t3E,\t3F,\t40,\t41,\t42,\t43,\t44,\t45,\t46,\t47,\t48,\t49,\t4A,\t4B,\t4C,\t4D,\t4E,\t4F,\t50,\t51\\\n",
    ",\t52,\t53,\t54,\t55,\t56,\t57,\t58,\t59,\t5A,\t5B,\t5C,\t5D,\t5E,\t5F,\t60,\t61,\t62,\t63,\t64,\t65,\t66,\t67,\t68,\t69,\t6A,\t6B,\t6C,\t6D\\\n",
    ",\t6E,\t6F,\t70,\t71,\t72,\t73,\t74,\t75,\t76,\t77,\t78,\t79,\t7A,\t7B,\t7C,\t7D,\t7E,\t7F,\t80,\t81,\t82,\t83,\t84,\t85,\t86,\t87,\t88,\t89\\\n",
    ",\t8A,\t8B,\t8C,\t8D,\t8E,\t8F,\t90,\t91,\t92,\t93,\t94,\t95,\t96,\t97,\t98,\t99,\t9A,\t9B,\t9C,\t9D,\t9E,\t9F,\tA0,\tA1,\tA2,\tA3,\tA4,\tA5\\\n",
    ",\tA6,\tA7,\tA8,\tA9,\tAA,\tAB,\tAC,\tAD,\tAE,\tAF,\tB0,\tB1,\tB2,\tB3,\tB4,\tB5,\tB6,\tB7,\tB8,\tB9,\tBA,\tBB,\tBC,\tBD,\tBE,\tBF,\tC0,\tC1\\\n",
    ",\tC2,\tC3,\tC4,\tC5,\tC6,\tC7,\tC8,\tC9,\tCA,\tCB,\tCC,\tCD,\tCE,\tCF,\tD0,\tD1,\tD2,\tD3,\tD4,\tD5,\tD6,\tD7,\tD8,\tD9,\tDA,\tDB,\tDC,\tDD\\\n",
    ",\tDE,\tDF,\tE0,\tE1,\tE2,\tE3,\tE4,\tE5,\tE6,\tE7,\tE8,\tE9,\tEA,\tEB,\tEC,\tED,\tEE,\tEF,\tF0,\tF1,\tF2,\tF3,\tF4,\tF5,\tF6,\tF7,\tF8,\tF9\\\n",
    ",\tFA,\tFB,\tFC,\tFD,\tFE,\tFF'.replace('\\t', '')\n",
    "\n",
    "hexList = hexstring.lower().split(',')\n",
    "hexList.append('<EOP>') #End Of Packet token\n",
    "hexDict = {}\n",
    "    \n",
    "for key, val in enumerate(hexList):\n",
    "    if len(val) == 1:\n",
    "        val = '0'+val\n",
    "    hexDict[val] = key  #dictionary k=hex, v=int  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#def hexOneHot(number):\n",
    "#    zeroVec = np.zeros(257)\n",
    "#    zeroVec[number] = 1.0\n",
    "#    return zeroVec\n",
    "\n",
    "def oneHot(index, granular = 'hex'):\n",
    "    if granular == 'hex':\n",
    "        vecLen = 257\n",
    "    else:\n",
    "        vecLen = 17\n",
    "    \n",
    "    zeroVec = np.zeros(vecLen)\n",
    "    zeroVec[index] = 1.0\n",
    "    \n",
    "    return zeroVec\n",
    "\n",
    "def oneHotSessions(sessionDict, maxPackets = 10, packetTimeSteps = 256,\n",
    "                   reverse = False, charLevel = False):\n",
    "    \"\"\"\n",
    "    This takes a list of int tokens and onehot encodes them, pads sessions with zero tensors according to maxPackets\n",
    "    and packet according to packetTimeSteps\n",
    "    \n",
    "    sessionDict = dict of lists of key = sessions and value = list of packets\n",
    "    timeSteps = maximum len of packet. it will be padded with zero vectors is packet is too short.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    listOsessions = []\n",
    "\n",
    "    if charLevel:\n",
    "        vecLen = 17\n",
    "    else:\n",
    "        vecLen = 257\n",
    "    \n",
    "    sessionKeys = sessionDict.keys()\n",
    "    \n",
    "    for session in sessionKeys:\n",
    "        #sessionTensor = np.zeros((maxPackets, packetTimeSteps, vecLen))\n",
    "        #lenSession = len(session)\n",
    "        sessionCollect = []\n",
    "        \n",
    "        if len(sessionDict[session]) > maxPackets: #crop the number of sessions to maxPackets\n",
    "            sessionList = sessionDict[session][:maxPackets]\n",
    "        else:\n",
    "            sessionList = sessionDict[session]\n",
    "        \n",
    "        for packet in sessionList:\n",
    "            packet = [hexDict[packet[i:i+2]] for i in xrange(0,len(packet)-2+1,2)]\n",
    "            \n",
    "            if len(packet) >= packetTimeSteps: #crop packet to length packetTimeSteps\n",
    "                packet = packet[:packetTimeSteps-1]\n",
    "            \n",
    "            packet = packet+[256] #add <EOP> end of packet token\n",
    "        \n",
    "            pacMat = np.array([oneHot(x) for x in packet]) #one hot encoding of packet into a matrix\n",
    "            pacMatLen = len(pacMat)\n",
    "        \n",
    "            #padding packet\n",
    "            if reverse:\n",
    "                pacMat = pacMat[::-1]\n",
    "            \n",
    "            if pacMatLen < packetTimeSteps:\n",
    "                #pad by stacking zeros on top of data so that earlier timesteps do not have information\n",
    "                pacMat = np.vstack((np.zeros((packetTimeSteps-pacMatLen,vecLen)), pacMat)) \n",
    "\n",
    "            if pacMatLen > packetTimeSteps:\n",
    "                pacMat = pacMat[:packetTimeSteps, :]\n",
    "                \n",
    "            sessionCollect.append(pacMat)\n",
    "        \n",
    "        #padding session\n",
    "        sessionCollect = np.asarray(sessionCollect)\n",
    "        numPacketsInSession = np.asarray(sessionCollect).shape[0]\n",
    "        if numPacketsInSession < maxPackets:\n",
    "            #pad sessions to fit the \n",
    "            sessionCollect = np.vstack((np.zeros((maxPackets-numPacketsInSession, packetTimeSteps, vecLen)), \n",
    "                                        sessionCollect))\n",
    "            \n",
    "        listOsessions.append(sessionCollect)\n",
    "        \n",
    "    return listOsessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sessions = oneHotSessions(hexSessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def dropout(X, p=0.):\n",
    "    if p != 0:\n",
    "        retain_prob = 1 - p\n",
    "        X = X / retain_prob * srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX)\n",
    "    return X\n",
    "\n",
    "# Gradient clipping\n",
    "def clip_norm(g, c, n): \n",
    "    '''n is the norm, c is the threashold, and g is the gradient'''\n",
    "    \n",
    "    if c > 0: \n",
    "        g = T.switch(T.ge(n, c), g*c/n, g) \n",
    "    return g\n",
    "\n",
    "def clip_norms(gs, c):\n",
    "    norm = T.sqrt(sum([T.sum(g**2) for g in gs]))\n",
    "    return [clip_norm(g, c, norm) for g in gs]\n",
    "\n",
    "# Regularizers\n",
    "def max_norm(p, maxnorm = 0.):\n",
    "    if maxnorm > 0:\n",
    "        norms = T.sqrt(T.sum(T.sqr(p), axis=0))\n",
    "        desired = T.clip(norms, 0, maxnorm)\n",
    "        p = p * (desired/ (1e-7 + norms))\n",
    "    return p\n",
    "\n",
    "def gradient_regularize(p, g, l1 = 0., l2 = 0.):\n",
    "    g += p * l2\n",
    "    g += T.sgn(p) * l1\n",
    "    return g\n",
    "\n",
    "def weight_regularize(p, maxnorm = 0.):\n",
    "    p = max_norm(p, maxnorm)\n",
    "    return p\n",
    "\n",
    "def Adam(params, cost, lr=0.0002, b1=0.1, b2=0.001, e=1e-8, l1 = 0., l2 = 0., maxnorm = 0., c = 8):\n",
    "    \n",
    "    updates = []\n",
    "    grads = T.grad(cost, params)\n",
    "    grads = clip_norms(grads, c)\n",
    "    \n",
    "    i = theano.shared(floatX(0.))\n",
    "    i_t = i + 1.\n",
    "    fix1 = 1. - b1**(i_t)\n",
    "    fix2 = 1. - b2**(i_t)\n",
    "    lr_t = lr * (T.sqrt(fix2) / fix1)\n",
    "    \n",
    "    for p, g in zip(params, grads):\n",
    "        m = theano.shared(p.get_value() * 0.)\n",
    "        v = theano.shared(p.get_value() * 0.)\n",
    "        m_t = (b1 * g) + ((1. - b1) * m)\n",
    "        v_t = (b2 * T.sqr(g)) + ((1. - b2) * v)\n",
    "        g_t = m_t / (T.sqrt(v_t) + e)\n",
    "        g_t = gradient_regularize(p, g_t, l1=l1, l2=l2)\n",
    "        p_t = p - (lr_t * g_t)\n",
    "        p_t = weight_regularize(p_t, maxnorm=maxnorm)\n",
    "        \n",
    "        updates.append((m, m_t))\n",
    "        updates.append((v, v_t))\n",
    "        updates.append((p, p_t))\n",
    "    \n",
    "    updates.append((i, i_t))\n",
    "    return updates\n",
    "\n",
    "def RMSprop(cost, params, lr = 0.001, l1 = 0., l2 = 0., maxnorm = 0., rho=0.9, epsilon=1e-6, c = 8):\n",
    "    \n",
    "    grads = T.grad(cost, params)\n",
    "    grads = clip_norms(grads, c)\n",
    "    updates = []\n",
    "    \n",
    "    for p, g in zip(params, grads):\n",
    "        g = gradient_regularize(p, g, l1 = l1, l2 = l2)\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        updates.append((acc, acc_new))\n",
    "        \n",
    "        updated_p = p - lr * (g / T.sqrt(acc_new + epsilon))\n",
    "        updated_p = weight_regularize(updated_p, maxnorm = maxnorm)\n",
    "        updates.append((p, updated_p))\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#makes output by shifting inputs down in time one step and then copying the last time step to the end.\n",
    "def targetModifier(targetArray):\n",
    "    newTarget = np.vstack((targetArray[1:, :], targetArray[-1,:]))\n",
    "    return newTarget\n",
    "\n",
    "def targetMaker(listOinputs):\n",
    "    #TODO: do this with arrays\n",
    "    outputs = []\n",
    "    for inp in listOinputs:\n",
    "        outputs.append(targetModifier(inp))\n",
    "    outputs = np.asarray(outputs)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The input dimension is 3d tensor\n",
    "\n",
    "\n",
    "dim = 257 #original data dimension/timesteps/columns\n",
    "rnnType = 'gru' #gru or lstm\n",
    "bidirectional = True\n",
    "X = T.tensor3('inputs')\n",
    "Xrev = T.matrix('reversed_inputs')\n",
    "linewt_init = Uniform(width=0.08)\n",
    "rnnwt_init = IsotropicGaussian(0.05)\n",
    "rnnbias_init = Constant(0.0)\n",
    "\n",
    "if rnnType == 'gru':\n",
    "    rnn = GatedRecurrent(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'gru')\n",
    "    dimMultiplier = 2\n",
    "else:\n",
    "    rnn = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'lstm')\n",
    "    dimMultiplier = 4\n",
    "\n",
    "###ICLR suggestion -> don't use bias in RNNs\n",
    "###RECURRENT LAYER\n",
    "\n",
    "#To use or not to use that is the question\n",
    "fork = Fork(output_names=['linear', 'gates'],\n",
    "            name='fork', input_dim=dim, output_dims=[dim, dim * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = rnnbias_init)\n",
    "data1, data2 = fork.apply(X)\n",
    "\n",
    "###for raw inputs\n",
    "#data1 = X\n",
    "#data2 = T.concatenate([X]*dimMultiplier, axis=2)\n",
    "\n",
    "if rnnType == 'gru':\n",
    "    hEnc = rnn.apply(data1, data2)#[:,-1] #the [:,-1] gets the last hidden state for each obs in minibatch\n",
    "                                         #i.e. the last state for each sentence\n",
    "else:\n",
    "    hinit, _ = rnn.apply(data2)\n",
    "    hEnc = hinit#[:,-1]\n",
    "\n",
    "#get weights initialized. without weights are nans.\n",
    "fork.initialize()\n",
    "rnn.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Encoder will return a maxPackets x packet length matrix\n",
    "encoder = theano.function([X], hEnc, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ..., \n",
       "       [-0.02446533,  0.00613252, -0.00284555, ..., -0.01066313,\n",
       "        -0.00112302, -0.01106142],\n",
       "       [-0.0258678 ,  0.00580584, -0.00322334, ..., -0.01103613,\n",
       "        -0.00067266, -0.01151582],\n",
       "       [-0.02678426,  0.00559139, -0.00346228, ..., -0.01120587,\n",
       "        -0.00042777, -0.01177805]], dtype=float32)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test\n",
    "dataTest(sessions[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if rnnType == 'gru':\n",
    "    rnnContext = GatedRecurrent(dim=dim, weights_init = rnnwt_init, \n",
    "                                biases_init = rnnbias_init, name = 'gruContext')\n",
    "    dimMultiplier = 2\n",
    "else:\n",
    "    rnnContext = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, \n",
    "                      name = 'lstmContext')\n",
    "    dimMultiplier = 4\n",
    "\n",
    "\n",
    "###ICLR suggestion -> don't use bias in RNNs\n",
    "#is encoding at each layer really the best way? or just feeding the raw through?\n",
    "###RECURRENT LAYER\n",
    "forkContext = Fork(output_names=['linearContext', 'gatesContext'],\n",
    "            name='forkContext', input_dim=dim, output_dims=[dim, dim * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = rnnbias_init)\n",
    "data3, data4 = forkContext.apply(hEnc)\n",
    "\n",
    "if rnnType == 'gru':\n",
    "    hContext = rnnContext.apply(data3, data4)\n",
    "else:\n",
    "    hinitContext, _ = rnnContext.apply(data4)\n",
    "    hContext = hinitContext\n",
    "\n",
    "if bidirectional:\n",
    "    \n",
    "    data3 = data3[::-1]\n",
    "    data4 = data4[::-1]\n",
    "    \n",
    "    if rnnType == 'gru':\n",
    "        rnnContextRev = GatedRecurrent(dim=dim, weights_init = rnnwt_init, \n",
    "                                       biases_init = rnnbias_init, name = 'gruContextRev')\n",
    "        hContextRev = rnnContextRev.apply(data3, data4)\n",
    "    else:\n",
    "        rnnContextRev = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init,\n",
    "                             name = 'lstmContextRev')\n",
    "        hinitContext, _ = rnnContextRev.apply(data4)\n",
    "        hContextRev = hinitContext\n",
    "    \n",
    "    \n",
    "    hContext = T.concatenate((hContext, hContextRev), axis = 1)\n",
    "    rnnContextRev.initialize()\n",
    "    \n",
    "#get weights initialized. without weights are nans.\n",
    "forkContext.initialize()\n",
    "rnnContext.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context = theano.function([X], hContext, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "contextTest = context(sessions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "        ..., \n",
       "        [ -2.55711284e-03,   6.22747117e-04,  -5.87851449e-04, ...,\n",
       "           3.36247234e-04,   2.19333684e-03,  -2.88968149e-04],\n",
       "        [ -2.55711284e-03,   6.22747117e-04,  -5.87851449e-04, ...,\n",
       "           3.36247234e-04,   2.19333684e-03,  -2.88968149e-04],\n",
       "        [ -2.55711284e-03,   6.22747117e-04,  -5.87851449e-04, ...,\n",
       "           3.36247234e-04,   2.19333684e-03,  -2.88968149e-04]],\n",
       "\n",
       "       [[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "        ..., \n",
       "        [ -8.54247541e-04,   6.56120654e-04,  -8.96860787e-04, ...,\n",
       "          -4.99854330e-04,   5.34644176e-04,  -3.63825791e-04],\n",
       "        [ -8.54247541e-04,   6.56120654e-04,  -8.96860787e-04, ...,\n",
       "          -4.99854330e-04,   5.34644176e-04,  -3.63825791e-04],\n",
       "        [ -8.54247541e-04,   6.56120654e-04,  -8.96860787e-04, ...,\n",
       "          -4.99854330e-04,   5.34644176e-04,  -3.63825791e-04]],\n",
       "\n",
       "       [[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "        ..., \n",
       "        [ -3.39909166e-04,   4.78121074e-04,  -8.07477045e-04, ...,\n",
       "          -5.37080399e-04,   6.56109623e-05,  -2.57707317e-04],\n",
       "        [ -3.39909166e-04,   4.78121074e-04,  -8.07477045e-04, ...,\n",
       "          -5.37080399e-04,   6.56109623e-05,  -2.57707317e-04],\n",
       "        [ -3.39909166e-04,   4.78121074e-04,  -8.07477045e-04, ...,\n",
       "          -5.37080399e-04,   6.56109623e-05,  -2.57707317e-04]],\n",
       "\n",
       "       ..., \n",
       "       [[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "        ..., \n",
       "        [ -8.92410317e-05,   3.20529434e-05,  -9.20757666e-05, ...,\n",
       "          -4.67497557e-05,   4.80251256e-05,   1.25184015e-05],\n",
       "        [ -8.92410317e-05,   3.20529434e-05,  -9.20757666e-05, ...,\n",
       "          -4.67497557e-05,   4.80251256e-05,   1.25184015e-05],\n",
       "        [ -8.92410317e-05,   3.20529434e-05,  -9.20757666e-05, ...,\n",
       "          -4.67497557e-05,   4.80251256e-05,   1.25184015e-05]],\n",
       "\n",
       "       [[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "        ..., \n",
       "        [ -6.68090215e-05,   2.27852997e-05,  -5.24713905e-05, ...,\n",
       "          -2.28297358e-05,   4.44133548e-05,   1.27497897e-05],\n",
       "        [ -6.68090215e-05,   2.27852997e-05,  -5.24713905e-05, ...,\n",
       "          -2.28297358e-05,   4.44133548e-05,   1.27497897e-05],\n",
       "        [ -6.68090215e-05,   2.27852997e-05,  -5.24713905e-05, ...,\n",
       "          -2.28297358e-05,   4.44133548e-05,   1.27497897e-05]],\n",
       "\n",
       "       [[ -2.55711284e-03,   6.22747117e-04,  -5.87851449e-04, ...,\n",
       "           3.36247234e-04,   2.19333684e-03,  -2.88968149e-04],\n",
       "        [ -2.55711284e-03,   6.22747117e-04,  -5.87851449e-04, ...,\n",
       "           3.36247234e-04,   2.19333684e-03,  -2.88968149e-04],\n",
       "        [ -2.55711284e-03,   6.22747117e-04,  -5.87851449e-04, ...,\n",
       "           3.36247234e-04,   2.19333684e-03,  -2.88968149e-04],\n",
       "        ..., \n",
       "        [ -4.79946466e-05,   1.77380771e-05,  -2.90737808e-05, ...,\n",
       "          -9.46215459e-06,   3.71720998e-05,   1.01576306e-05],\n",
       "        [ -4.79946466e-05,   1.77380771e-05,  -2.90737808e-05, ...,\n",
       "          -9.46215459e-06,   3.71720998e-05,   1.01576306e-05],\n",
       "        [ -4.79946466e-05,   1.77380771e-05,  -2.90737808e-05, ...,\n",
       "          -9.46215459e-06,   3.71720998e-05,   1.01576306e-05]]], dtype=float32)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input is hContext\n",
    "#we need original input for softmax\n",
    "#figure out how cost is calculated\n",
    "#pray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numTokens = 257\n",
    "rnnType = 'gru'\n",
    "X = T.tensor3('inputs')\n",
    "Y = T.tensor3('outputs')\n",
    "linewt_init = Uniform(width=0.02)\n",
    "rnnwt_init = IsotropicGaussian(0.08)\n",
    "rnnbias_init = Constant(0.0)\n",
    "\n",
    "if rnnType == 'gru':\n",
    "    rnnDec = GatedRecurrent(dim=numTokens, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'gru')\n",
    "    dimMultiplier = 2\n",
    "else:\n",
    "    rnnDec = LSTM(dim=numTokens, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'lstm')\n",
    "    dimMultiplier = 4\n",
    "\n",
    "###ICLR suggestion -> don't use bias in RNNs\n",
    "###RECURRENT LAYER\n",
    "forkDec = Fork(output_names=['linear', 'gates'],\n",
    "            name='fork', input_dim=numTokens, output_dims=[numTokens, numTokens * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = rnnbias_init)\n",
    "data5, data6 = forkDec.apply(X)\n",
    "\n",
    "if rnnType == 'gru':\n",
    "    hDec = rnnDec.apply(data5, data6) \n",
    "else:\n",
    "    hinit, _ = rnnDec.apply(data6)\n",
    "    hDec = hinit\n",
    "\n",
    "#get weights initialized\n",
    "forkDec.initialize()\n",
    "rnnDec.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pYx = 1/(1+T.exp(-hDec))\n",
    "softmax = NDimensionalSoftmax()\n",
    "softout = softmax.apply(pYx, extra_ndim = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#test\n",
    "decoderTest = theano.function([X], softout, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#cost = BinaryCrossEntropy().apply(Y, softout)\n",
    "precost = Y*np.log(softout) + (1-Y)*np.log(1-softout)\n",
    "cost = -T.mean(T.sum(T.sum(precost[:,:-1,:], axis = 2), axis = 1))\n",
    "cg = ComputationGraph([cost])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "params = VariableFilter(roles = [PARAMETER])(cg.variables)\n",
    "#updates = Adam(params, cost, learning_rate, c=10) #c is gradient clipping parameter\n",
    "updates = RMSprop(cost, params, learning_rate, c=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gradients = T.grad(cost, params)\n",
    "gradients = clip_norms(gradients, 1)\n",
    "gradientFun = theano.function([X,Y], gradients, allow_input_downcast=True)\n",
    "train = theano.function([X,Y], cost, updates = updates, allow_input_downcast=True)\n",
    "predict = theano.function([X], softout, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#test\n",
    "inputs = np.asarray(normalizedData[:3])\n",
    "outputs = targetMaker(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#shuffle data\n",
    "random.shuffle(normalizedData)\n",
    "trainPercent = 0.9\n",
    "trainIndex = int(len(normalizedData)*trainPercent)\n",
    "\n",
    "trainData = normalizedData[0:trainIndex]\n",
    "testData = normalizedData[trainIndex:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO: make a training function\n",
    "runname = 'firstRun'\n",
    "epochCost = []\n",
    "gradNorms = []\n",
    "\n",
    "epochs = 200000\n",
    "batch_size = 64\n",
    "iteration = 0\n",
    "\n",
    "for epoch in xrange(epochs):\n",
    "    \n",
    "    costCollect = []\n",
    "\n",
    "    for start, end in zip(range(0, len(trainData),batch_size), range(batch_size, len(trainData), batch_size)):\n",
    "        \n",
    "        inputs = trainData[start:end]\n",
    "        outputs = targetMaker(inputs)\n",
    "        costfun = train(inputs, outputs)\n",
    "        \n",
    "        \n",
    "        costCollect.append(costfun)\n",
    "                \n",
    "        iteration+=1\n",
    "        \n",
    "    ####SAVE COST TO FILE  \n",
    "    if epoch%30 == 0:\n",
    "        print(' ')\n",
    "        print 'Epoch: ', epoch\n",
    "        epochCost.append(np.mean(costCollect))\n",
    "        print 'Epoch cost average: ', epochCost[-1]\n",
    "        grads = gradientFun(inputs, outputs)\n",
    "        for gra in grads:\n",
    "            print '  gradient norms: ', np.linalg.norm(gra)\n",
    "        \n",
    "    \n",
    "    np.savetxt(runname+\"_COST.csv\", epochCost, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataPath = '/data/bradspcaps.txt'\n",
    "data = []\n",
    "with open(dataPath, 'rb') as f:\n",
    "    for line in f.readlines():\n",
    "        data.append(line.split(\"'data': \")[-1].split(',')[0].replace(\"'\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Making the hex dictionary\n",
    "hexstring = '0,\t1,\t2,\t3,\t4,\t5,\t6,\t7,\t8,\t9,\tA,\tB,\tC,\tD,\tE,\tF,\t10,\t11,\t12,\t13,\t14,\t15,\t16,\t17,\t18,\t19\\\n",
    ",\t1A,\t1B,\t1C,\t1D,\t1E,\t1F,\t20,\t21,\t22,\t23,\t24,\t25,\t26,\t27,\t28,\t29,\t2A,\t2B,\t2C,\t2D,\t2E,\t2F,\t30,\t31,\t32,\t33,\t34,\t35\\\n",
    ",\t36,\t37,\t38,\t39,\t3A,\t3B,\t3C,\t3D,\t3E,\t3F,\t40,\t41,\t42,\t43,\t44,\t45,\t46,\t47,\t48,\t49,\t4A,\t4B,\t4C,\t4D,\t4E,\t4F,\t50,\t51\\\n",
    ",\t52,\t53,\t54,\t55,\t56,\t57,\t58,\t59,\t5A,\t5B,\t5C,\t5D,\t5E,\t5F,\t60,\t61,\t62,\t63,\t64,\t65,\t66,\t67,\t68,\t69,\t6A,\t6B,\t6C,\t6D\\\n",
    ",\t6E,\t6F,\t70,\t71,\t72,\t73,\t74,\t75,\t76,\t77,\t78,\t79,\t7A,\t7B,\t7C,\t7D,\t7E,\t7F,\t80,\t81,\t82,\t83,\t84,\t85,\t86,\t87,\t88,\t89\\\n",
    ",\t8A,\t8B,\t8C,\t8D,\t8E,\t8F,\t90,\t91,\t92,\t93,\t94,\t95,\t96,\t97,\t98,\t99,\t9A,\t9B,\t9C,\t9D,\t9E,\t9F,\tA0,\tA1,\tA2,\tA3,\tA4,\tA5\\\n",
    ",\tA6,\tA7,\tA8,\tA9,\tAA,\tAB,\tAC,\tAD,\tAE,\tAF,\tB0,\tB1,\tB2,\tB3,\tB4,\tB5,\tB6,\tB7,\tB8,\tB9,\tBA,\tBB,\tBC,\tBD,\tBE,\tBF,\tC0,\tC1\\\n",
    ",\tC2,\tC3,\tC4,\tC5,\tC6,\tC7,\tC8,\tC9,\tCA,\tCB,\tCC,\tCD,\tCE,\tCF,\tD0,\tD1,\tD2,\tD3,\tD4,\tD5,\tD6,\tD7,\tD8,\tD9,\tDA,\tDB,\tDC,\tDD\\\n",
    ",\tDE,\tDF,\tE0,\tE1,\tE2,\tE3,\tE4,\tE5,\tE6,\tE7,\tE8,\tE9,\tEA,\tEB,\tEC,\tED,\tEE,\tEF,\tF0,\tF1,\tF2,\tF3,\tF4,\tF5,\tF6,\tF7,\tF8,\tF9\\\n",
    ",\tFA,\tFB,\tFC,\tFD,\tFE,\tFF'.replace('\\t', '')\n",
    "\n",
    "hexList = hexstring.lower().split(',')\n",
    "hexList.append('EOP') #End Of Packet token\n",
    "hexDict = {}\n",
    "    \n",
    "for key, val in enumerate(hexList):\n",
    "    if len(val) == 1:\n",
    "        val = '0'+val\n",
    "    hexDict[val] = key    \n",
    "\n",
    "#we add 256 on the end to signify the end of the packet ('EOP')\n",
    "tokenizedHeader = [[hexDict[header[i:i+2]] for i in xrange(0,len(header)-2+1,2)]+[256] for header in data]\n",
    "\n",
    "\n",
    "#list of arrays that represent a header with row = time \n",
    "oneHotHeaders = [np.asarray([oneHot(item) for item in header]) for header in tokenizedHeader]\n",
    "\n",
    "normalizedData = normalizeArrays(oneHotHeaders, 253, reverse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Pretraining essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numTokens = 257\n",
    "rnnType = 'gru'\n",
    "X = T.tensor3('inputs')\n",
    "Y = T.tensor3('outputs')\n",
    "linewt_init = Uniform(width=0.02)\n",
    "rnnwt_init = IsotropicGaussian(0.08)\n",
    "rnnbias_init = Constant(0.0)\n",
    "\n",
    "if rnnType == 'gru':\n",
    "    rnnDec = GatedRecurrent(dim=numTokens, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'gru')\n",
    "    dimMultiplier = 2\n",
    "else:\n",
    "    rnnDec = LSTM(dim=numTokens, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'lstm')\n",
    "    dimMultiplier = 4\n",
    "\n",
    "###ICLR suggestion -> don't use bias in RNNs\n",
    "###RECURRENT LAYER\n",
    "forkDec = Fork(output_names=['linear', 'gates'],\n",
    "            name='fork', input_dim=numTokens, output_dims=[numTokens, numTokens * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = rnnbias_init)\n",
    "data5, data6 = forkDec.apply(X)\n",
    "\n",
    "if rnnType == 'gru':\n",
    "    hDec = rnnDec.apply(data5, data6) \n",
    "else:\n",
    "    hinit, _ = rnnDec.apply(data6)\n",
    "    hDec = hinit\n",
    "\n",
    "#CRITICAL: need to loop through the arrays. Do regular people update after every sequence? or minibatch of seqs?\n",
    "    \n",
    "pYx = 1/(1+T.exp(-hDec))\n",
    "softmax = NDimensionalSoftmax()\n",
    "softout = softmax.apply(pYx, extra_ndim = 1)\n",
    "\n",
    "#get weights initialized\n",
    "forkDec.initialize()\n",
    "rnnDec.initialize()\n",
    "\n",
    "#cost = BinaryCrossEntropy().apply(Y, softout)\n",
    "precost = Y*np.log(softout) + (1-Y)*np.log(1-softout)\n",
    "cost = -T.mean(T.sum(T.sum(precost[:,:-1,:], axis = 2), axis = 1))\n",
    "cg = ComputationGraph([cost])\n",
    "\n",
    "learning_rate = 0.01\n",
    "params = VariableFilter(roles = [PARAMETER])(cg.variables)\n",
    "#updates = Adam(params, cost, learning_rate, c=10) #c is gradient clipping parameter\n",
    "updates = RMSprop(cost, params, learning_rate, c=1)\n",
    "\n",
    "gradients = T.grad(cost, params)\n",
    "gradients = clip_norms(gradients, 1)\n",
    "gradientFun = theano.function([X,Y], gradients, allow_input_downcast=True)\n",
    "train = theano.function([X,Y], cost, updates = updates, allow_input_downcast=True)\n",
    "predict = theano.function([X], softout, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random.shuffle(normalizedData)\n",
    "trainPercent = 0.9\n",
    "trainIndex = int(len(normalizedData)*trainPercent)\n",
    "\n",
    "trainData = normalizedData[0:trainIndex]\n",
    "testData = normalizedData[trainIndex:]\n",
    "\n",
    "runname = 'firstRun'\n",
    "epochCost = []\n",
    "gradNorms = []\n",
    "\n",
    "epochs = 200000\n",
    "batch_size = 64\n",
    "iteration = 0\n",
    "\n",
    "for epoch in xrange(epochs):\n",
    "    \n",
    "    costCollect = []\n",
    "\n",
    "    for start, end in zip(range(0, len(trainData),batch_size), range(batch_size, len(trainData), batch_size)):\n",
    "        \n",
    "        inputs = trainData[start:end]\n",
    "        outputs = targetMaker(inputs)\n",
    "        costfun = train(inputs, outputs)\n",
    "        \n",
    "        \n",
    "        costCollect.append(costfun)\n",
    "                \n",
    "        iteration+=1\n",
    "        \n",
    "    ####SAVE COST TO FILE  \n",
    "    if epoch%30 == 0:\n",
    "        print(' ')\n",
    "        print 'Epoch: ', epoch\n",
    "        epochCost.append(np.mean(costCollect))\n",
    "        print 'Epoch cost average: ', epochCost[-1]\n",
    "        grads = gradientFun(inputs, outputs)\n",
    "        for gra in grads:\n",
    "            print '  gradient norms: ', np.linalg.norm(gra)\n",
    "        \n",
    "    \n",
    "    np.savetxt(runname+\"_COST.csv\", epochCost, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting to CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GPU TO CPU conversion\n",
    "#Now get the weights from the test function. These weights will be numpy arrays\n",
    "w1 = test.get_shared()[0].get_value()\n",
    "\n",
    "#Here the weights are going to be set to the numpy arrays taken from the GPU predict function\n",
    "input_linear.parameters[0].set_value(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.get_shared()[2].get_value().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = '1234567890abcdefghijklmnopqrstuvwxyz'\n",
    "words = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#we add 256 on the end to signify the end of the packet ('EOP')\n",
    "\n",
    "maxPackets = 10 #limit the number of packets\n",
    "tokSessions = []\n",
    "oneHotSessions = []\n",
    "\n",
    "for ses in hexSessions.keys():    \n",
    "    tokPacket = []\n",
    "    oneHotPacket = []\n",
    "    for p in hexSessions[ses][:maxPackets]:\n",
    "        tokP = [hexDict[p[i:i+2]] for i in xrange(0,len(p)-2+1,2)]+[256] #takes hexstring and tokenizes hex pairs\n",
    "        tokPacket.append(tokP)\n",
    "        oneHotPacket.append(oneHot(tokP))\n",
    "\n",
    "    tokSessions.append(tokPacket)\n",
    "    oneHotSessions.append(oneHotPacket)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###ALT RNN LAYER\n",
    "def initialize(to_init):\n",
    "    for bricks in to_init:\n",
    "        bricks.weights_init = initialization.Uniform(width=0.08)\n",
    "        bricks.biases_init = initialization.Constant(0)\n",
    "        bricks.initialize()\n",
    "\n",
    "def gru_layer(dim, h, n):\n",
    "    fork = Fork(output_names=['linear' + str(n), 'gates' + str(n)],\n",
    "                name='fork' + str(n), input_dim=dim, output_dims=[dim, dim * 2])\n",
    "    gru = GatedRecurrent(dim=dim, name='gru' + str(n))\n",
    "    initialize([fork, gru])\n",
    "    linear, gates = fork.apply(h)\n",
    "    return gru.apply(linear, gates)\n",
    "\n",
    "\n",
    "def lstm_layer(dim, h, n):\n",
    "    linear = Linear(input_dim=dim, output_dim=dim * 4, name='linear' + str(n))\n",
    "    lstm = LSTM(dim=dim, name='lstm' + str(n))\n",
    "    initialize([linear, lstm])\n",
    "    return lstm.apply(linear.apply(h))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
