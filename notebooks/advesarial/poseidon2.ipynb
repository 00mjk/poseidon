{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to execute tcpdump. Check it is installed and in the PATH\n",
      "WARNING: No route found for IPv6 destination :: (no default route?)\n",
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled, cuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import binascii\n",
    "import multiprocessing as mp\n",
    "from itertools import chain\n",
    "from scapy.all import *\n",
    "sys.path.append('hed-dlg/')\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.stats import itemfreq\n",
    "\n",
    "import blocks\n",
    "from blocks.bricks import Linear, Softmax, Softplus, NDimensionalSoftmax\n",
    "from blocks.bricks.recurrent import GatedRecurrent, Fork, LSTM\n",
    "from blocks.initialization import Constant, IsotropicGaussian, Identity, Uniform\n",
    "from blocks.bricks.cost import BinaryCrossEntropy, CategoricalCrossEntropy\n",
    "from blocks.filter import VariableFilter\n",
    "from blocks.roles import PARAMETER\n",
    "from blocks.graph import ComputationGraph\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "\n",
    "###These warnings do not impede progress\n",
    "#WARNING: Failed to execute tcpdump. Check it is installed and in the PATH\n",
    "#WARNING: No route found for IPv6 destination :: (no default route?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataPath = '/data/bradsfirstpcaps.pcap'\n",
    "pcaps = rdpcap(dataPath)\n",
    "sessions = pcaps.sessions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#turn the sessions into a dictionary key = session_number, val = list of packages in hex\n",
    "\n",
    "i=0\n",
    "hexSessions = {}\n",
    "\n",
    "for k,v in sessions.items(): # v is the session\n",
    "    #for attr, value in v.__dict__.iteritems(): THIS IS TO GET DICT OF VALUES\n",
    "    #    print attr, value\n",
    "    #if i == 2:\n",
    "    #    break\n",
    "    scpcaps = []    \n",
    "    for p in v: #p is the individual packet in the session\n",
    "        \n",
    "        try:\n",
    "            rawindex = len(p[Raw])\n",
    "            payloadLens.append(rawindex)\n",
    "            scpcaps.append(binascii.hexlify(str(p.original)[:-rawindex])) #turn it into hex\n",
    "        except:\n",
    "            scpcaps.append(binascii.hexlify(p.original))\n",
    "        #for attr, value in p.payload.__dict__.iteritems():#this give the fields that are accessable\n",
    "        #    print attr, value\n",
    "        \n",
    "        #print len(binascii.hexlify(p.original))\n",
    "    hexSessions['session_' + str(i)] = scpcaps\n",
    "    \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Making the hex dictionary\n",
    "hexstring = '0,\t1,\t2,\t3,\t4,\t5,\t6,\t7,\t8,\t9,\tA,\tB,\tC,\tD,\tE,\tF,\t10,\t11,\t12,\t13,\t14,\t15,\t16,\t17,\t18,\t19\\\n",
    ",\t1A,\t1B,\t1C,\t1D,\t1E,\t1F,\t20,\t21,\t22,\t23,\t24,\t25,\t26,\t27,\t28,\t29,\t2A,\t2B,\t2C,\t2D,\t2E,\t2F,\t30,\t31,\t32,\t33,\t34,\t35\\\n",
    ",\t36,\t37,\t38,\t39,\t3A,\t3B,\t3C,\t3D,\t3E,\t3F,\t40,\t41,\t42,\t43,\t44,\t45,\t46,\t47,\t48,\t49,\t4A,\t4B,\t4C,\t4D,\t4E,\t4F,\t50,\t51\\\n",
    ",\t52,\t53,\t54,\t55,\t56,\t57,\t58,\t59,\t5A,\t5B,\t5C,\t5D,\t5E,\t5F,\t60,\t61,\t62,\t63,\t64,\t65,\t66,\t67,\t68,\t69,\t6A,\t6B,\t6C,\t6D\\\n",
    ",\t6E,\t6F,\t70,\t71,\t72,\t73,\t74,\t75,\t76,\t77,\t78,\t79,\t7A,\t7B,\t7C,\t7D,\t7E,\t7F,\t80,\t81,\t82,\t83,\t84,\t85,\t86,\t87,\t88,\t89\\\n",
    ",\t8A,\t8B,\t8C,\t8D,\t8E,\t8F,\t90,\t91,\t92,\t93,\t94,\t95,\t96,\t97,\t98,\t99,\t9A,\t9B,\t9C,\t9D,\t9E,\t9F,\tA0,\tA1,\tA2,\tA3,\tA4,\tA5\\\n",
    ",\tA6,\tA7,\tA8,\tA9,\tAA,\tAB,\tAC,\tAD,\tAE,\tAF,\tB0,\tB1,\tB2,\tB3,\tB4,\tB5,\tB6,\tB7,\tB8,\tB9,\tBA,\tBB,\tBC,\tBD,\tBE,\tBF,\tC0,\tC1\\\n",
    ",\tC2,\tC3,\tC4,\tC5,\tC6,\tC7,\tC8,\tC9,\tCA,\tCB,\tCC,\tCD,\tCE,\tCF,\tD0,\tD1,\tD2,\tD3,\tD4,\tD5,\tD6,\tD7,\tD8,\tD9,\tDA,\tDB,\tDC,\tDD\\\n",
    ",\tDE,\tDF,\tE0,\tE1,\tE2,\tE3,\tE4,\tE5,\tE6,\tE7,\tE8,\tE9,\tEA,\tEB,\tEC,\tED,\tEE,\tEF,\tF0,\tF1,\tF2,\tF3,\tF4,\tF5,\tF6,\tF7,\tF8,\tF9\\\n",
    ",\tFA,\tFB,\tFC,\tFD,\tFE,\tFF'.replace('\\t', '')\n",
    "\n",
    "hexList = hexstring.lower().split(',')\n",
    "hexList.append('<EOP>') #End Of Packet token\n",
    "hexDict = {}\n",
    "    \n",
    "for key, val in enumerate(hexList):\n",
    "    if len(val) == 1:\n",
    "        val = '0'+val\n",
    "    hexDict[val] = key  #dictionary k=hex, v=int  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Is padding on top (older timesteps/sessions) better than padding on bottom (recent timesteps)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxPackets = 10\n",
    "#def hexOneHot(number):\n",
    "#    zeroVec = np.zeros(257)\n",
    "#    zeroVec[number] = 1.0\n",
    "#    return zeroVec\n",
    "\n",
    "def oneHot(index, granular = 'hex'):\n",
    "    if granular == 'hex':\n",
    "        vecLen = 257\n",
    "    else:\n",
    "        vecLen = 17\n",
    "    \n",
    "    zeroVec = np.zeros(vecLen)\n",
    "    zeroVec[index] = 1.0\n",
    "    \n",
    "    return zeroVec\n",
    "\n",
    "def oneHotSessions(sessionDict, maxPackets = maxPackets, packetTimeSteps = 256,\n",
    "                   reverse = False, charLevel = False):\n",
    "    \"\"\"\n",
    "    This takes a list of int tokens and onehot encodes them, pads sessions with zero tensors according to maxPackets\n",
    "    and packet according to packetTimeSteps\n",
    "    \n",
    "    sessionDict = dict of lists of key = sessions and value = list of packets\n",
    "    timeSteps = maximum len of packet. it will be padded with zero vectors is packet is too short.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    listOsessions = []\n",
    "\n",
    "    if charLevel:\n",
    "        vecLen = 17\n",
    "    else:\n",
    "        vecLen = 257\n",
    "    \n",
    "    sessionKeys = sessionDict.keys()\n",
    "    \n",
    "    for session in sessionKeys:\n",
    "        #sessionTensor = np.zeros((maxPackets, packetTimeSteps, vecLen))\n",
    "        #lenSession = len(session)\n",
    "        sessionCollect = []\n",
    "        \n",
    "        if len(sessionDict[session]) > maxPackets: #crop the number of sessions to maxPackets\n",
    "            sessionList = sessionDict[session][:maxPackets]\n",
    "        else:\n",
    "            sessionList = sessionDict[session]\n",
    "        \n",
    "        for packet in sessionList:\n",
    "            packet = [hexDict[packet[i:i+2]] for i in xrange(0,len(packet)-2+1,2)]\n",
    "            \n",
    "            if len(packet) >= packetTimeSteps: #crop packet to length packetTimeSteps\n",
    "                packet = packet[:packetTimeSteps-1]\n",
    "            \n",
    "            packet = packet+[256] #add <EOP> end of packet token\n",
    "        \n",
    "            pacMat = np.array([oneHot(x) for x in packet]) #one hot encoding of packet into a matrix\n",
    "            pacMatLen = len(pacMat)\n",
    "        \n",
    "            #padding packet\n",
    "            if reverse:\n",
    "                pacMat = pacMat[::-1]\n",
    "            \n",
    "            if pacMatLen < packetTimeSteps:\n",
    "                #pad by stacking zeros on top of data so that earlier timesteps do not have information\n",
    "                #padding the packet such that zeros are after the actual info for better translation\n",
    "                pacMat = np.vstack( (pacMat, np.zeros((packetTimeSteps-pacMatLen,vecLen))) ) \n",
    "\n",
    "            if pacMatLen > packetTimeSteps:\n",
    "                pacMat = pacMat[:packetTimeSteps, :]\n",
    "                \n",
    "            sessionCollect.append(pacMat)\n",
    "        \n",
    "        #padding session\n",
    "        sessionCollect = np.asarray(sessionCollect)\n",
    "        numPacketsInSession = np.asarray(sessionCollect).shape[0]\n",
    "        if numPacketsInSession < maxPackets:\n",
    "            #pad sessions to fit the \n",
    "            sessionCollect = np.vstack( (sessionCollect,np.zeros((maxPackets-numPacketsInSession, \n",
    "                                                                 packetTimeSteps, vecLen))) )\n",
    "            \n",
    "        listOsessions.append(sessionCollect)\n",
    "        \n",
    "    return listOsessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sessions = oneHotSessions(hexSessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def dropout(X, p=0.):\n",
    "    if p != 0:\n",
    "        retain_prob = 1 - p\n",
    "        X = X / retain_prob * srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX)\n",
    "    return X\n",
    "\n",
    "# Gradient clipping\n",
    "def clip_norm(g, c, n): \n",
    "    '''n is the norm, c is the threashold, and g is the gradient'''\n",
    "    \n",
    "    if c > 0: \n",
    "        g = T.switch(T.ge(n, c), g*c/n, g) \n",
    "    return g\n",
    "\n",
    "def clip_norms(gs, c):\n",
    "    norm = T.sqrt(sum([T.sum(g**2) for g in gs]))\n",
    "    return [clip_norm(g, c, norm) for g in gs]\n",
    "\n",
    "# Regularizers\n",
    "def max_norm(p, maxnorm = 0.):\n",
    "    if maxnorm > 0:\n",
    "        norms = T.sqrt(T.sum(T.sqr(p), axis=0))\n",
    "        desired = T.clip(norms, 0, maxnorm)\n",
    "        p = p * (desired/ (1e-7 + norms))\n",
    "    return p\n",
    "\n",
    "def gradient_regularize(p, g, l1 = 0., l2 = 0.):\n",
    "    g += p * l2\n",
    "    g += T.sgn(p) * l1\n",
    "    return g\n",
    "\n",
    "def weight_regularize(p, maxnorm = 0.):\n",
    "    p = max_norm(p, maxnorm)\n",
    "    return p\n",
    "\n",
    "def Adam(params, cost, lr=0.0002, b1=0.1, b2=0.001, e=1e-8, l1 = 0., l2 = 0., maxnorm = 0., c = 8):\n",
    "    \n",
    "    updates = []\n",
    "    grads = T.grad(cost, params)\n",
    "    grads = clip_norms(grads, c)\n",
    "    \n",
    "    i = theano.shared(floatX(0.))\n",
    "    i_t = i + 1.\n",
    "    fix1 = 1. - b1**(i_t)\n",
    "    fix2 = 1. - b2**(i_t)\n",
    "    lr_t = lr * (T.sqrt(fix2) / fix1)\n",
    "    \n",
    "    for p, g in zip(params, grads):\n",
    "        m = theano.shared(p.get_value() * 0.)\n",
    "        v = theano.shared(p.get_value() * 0.)\n",
    "        m_t = (b1 * g) + ((1. - b1) * m)\n",
    "        v_t = (b2 * T.sqr(g)) + ((1. - b2) * v)\n",
    "        g_t = m_t / (T.sqrt(v_t) + e)\n",
    "        g_t = gradient_regularize(p, g_t, l1=l1, l2=l2)\n",
    "        p_t = p - (lr_t * g_t)\n",
    "        p_t = weight_regularize(p_t, maxnorm=maxnorm)\n",
    "        \n",
    "        updates.append((m, m_t))\n",
    "        updates.append((v, v_t))\n",
    "        updates.append((p, p_t))\n",
    "    \n",
    "    updates.append((i, i_t))\n",
    "    return updates\n",
    "\n",
    "def RMSprop(cost, params, lr = 0.001, l1 = 0., l2 = 0., maxnorm = 0., rho=0.9, epsilon=1e-6, c = 8):\n",
    "    \n",
    "    grads = T.grad(cost, params)\n",
    "    grads = clip_norms(grads, c)\n",
    "    updates = []\n",
    "    \n",
    "    for p, g in zip(params, grads):\n",
    "        g = gradient_regularize(p, g, l1 = l1, l2 = l2)\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        updates.append((acc, acc_new))\n",
    "        \n",
    "        updated_p = p - lr * (g / T.sqrt(acc_new + epsilon))\n",
    "        updated_p = weight_regularize(updated_p, maxnorm = maxnorm)\n",
    "        updates.append((p, updated_p))\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#makes output by shifting inputs down in time one step and then copying the last time step to the end.\n",
    "def targetModifier(targetArray):\n",
    "    newTarget = np.vstack((targetArray[1:, :], targetArray[-1,:]))\n",
    "    return newTarget\n",
    "\n",
    "def targetMaker(listOinputs):\n",
    "    #TODO: do this with arrays\n",
    "    outputs = []\n",
    "    for inp in listOinputs:\n",
    "        outputs.append(targetModifier(inp))\n",
    "    outputs = np.asarray(outputs)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim = 257 #original data dimension/timesteps/columns\n",
    "rnnType = 'gru' #gru or lstm\n",
    "bidirectional = True\n",
    "X = T.tensor3('inputs')\n",
    "Xrev = T.matrix('reversed_inputs')\n",
    "linewt_init = Uniform(width=0.08)\n",
    "rnnwt_init = IsotropicGaussian(0.05)\n",
    "rnnbias_init = Constant(0.0)\n",
    "\n",
    "if rnnType == 'gru':\n",
    "    rnn = GatedRecurrent(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'gru')\n",
    "    dimMultiplier = 2\n",
    "else:\n",
    "    rnn = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'lstm')\n",
    "    dimMultiplier = 4\n",
    "\n",
    "###ICLR suggestion -> don't use bias in RNNs\n",
    "###RECURRENT LAYER\n",
    "\n",
    "#To use or not to use that is the question\n",
    "fork = Fork(output_names=['linear', 'gates'],\n",
    "            name='fork', input_dim=dim, output_dims=[dim, dim * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = rnnbias_init)\n",
    "data1, data2 = fork.apply(X)\n",
    "\n",
    "###for raw inputs\n",
    "#data1 = X\n",
    "#data2 = T.concatenate([X]*dimMultiplier, axis=2)\n",
    "\n",
    "if rnnType == 'gru':\n",
    "    hEnc = rnn.apply(data1, data2)[:,-1] #the [:,-1] gets the last hidden state for each obs in minibatch\n",
    "                                         #i.e. the last state for each sentence\n",
    "else:\n",
    "    hinit, _ = rnn.apply(data2)\n",
    "    hEnc = hinit[:,-1]\n",
    "\n",
    "hEnc = T.reshape(hEnc,(maxPackets, 1, dim))\n",
    "#get weights initialized. without weights are nans.\n",
    "fork.initialize()\n",
    "rnn.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Encoder will return a maxPackets x packet length matrix\n",
    "encoder = theano.function([X], hEnc, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1, 257)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test ENCODED PACKETS shape = (maxPackets, 1, dim)\n",
    "encoder(sessions[1]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "if rnnType == 'gru':\n",
    "    rnnContext = GatedRecurrent(dim=dim, weights_init = rnnwt_init, \n",
    "                                biases_init = rnnbias_init, name = 'gruContext')\n",
    "    dimMultiplier = 2\n",
    "else:\n",
    "    rnnContext = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, \n",
    "                      name = 'lstmContext')\n",
    "    dimMultiplier = 4\n",
    "\n",
    "\n",
    "###ICLR suggestion -> don't use bias in RNNs\n",
    "#is encoding at each layer really the best way? or just feeding the raw through?\n",
    "###RECURRENT LAYER\n",
    "forkContext = Fork(output_names=['linearContext', 'gatesContext'],\n",
    "            name='forkContext', input_dim=dim, output_dims=[dim, dim * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = rnnbias_init)\n",
    "data3, data4 = forkContext.apply(hEnc)\n",
    "\n",
    "if rnnType == 'gru':\n",
    "    hContext = rnnContext.apply(data3, data4)\n",
    "else:\n",
    "    hinitContext, _ = rnnContext.apply(data4)\n",
    "    hContext = hinitContext\n",
    "\n",
    "#THINK ABOUT ADDING L2 POOLING BEFORE CAT\n",
    "if bidirectional:\n",
    "    \n",
    "    data3 = data3[::-1]\n",
    "    data4 = data4[::-1]\n",
    "    \n",
    "    if rnnType == 'gru':\n",
    "        rnnContextRev = GatedRecurrent(dim=dim, weights_init = rnnwt_init, \n",
    "                                       biases_init = rnnbias_init, name = 'gruContextRev')\n",
    "        hContextRev = rnnContextRev.apply(data3, data4)\n",
    "    else:\n",
    "        rnnContextRev = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init,\n",
    "                             name = 'lstmContextRev')\n",
    "        hinitContext, _ = rnnContextRev.apply(data4)\n",
    "        hContextRev = hinitContext\n",
    "    \n",
    "    \n",
    "    hContext = T.concatenate((hContext, hContextRev), axis=2)\n",
    "    rnnContextRev.initialize()\n",
    "    \n",
    "#get weights initialized. without weights are nans.\n",
    "forkContext.initialize()\n",
    "rnnContext.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#output shape = (maxPackets, 1, dim*2)\n",
    "context = theano.function([X], hContext, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1, 514)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context(sessions[1]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#does the fork encoding need to happen here?\n",
    "#do we simply cat the hContext with the next words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dimDec = dim*2\n",
    "\n",
    "if rnnType == 'gru':\n",
    "    rnnDec = GatedRecurrent(dim=dim, weights_init = rnnwt_init, \n",
    "                            biases_init = rnnbias_init, name = 'gruDecoder')\n",
    "    dimMultiplier = 2\n",
    "else:\n",
    "    rnnDec = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'lstmDecoder')\n",
    "    dimMultiplier = 4\n",
    "\n",
    "\n",
    "forkDec = Fork(output_names=['linear', 'gates'],\n",
    "            name='forkDec', input_dim=dimDec, output_dims=[dim, dim*dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = rnnbias_init)\n",
    "\n",
    "forkFinal = Fork(output_names=['linear', 'gates'],\n",
    "            name='forkDec', input_dim=dim, output_dims=[dim, dim*dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = rnnbias_init)\n",
    "\n",
    "data5, data6 = forkDec.apply(hContext)#reduce dimension of bidirectLSTM\n",
    "\n",
    "#decoding data needs to be one timestep (next packet in session) ahead, thus data1 we ignore the first packet\n",
    "#and the last hidden state of the context RNN.\n",
    "data7 = T.concatenate((data5[:-1,:,:], data1[1:, :-1, :]), axis=1) #data1 is the original embedding of X\n",
    "\n",
    "#data8 = T.concatenate((data7, data5), axis = 2)\n",
    "data8, data9 = forkFinal.apply(data7)\n",
    "\n",
    "\n",
    "if rnnType == 'gru':\n",
    "    hDec = rnnDec.apply(data8, data9) \n",
    "else:\n",
    "    hinit, _ = rnnDec.apply(data9)\n",
    "    hDec = hinit\n",
    "\n",
    "#Smooth out the probabilities of hDec\n",
    "softmax = NDimensionalSoftmax()\n",
    "softout = softmax.apply(hDec, extra_ndim = 1)\n",
    "    \n",
    "\n",
    "precost = X[1:, :, :]*np.log(softout) + (1-X[1:, :, :])*np.log(1-softout)\n",
    "cost = -T.mean(T.sum(T.sum(precost, axis = 2), axis = 1))\n",
    "#cost = BinaryCrossEntropy().apply(X[1:, :, :], softout)\n",
    "\n",
    "#get weights initialized\n",
    "forkDec.initialize()\n",
    "forkFinal.initialize()\n",
    "rnnDec.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoderTest = theano.function([X], cost, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(3.7743141651153564, dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoderTest(sessions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#output shape = (maxPackets, )\n",
    "decoder = theano.function([X], hDec, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decTest = decoder(sessions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  2.84144655e-04,  -1.53979345e-04,  -1.43260710e-04, ...,\n",
       "           5.78663048e-06,  -8.96845377e-05,   3.08299241e-05],\n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "        ..., \n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   0.00000000e+00]],\n",
       "\n",
       "       [[  6.37453981e-04,  -2.28210120e-04,  -1.84183169e-04, ...,\n",
       "           6.43644235e-05,  -2.77327839e-04,   1.47678642e-04],\n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "        ..., \n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   0.00000000e+00]],\n",
       "\n",
       "       [[  9.47057793e-04,  -2.40589114e-04,  -1.43340629e-04, ...,\n",
       "           1.28893953e-04,  -4.82089934e-04,   2.88722222e-04],\n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "        ..., \n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "           0.00000000e+00,   0.00000000e+00,   0.00000000e+00]],\n",
       "\n",
       "       ..., \n",
       "       [[  2.30977917e-03,  -1.60942349e-04,  -1.48877618e-03, ...,\n",
       "           9.23844636e-04,  -8.49577365e-04,   3.51758965e-04],\n",
       "        [  1.22066159e-02,   8.52424558e-03,  -1.13647254e-02, ...,\n",
       "           5.08341240e-03,  -2.36705155e-03,  -1.28522590e-02],\n",
       "        [  9.65225603e-03,   1.73035008e-03,  -6.54726941e-03, ...,\n",
       "          -1.16372190e-03,  -2.27939733e-03,   3.89045570e-03],\n",
       "        ..., \n",
       "        [ -5.67761424e-04,  -5.31268679e-03,   4.60300082e-03, ...,\n",
       "          -9.89840366e-03,   7.07967964e-04,  -3.65536893e-03],\n",
       "        [ -8.77774041e-03,  -1.89916126e-03,  -8.88287614e-04, ...,\n",
       "           5.94866753e-04,  -1.20164489e-03,   1.20546739e-03],\n",
       "        [  4.49120300e-03,  -5.22807473e-03,  -9.77104250e-03, ...,\n",
       "          -5.71093615e-03,   2.95324228e-03,   4.90892620e-04]],\n",
       "\n",
       "       [[  2.20467499e-03,  -3.34427423e-05,  -1.79887400e-03, ...,\n",
       "           8.95515492e-04,  -7.01483281e-04,   2.79831060e-04],\n",
       "        [  1.31708290e-02,   9.79325548e-03,  -1.22276023e-02, ...,\n",
       "           6.59221737e-03,  -2.11373251e-03,  -1.40027888e-02],\n",
       "        [  1.01839555e-02,   2.12800386e-03,  -6.14016363e-03, ...,\n",
       "          -1.34852983e-03,  -2.68176035e-03,   5.40152611e-03],\n",
       "        ..., \n",
       "        [ -2.47608498e-03,  -1.09406449e-02,   2.17915070e-03, ...,\n",
       "          -5.97661547e-03,   1.68137078e-03,  -3.73530528e-03],\n",
       "        [ -8.98720697e-03,  -8.87220632e-03,   2.79108901e-03, ...,\n",
       "           3.09082563e-03,   1.56346476e-03,  -1.78199145e-03],\n",
       "        [  5.28892223e-03,  -6.65067276e-03,  -7.93376379e-03, ...,\n",
       "          -2.42890688e-04,  -7.71941198e-03,   1.67615782e-03]],\n",
       "\n",
       "       [[  2.09395355e-03,   6.38966740e-05,  -2.05070199e-03, ...,\n",
       "           8.18719971e-04,  -5.53162186e-04,   1.75211491e-04],\n",
       "        [  7.09941331e-03,   6.60031429e-03,  -6.63905870e-03, ...,\n",
       "           6.77918270e-03,   2.69954791e-04,  -7.79951690e-03],\n",
       "        [  5.01088984e-03,   2.17807945e-03,  -9.36248252e-05, ...,\n",
       "          -1.40730885e-03,  -2.47993669e-03,   6.58421451e-03],\n",
       "        ..., \n",
       "        [ -3.70115601e-03,  -9.34684556e-03,  -1.68227672e-03, ...,\n",
       "          -8.14411137e-03,   1.01184053e-03,  -9.55313630e-03],\n",
       "        [ -4.39200434e-04,  -9.66731750e-04,   3.23844119e-03, ...,\n",
       "          -2.88038072e-03,   4.92961553e-04,  -7.89313577e-03],\n",
       "        [  4.31921473e-03,  -2.81898072e-03,  -2.34082341e-03, ...,\n",
       "          -2.03736662e-03,  -8.98009352e-03,   1.05126190e-03]]], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pYx = 1/(1+T.exp(-hDec))\n",
    "softmax = NDimensionalSoftmax()\n",
    "softout = softmax.apply(pYx, extra_ndim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#test\n",
    "decoderTest = theano.function([X], softout, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#cost = BinaryCrossEntropy().apply(Y, softout)\n",
    "\n",
    "#cg = ComputationGraph([cost])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "costTest = theano.function([X], precost, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testOut = costTest(sessions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ -3.89871583e-03,  -3.89872422e-03,  -3.89882107e-03, ...,\n",
       "          -3.89862969e-03,  -3.89850629e-03,  -3.89875122e-03],\n",
       "        [ -3.89181473e-03,  -3.88616836e-03,  -3.89087247e-03, ...,\n",
       "          -3.89794679e-03,  -3.91108310e-03,  -3.93225765e-03],\n",
       "        [ -3.91480559e-03,  -3.91625846e-03,  -3.90912173e-03, ...,\n",
       "          -3.90114449e-03,  -3.88275879e-03,  -3.90282343e-03],\n",
       "        ..., \n",
       "        [ -3.89864040e-03,  -3.89864040e-03,  -3.89864040e-03, ...,\n",
       "          -3.89864040e-03,  -3.89864040e-03,  -3.89864040e-03],\n",
       "        [ -3.89864040e-03,  -3.89864040e-03,  -3.89864040e-03, ...,\n",
       "          -3.89864040e-03,  -3.89864040e-03,  -3.89864040e-03],\n",
       "        [ -3.89864040e-03,  -3.89864040e-03,  -3.89864040e-03, ...,\n",
       "          -3.89864040e-03,  -3.89864040e-03,  -3.89864040e-03]],\n",
       "\n",
       "       [[ -3.89882922e-03,  -3.89889674e-03,  -3.89896357e-03, ...,\n",
       "          -3.89861921e-03,  -3.89827415e-03,  -3.89897381e-03],\n",
       "        [ -3.88982450e-03,  -3.88449896e-03,  -3.88299325e-03, ...,\n",
       "          -3.89716262e-03,  -3.91514041e-03,  -3.95373814e-03],\n",
       "        [ -3.92720615e-03,  -3.92896123e-03,  -3.91157297e-03, ...,\n",
       "          -3.90596339e-03,  -3.87852709e-03,  -3.90627328e-03],\n",
       "        ..., \n",
       "        [ -3.90454591e-03,  -3.90042504e-03,  -3.87092726e-03, ...,\n",
       "          -3.90566746e-03,  -3.91491409e-03,  -3.90987657e-03],\n",
       "        [ -3.90627282e-03,  -3.90441623e-03,  -3.89422616e-03, ...,\n",
       "          -3.92215792e-03,  -3.93405836e-03,  -3.91016621e-03],\n",
       "        [ -3.88450711e-03,  -3.89784621e-03,  -3.90994176e-03, ...,\n",
       "          -3.87691217e-03,  -3.92973702e-03,  -5.55008650e+00]],\n",
       "\n",
       "       [[ -3.89942038e-03,  -3.89976171e-03,  -3.89866321e-03, ...,\n",
       "          -3.89860105e-03,  -3.89772654e-03,  -3.89807555e-03],\n",
       "        [ -3.88886686e-03,  -3.88651621e-03,  -3.87682696e-03, ...,\n",
       "          -3.89551045e-03,  -3.91649548e-03,  -3.96814477e-03],\n",
       "        [ -3.93609144e-03,  -3.93715641e-03,  -3.91191337e-03, ...,\n",
       "          -3.91007867e-03,  -3.87859717e-03,  -3.90792871e-03],\n",
       "        ..., \n",
       "        [ -3.91117949e-03,  -3.91453505e-03,  -3.88937653e-03, ...,\n",
       "          -3.94215062e-03,  -3.91416159e-03,  -3.90091096e-03],\n",
       "        [ -3.91602982e-03,  -3.87620809e-03,  -3.89354979e-03, ...,\n",
       "          -3.93342460e-03,  -3.90640274e-03,  -3.90664488e-03],\n",
       "        [ -3.91638465e-03,  -3.87464580e-03,  -3.85508244e-03, ...,\n",
       "          -3.89552047e-03,  -3.91450385e-03,  -5.55618525e+00]],\n",
       "\n",
       "       ..., \n",
       "       [[ -3.90641810e-03,  -3.90446489e-03,  -3.89394071e-03, ...,\n",
       "          -3.90287070e-03,  -3.89272952e-03,  -3.89587064e-03],\n",
       "        [ -3.89277376e-03,  -3.90693406e-03,  -3.87576385e-03, ...,\n",
       "          -3.89101985e-03,  -3.90441320e-03,  -3.95750813e-03],\n",
       "        [ -3.93314939e-03,  -3.92931374e-03,  -3.90155637e-03, ...,\n",
       "          -3.91528010e-03,  -3.89867160e-03,  -3.90347955e-03],\n",
       "        ..., \n",
       "        [ -3.89659661e-03,  -3.89348203e-03,  -3.89196887e-03, ...,\n",
       "          -3.91113339e-03,  -3.89966439e-03,  -3.89550114e-03],\n",
       "        [ -3.89968674e-03,  -3.88838071e-03,  -3.89474072e-03, ...,\n",
       "          -3.90587375e-03,  -3.91453737e-03,  -3.89957801e-03],\n",
       "        [ -3.89764411e-03,  -3.89568624e-03,  -3.89397447e-03, ...,\n",
       "          -3.89166037e-03,  -3.89685319e-03,  -3.89582757e-03]],\n",
       "\n",
       "       [[ -3.90631752e-03,  -3.90367885e-03,  -3.89372674e-03, ...,\n",
       "          -3.90348141e-03,  -3.89256165e-03,  -3.89709114e-03],\n",
       "        [ -3.89422243e-03,  -3.90937412e-03,  -3.88323120e-03, ...,\n",
       "          -3.89141566e-03,  -3.90026579e-03,  -3.93774081e-03],\n",
       "        [ -3.92138772e-03,  -3.91698396e-03,  -3.89914168e-03, ...,\n",
       "          -3.91115341e-03,  -3.90347769e-03,  -3.89983063e-03],\n",
       "        ..., \n",
       "        [ -3.89698474e-03,  -3.89513769e-03,  -3.89397284e-03, ...,\n",
       "          -3.90458223e-03,  -3.89865623e-03,  -3.89775936e-03],\n",
       "        [ -3.90030374e-03,  -3.89204407e-03,  -3.89565923e-03, ...,\n",
       "          -3.90268373e-03,  -3.90866632e-03,  -3.89864552e-03],\n",
       "        [ -3.89689393e-03,  -3.89717985e-03,  -3.89612210e-03, ...,\n",
       "          -3.89317679e-03,  -3.89816472e-03,  -3.89604294e-03]],\n",
       "\n",
       "       [[ -3.90474079e-03,  -3.90198315e-03,  -3.89433117e-03, ...,\n",
       "          -3.90280248e-03,  -3.89324478e-03,  -3.89801594e-03],\n",
       "        [ -3.89472209e-03,  -3.90781788e-03,  -3.88922310e-03, ...,\n",
       "          -3.89289716e-03,  -3.89882829e-03,  -3.92455282e-03],\n",
       "        [ -3.91284749e-03,  -3.90900997e-03,  -3.89882363e-03, ...,\n",
       "          -3.90748307e-03,  -3.90379969e-03,  -3.89805948e-03],\n",
       "        ..., \n",
       "        [ -3.89787112e-03,  -3.89624853e-03,  -3.89545131e-03, ...,\n",
       "          -3.90120829e-03,  -3.89845669e-03,  -3.89886810e-03],\n",
       "        [ -3.90051841e-03,  -3.89457052e-03,  -3.89645365e-03, ...,\n",
       "          -3.90091236e-03,  -3.90506862e-03,  -3.89855914e-03],\n",
       "        [ -3.89699358e-03,  -3.89792072e-03,  -3.89714818e-03, ...,\n",
       "          -3.89443827e-03,  -3.89894797e-03,  -3.89658450e-03]]], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-969.9986"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.sum(np.sum(testOut, axis = 2), axis =1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "params = VariableFilter(roles = [PARAMETER])(cg.variables)\n",
    "#updates = Adam(params, cost, learning_rate, c=10) #c is gradient clipping parameter\n",
    "updates = RMSprop(cost, params, learning_rate, c=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gradients = T.grad(cost, params)\n",
    "gradients = clip_norms(gradients, 1)\n",
    "gradientFun = theano.function([X,Y], gradients, allow_input_downcast=True)\n",
    "train = theano.function([X,Y], cost, updates = updates, allow_input_downcast=True)\n",
    "predict = theano.function([X], softout, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#test\n",
    "inputs = np.asarray(normalizedData[:3])\n",
    "outputs = targetMaker(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#shuffle data\n",
    "random.shuffle(normalizedData)\n",
    "trainPercent = 0.9\n",
    "trainIndex = int(len(normalizedData)*trainPercent)\n",
    "\n",
    "trainData = normalizedData[0:trainIndex]\n",
    "testData = normalizedData[trainIndex:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO: make a training function\n",
    "runname = 'firstRun'\n",
    "epochCost = []\n",
    "gradNorms = []\n",
    "\n",
    "epochs = 200000\n",
    "batch_size = 64\n",
    "iteration = 0\n",
    "\n",
    "for epoch in xrange(epochs):\n",
    "    \n",
    "    costCollect = []\n",
    "\n",
    "    for start, end in zip(range(0, len(trainData),batch_size), range(batch_size, len(trainData), batch_size)):\n",
    "        \n",
    "        inputs = trainData[start:end]\n",
    "        outputs = targetMaker(inputs)\n",
    "        costfun = train(inputs, outputs)\n",
    "        \n",
    "        \n",
    "        costCollect.append(costfun)\n",
    "                \n",
    "        iteration+=1\n",
    "        \n",
    "    ####SAVE COST TO FILE  \n",
    "    if epoch%30 == 0:\n",
    "        print(' ')\n",
    "        print 'Epoch: ', epoch\n",
    "        epochCost.append(np.mean(costCollect))\n",
    "        print 'Epoch cost average: ', epochCost[-1]\n",
    "        grads = gradientFun(inputs, outputs)\n",
    "        for gra in grads:\n",
    "            print '  gradient norms: ', np.linalg.norm(gra)\n",
    "        \n",
    "    \n",
    "    np.savetxt(runname+\"_COST.csv\", epochCost, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataPath = '/data/bradspcaps.txt'\n",
    "data = []\n",
    "with open(dataPath, 'rb') as f:\n",
    "    for line in f.readlines():\n",
    "        data.append(line.split(\"'data': \")[-1].split(',')[0].replace(\"'\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Making the hex dictionary\n",
    "hexstring = '0,\t1,\t2,\t3,\t4,\t5,\t6,\t7,\t8,\t9,\tA,\tB,\tC,\tD,\tE,\tF,\t10,\t11,\t12,\t13,\t14,\t15,\t16,\t17,\t18,\t19\\\n",
    ",\t1A,\t1B,\t1C,\t1D,\t1E,\t1F,\t20,\t21,\t22,\t23,\t24,\t25,\t26,\t27,\t28,\t29,\t2A,\t2B,\t2C,\t2D,\t2E,\t2F,\t30,\t31,\t32,\t33,\t34,\t35\\\n",
    ",\t36,\t37,\t38,\t39,\t3A,\t3B,\t3C,\t3D,\t3E,\t3F,\t40,\t41,\t42,\t43,\t44,\t45,\t46,\t47,\t48,\t49,\t4A,\t4B,\t4C,\t4D,\t4E,\t4F,\t50,\t51\\\n",
    ",\t52,\t53,\t54,\t55,\t56,\t57,\t58,\t59,\t5A,\t5B,\t5C,\t5D,\t5E,\t5F,\t60,\t61,\t62,\t63,\t64,\t65,\t66,\t67,\t68,\t69,\t6A,\t6B,\t6C,\t6D\\\n",
    ",\t6E,\t6F,\t70,\t71,\t72,\t73,\t74,\t75,\t76,\t77,\t78,\t79,\t7A,\t7B,\t7C,\t7D,\t7E,\t7F,\t80,\t81,\t82,\t83,\t84,\t85,\t86,\t87,\t88,\t89\\\n",
    ",\t8A,\t8B,\t8C,\t8D,\t8E,\t8F,\t90,\t91,\t92,\t93,\t94,\t95,\t96,\t97,\t98,\t99,\t9A,\t9B,\t9C,\t9D,\t9E,\t9F,\tA0,\tA1,\tA2,\tA3,\tA4,\tA5\\\n",
    ",\tA6,\tA7,\tA8,\tA9,\tAA,\tAB,\tAC,\tAD,\tAE,\tAF,\tB0,\tB1,\tB2,\tB3,\tB4,\tB5,\tB6,\tB7,\tB8,\tB9,\tBA,\tBB,\tBC,\tBD,\tBE,\tBF,\tC0,\tC1\\\n",
    ",\tC2,\tC3,\tC4,\tC5,\tC6,\tC7,\tC8,\tC9,\tCA,\tCB,\tCC,\tCD,\tCE,\tCF,\tD0,\tD1,\tD2,\tD3,\tD4,\tD5,\tD6,\tD7,\tD8,\tD9,\tDA,\tDB,\tDC,\tDD\\\n",
    ",\tDE,\tDF,\tE0,\tE1,\tE2,\tE3,\tE4,\tE5,\tE6,\tE7,\tE8,\tE9,\tEA,\tEB,\tEC,\tED,\tEE,\tEF,\tF0,\tF1,\tF2,\tF3,\tF4,\tF5,\tF6,\tF7,\tF8,\tF9\\\n",
    ",\tFA,\tFB,\tFC,\tFD,\tFE,\tFF'.replace('\\t', '')\n",
    "\n",
    "hexList = hexstring.lower().split(',')\n",
    "hexList.append('EOP') #End Of Packet token\n",
    "hexDict = {}\n",
    "    \n",
    "for key, val in enumerate(hexList):\n",
    "    if len(val) == 1:\n",
    "        val = '0'+val\n",
    "    hexDict[val] = key    \n",
    "\n",
    "#we add 256 on the end to signify the end of the packet ('EOP')\n",
    "tokenizedHeader = [[hexDict[header[i:i+2]] for i in xrange(0,len(header)-2+1,2)]+[256] for header in data]\n",
    "\n",
    "\n",
    "#list of arrays that represent a header with row = time \n",
    "oneHotHeaders = [np.asarray([oneHot(item) for item in header]) for header in tokenizedHeader]\n",
    "\n",
    "normalizedData = normalizeArrays(oneHotHeaders, 253, reverse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Pretraining essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numTokens = 257\n",
    "rnnType = 'gru'\n",
    "X = T.tensor3('inputs')\n",
    "Y = T.tensor3('outputs')\n",
    "linewt_init = Uniform(width=0.02)\n",
    "rnnwt_init = IsotropicGaussian(0.08)\n",
    "rnnbias_init = Constant(0.0)\n",
    "\n",
    "if rnnType == 'gru':\n",
    "    rnnDec = GatedRecurrent(dim=numTokens, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'gru')\n",
    "    dimMultiplier = 2\n",
    "else:\n",
    "    rnnDec = LSTM(dim=numTokens, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'lstm')\n",
    "    dimMultiplier = 4\n",
    "\n",
    "###ICLR suggestion -> don't use bias in RNNs\n",
    "###RECURRENT LAYER\n",
    "forkDec = Fork(output_names=['linear', 'gates'],\n",
    "            name='fork', input_dim=numTokens, output_dims=[numTokens, numTokens * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = rnnbias_init)\n",
    "data5, data6 = forkDec.apply(X)\n",
    "\n",
    "if rnnType == 'gru':\n",
    "    hDec = rnnDec.apply(data5, data6) \n",
    "else:\n",
    "    hinit, _ = rnnDec.apply(data6)\n",
    "    hDec = hinit\n",
    "\n",
    "#CRITICAL: need to loop through the arrays. Do regular people update after every sequence? or minibatch of seqs?\n",
    "    \n",
    "pYx = 1/(1+T.exp(-hDec))\n",
    "softmax = NDimensionalSoftmax()\n",
    "softout = softmax.apply(pYx, extra_ndim = 1)\n",
    "\n",
    "#get weights initialized\n",
    "forkDec.initialize()\n",
    "rnnDec.initialize()\n",
    "\n",
    "#cost = BinaryCrossEntropy().apply(Y, softout)\n",
    "precost = Y*np.log(softout) + (1-Y)*np.log(1-softout)\n",
    "cost = -T.mean(T.sum(T.sum(precost[:,:-1,:], axis = 2), axis = 1))\n",
    "cg = ComputationGraph([cost])\n",
    "\n",
    "learning_rate = 0.01\n",
    "params = VariableFilter(roles = [PARAMETER])(cg.variables)\n",
    "#updates = Adam(params, cost, learning_rate, c=10) #c is gradient clipping parameter\n",
    "updates = RMSprop(cost, params, learning_rate, c=1)\n",
    "\n",
    "gradients = T.grad(cost, params)\n",
    "gradients = clip_norms(gradients, 1)\n",
    "gradientFun = theano.function([X,Y], gradients, allow_input_downcast=True)\n",
    "train = theano.function([X,Y], cost, updates = updates, allow_input_downcast=True)\n",
    "predict = theano.function([X], softout, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random.shuffle(normalizedData)\n",
    "trainPercent = 0.9\n",
    "trainIndex = int(len(normalizedData)*trainPercent)\n",
    "\n",
    "trainData = normalizedData[0:trainIndex]\n",
    "testData = normalizedData[trainIndex:]\n",
    "\n",
    "runname = 'firstRun'\n",
    "epochCost = []\n",
    "gradNorms = []\n",
    "\n",
    "epochs = 200000\n",
    "batch_size = 64\n",
    "iteration = 0\n",
    "\n",
    "for epoch in xrange(epochs):\n",
    "    \n",
    "    costCollect = []\n",
    "\n",
    "    for start, end in zip(range(0, len(trainData),batch_size), range(batch_size, len(trainData), batch_size)):\n",
    "        \n",
    "        inputs = trainData[start:end]\n",
    "        outputs = targetMaker(inputs)\n",
    "        costfun = train(inputs, outputs)\n",
    "        \n",
    "        \n",
    "        costCollect.append(costfun)\n",
    "                \n",
    "        iteration+=1\n",
    "        \n",
    "    ####SAVE COST TO FILE  \n",
    "    if epoch%30 == 0:\n",
    "        print(' ')\n",
    "        print 'Epoch: ', epoch\n",
    "        epochCost.append(np.mean(costCollect))\n",
    "        print 'Epoch cost average: ', epochCost[-1]\n",
    "        grads = gradientFun(inputs, outputs)\n",
    "        for gra in grads:\n",
    "            print '  gradient norms: ', np.linalg.norm(gra)\n",
    "        \n",
    "    \n",
    "    np.savetxt(runname+\"_COST.csv\", epochCost, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting to CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GPU TO CPU conversion\n",
    "#Now get the weights from the test function. These weights will be numpy arrays\n",
    "w1 = test.get_shared()[0].get_value()\n",
    "\n",
    "#Here the weights are going to be set to the numpy arrays taken from the GPU predict function\n",
    "input_linear.parameters[0].set_value(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.get_shared()[2].get_value().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = '1234567890abcdefghijklmnopqrstuvwxyz'\n",
    "words = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#we add 256 on the end to signify the end of the packet ('EOP')\n",
    "\n",
    "maxPackets = 10 #limit the number of packets\n",
    "tokSessions = []\n",
    "oneHotSessions = []\n",
    "\n",
    "for ses in hexSessions.keys():    \n",
    "    tokPacket = []\n",
    "    oneHotPacket = []\n",
    "    for p in hexSessions[ses][:maxPackets]:\n",
    "        tokP = [hexDict[p[i:i+2]] for i in xrange(0,len(p)-2+1,2)]+[256] #takes hexstring and tokenizes hex pairs\n",
    "        tokPacket.append(tokP)\n",
    "        oneHotPacket.append(oneHot(tokP))\n",
    "\n",
    "    tokSessions.append(tokPacket)\n",
    "    oneHotSessions.append(oneHotPacket)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###ALT RNN LAYER\n",
    "def initialize(to_init):\n",
    "    for bricks in to_init:\n",
    "        bricks.weights_init = initialization.Uniform(width=0.08)\n",
    "        bricks.biases_init = initialization.Constant(0)\n",
    "        bricks.initialize()\n",
    "\n",
    "def gru_layer(dim, h, n):\n",
    "    fork = Fork(output_names=['linear' + str(n), 'gates' + str(n)],\n",
    "                name='fork' + str(n), input_dim=dim, output_dims=[dim, dim * 2])\n",
    "    gru = GatedRecurrent(dim=dim, name='gru' + str(n))\n",
    "    initialize([fork, gru])\n",
    "    linear, gates = fork.apply(h)\n",
    "    return gru.apply(linear, gates)\n",
    "\n",
    "\n",
    "def lstm_layer(dim, h, n):\n",
    "    linear = Linear(input_dim=dim, output_dim=dim * 4, name='linear' + str(n))\n",
    "    lstm = LSTM(dim=dim, name='lstm' + str(n))\n",
    "    initialize([linear, lstm])\n",
    "    return lstm.apply(linear.apply(h))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
